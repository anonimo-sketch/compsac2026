
@article{nardone_video_2023,
	title = {Video {Game} {Bad} {Smells}: {What} {They} {Are} and {How} {Developers} {Perceive} {Them}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3563214},
	doi = {10.1145/3563214},
	abstract = {Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Nardone, Vittoria and Muse, Biruk and Abidi, Mouna and Khomh, Foutse and Di Penta, Massimiliano},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {A forums, bad smells, empirical study, Q\&amp, Video games},
}

@article{rio_php_2023,
	title = {{PHP} code smells in web apps: {Evolution}, survival and anomalies},
	volume = {200},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111644},
	doi = {10.1016/j.jss.2023.111644},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Rio, Américo and Brito e Abreu, Fernando},
	month = jun,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Code smells, PHP, Software evolution, Survival, Web apps},
}

@article{dong_bash_2023,
	title = {Bash in the {Wild}: {Language} {Usage}, {Code} {Smells}, and {Bugs}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3517193},
	doi = {10.1145/3517193},
	abstract = {The Bourne-again shell (Bash) is a prevalent scripting language for orchestrating shell commands and managing resources in Unix-like environments. It is one of the mainstream shell dialects that is available on most GNU Linux systems. However, the unique syntax and semantics of Bash could easily lead to unintended behaviors if carelessly used. Prior studies primarily focused on improving the reliability of Bash scripts or facilitating writing Bash scripts; there is yet no empirical study on the characteristics of Bash programs written in reality, e.g., frequently used language features, common code smells, and bugs. In this article, we perform a large-scale empirical study of Bash usage, based on analyses over one million open source Bash scripts found in Github repositories. We identify and discuss which features and utilities of Bash are most often used. Using static analysis, we find that Bash scripts are often error-prone, and the error-proneness has a moderately positive correlation with the size of the scripts. We also find that the most common problem areas concern quoting, resource management, command options, permissions, and error handling. We envision that these findings can be beneficial for learning Bash and future research that aims to improve shell and command-line productivity and reliability.},
	number = {1},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Dong, Yiwen and Li, Zheyang and Tian, Yongqiang and Sun, Chengnian and Godfrey, Michael W. and Nagappan, Meiyappan},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {bash, bugs, code smells, Empirical studies, language features, shell scripts},
}

@article{bessghaier_longitudinal_2021,
	title = {A longitudinal exploratory study on code smells in server side web applications},
	volume = {29},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-021-09567-w},
	doi = {10.1007/s11219-021-09567-w},
	abstract = {Modern web applications have become one of the largest parts of the current software market over years, bringing cross-platform compatibility and data integration advantages that encouraged businesses to shift toward their adoption. Like any software application, code smells can be manifested as violations of implementation and design standards which could impact the maintainability, comprehensibility and performance of web applications. While there have been extensive studies on traditional code smells recently, little knowledge is available on code smells in web-based applications (web apps). As web applications are split into their client and server sides, we present in this study a first step in exploring the code smells diffuseness and effect on the server side of web applications. To this end, we conduct an exploratory study on a total of 430 releases from 10 long-lived open-source web-based applications on 12 common code smell types. We aim to better understand and gain insights into the diffuseness of code smells, their co-occurrences and effects on the change- and fault-proneness in server side code. Our study delivers several important findings. First, code smells are not equally diffused in web apps server side, among which smells related to complex, and large code components display high diffuseness and frequency rates. Second, the co-occurrence phenomenon is highly common, but the association degree between code smell pairs is weak. Code smells related to large size and high complexity exhibit a higher degree of co-occurrences. Third, smelly files are more likely to change than smell-free files, whereas not all smell types are likely to cause equal change sizes in the code base. Fourth, smelly files are more vulnerable to faults than smell-free files, and 86\% of smelly files are more likely to manifest more faults than other files. Hence, developers should be aware of the existence of code smells in their web applications and consider detecting and refactoring them from their code bases, using appropriate tools.},
	number = {4},
	journal = {Software Quality Journal},
	author = {Bessghaier, Narjes and Ouni, Ali and Mkaouer, Mohamed Wiem},
	month = dec,
	year = {2021},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	pages = {901--941},
}

@inproceedings{abidi_code_2019,
	address = {New York, NY, USA},
	series = {{EuroPLop} '19},
	title = {Code smells for multi-language systems},
	isbn = {978-1-4503-6206-1},
	url = {https://doi.org/10.1145/3361149.3361161},
	doi = {10.1145/3361149.3361161},
	abstract = {Software quality becomes a necessity and no longer an advantage. In fact, with the advancement of technologies, companies must provide software with good quality. Many studies introduce the use of design patterns as improving software quality and discuss the presence of occurrences of design defects as decreasing software quality. Code smells include low-level problems in source code, poor coding decisions that are symptoms of the presence of anti-patterns in the code. Most of the studies present in the literature discuss the occurrences of design defects for mono-language systems. However, nowadays most of the systems are developed using a combination of several programming languages, in order to use particular features of each of them. As the number of languages increases, so does the number of design defects. They generally do not prevent the program from functioning correctly, but they indicate a higher risk of future bugs and makes the code less readable and harder to maintain. We analysed open-source systems, developers' documentation, bug reports, and programming language specifications and extracted bad practices related to multi-language systems. We encoded these practices in the form of code smells. We report in this paper 12 code smells.},
	booktitle = {Proceedings of the 24th {European} {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {Association for Computing Machinery},
	author = {Abidi, Mouna and Grichi, Manel and Khomh, Foutse and Guéhéneuc, Yann-Gaël},
	year = {2019},
	note = {event-place: Irsee, Germany},
	keywords = {code smells, code analysis, multi-language systems, software quality},
}

@inproceedings{bessghaier_diffusion_2020,
	address = {Berlin, Heidelberg},
	title = {On the {Diffusion} and {Impact} of {Code} {Smells} in {Web} {Applications}},
	isbn = {978-3-030-59591-3},
	url = {https://doi.org/10.1007/978-3-030-59592-0_5},
	doi = {10.1007/978-3-030-59592-0_5},
	abstract = {Web applications (web apps) have become one of the largest parts of the current software market over years. Modern web apps offer several business benefits over other traditional and standalone applications. Mainly, cross-platform compatibility and data integration are some of the critical features that encouraged businesses to shift towards the adoption of Web apps. Web apps are evolving rapidly to acquire new features, correct errors or adapt to new environment changes especially with the volatile context of the web development. These ongoing amends often affect software quality due to poor coding and bad design practices, known as code smells or anti-patterns. The presence of code smells in a software project is widely considered as form of technical debt and makes the software harder to understand, maintain and evolve, besides leading to failures and unforeseen costs. Therefore, it is critical for web apps to monitor the existence and spread of such anti-patterns. In this paper, we specifically target web apps built with PHP being the most used server-side programming language. We conduct the first empirical study to investigate the diffuseness of code smells in Web apps and their relationship with the change proneness of affected code. We detect 12 types of common code smells across a total of 223 releases of 5 popular and long-lived open-source web apps. The key findings of our study include: 1) complex and large classes and methods are frequently committed in PHP files, 2) smelly files are more prone to change than non-smelly files, and 3) Too Many Methods and High Coupling are the most associated smells with files change-proneness.},
	booktitle = {Services {Computing} – {SCC} 2020: 17th {International} {Conference}, {Held} as {Part} of the {Services} {Conference} {Federation}, {SCF} 2020, {Honolulu}, {HI}, {USA}, {September} 18–20, 2020, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Bessghaier, Narjes and Ouni, Ali and Mkaouer, Mohamed Wiem},
	year = {2020},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Code smells, PHP, Change proneness, Diffuseness, Web applications},
	pages = {67--84},
}

@inproceedings{souza_applying_2017,
	address = {New York, NY, USA},
	series = {{SBCARS} '17},
	title = {Applying software metric thresholds for detection of bad smells},
	isbn = {978-1-4503-5325-0},
	url = {https://doi.org/10.1145/3132498.3134268},
	doi = {10.1145/3132498.3134268},
	abstract = {Software metrics can be an effective measurement tool to assess the quality of software. In the literature, there are a lot of software metrics applicable to systems implemented in different paradigms like Objects Oriented Programming (OOP). To guide the use of these metrics in the evaluation of the quality of software systems, it is important to define their thresholds. The aim of this study is to investigate the effectiveness of the thresholds in the evaluation of the quality of object oriented software. To do that, we used a threshold catalog of 18 software metrics derived from 100 software systems to define detection strategies for five bad smells. They are: Large Class, Long Method, Data Class, Feature Envy and Refused Bequest. We investigate the effectiveness of the thresholds in detection analysis of 12 software systems using these strategies. The results obtained by the proposed strategies were compared with the results obtained by the tools JDeodorant and JSPiRIT, used to identify bad smells. This study shows that the metric thresholds were significantly effective in supporting the detection of bad smells.},
	booktitle = {Proceedings of the 11th {Brazilian} {Symposium} on {Software} {Components}, {Architectures}, and {Reuse}},
	publisher = {Association for Computing Machinery},
	author = {Souza, Priscila P. and Sousa, Bruno L. and Ferreira, Kecia A. M. and Bigonha, Mariza A. S.},
	year = {2017},
	note = {event-place: Fortaleza, Ceará, Brazil},
	keywords = {software quality, bad smells detection, software metrics, thresholds},
}

@inproceedings{hecht_empirical_2016,
	address = {New York, NY, USA},
	series = {{MOBILESoft} '16},
	title = {An empirical study of the performance impacts of {Android} code smells},
	isbn = {978-1-4503-4178-3},
	url = {https://doi.org/10.1145/2897073.2897100},
	doi = {10.1145/2897073.2897100},
	abstract = {Android code smells are bad implementation practices within Android applications (or apps) that may lead to poor software quality, in particular in terms of performance. Yet, performance is a main software quality concern in the development of mobile apps. Correcting Android code smells is thus an important activity to increase the performance of mobile apps and to provide the best experience to mobile end-users while considering the limited constraints of mobile devices (e.g., CPU, memory, battery). However, no empirical study has assessed the positive performance impacts of correcting mobile code smells.In this paper, we therefore conduct an empirical study focusing on the individual and combined performance impacts of three Android performance code smells (namely, Internal Getter/Setter, Member Ignoring Method, and HashMap Usage) on two open source Android apps. To perform this study, we use the Paprika toolkit to detect these three code smells in the analyzed apps, and we derive four versions of the apps by correcting each detected smell independently, and all of them. Then, we evaluate the performance of each version on a common user scenario test. In particular, we evaluate the UI and memory performance using the following metrics: frame time, number of delayed frames, memory usage, and number of garbage collection calls. Our results show that correcting these Android code smells effectively improve the UI and memory performance. In particular, we observe an improvement up to 12.4\% on UI metrics when correcting Member Ignoring Method and up to 3.6\% on memory-related metrics when correcting the three Android code smells. We believe that developers can benefit from these results to guide their refactoring, and thus improve the quality of their mobile apps.},
	booktitle = {Proceedings of the {International} {Conference} on {Mobile} {Software} {Engineering} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hecht, Geoffrey and Moha, Naouel and Rouvoy, Romain},
	year = {2016},
	note = {event-place: Austin, Texas},
	keywords = {code smells, Android, metrics, mobile computing, performance},
	pages = {59--69},
}

@inproceedings{fontana_experimenting_2015,
	address = {New York, NY, USA},
	series = {{XP} '15 workshops},
	title = {On experimenting refactoring tools to remove code smells},
	isbn = {978-1-4503-3409-9},
	url = {https://doi.org/10.1145/2764979.2764986},
	doi = {10.1145/2764979.2764986},
	abstract = {When we develop a software project of a certain complexity, source code maintainability could become a problem, in particular if developers do not use a consolidate development process that simplifies the management of the entire project. When source code becomes very complex, it is difficult for developers to share and modify it. We can improve internal software qualities such as reusability, maintainability and readability through refactoring. Refactoring can be applied to remove possible problems in the code, as code smells. Identifying code smells and removing them through refactoring results in better code maintainability, but it can be an overwhelming task. In this paper, we describe our experimentation on using four refactoring tools to remove code smells in four systems, with the aim to outline advantages and disadvantages of the tools with respect to the accomplishment of this task, and to identify the smells easier to be removed among the ones we considered in this paper.},
	booktitle = {Scientific {Workshop} {Proceedings} of the {XP2015}},
	publisher = {Association for Computing Machinery},
	author = {Fontana, Francesca Arcelli and Mangiacavalli, Marco and Pochiero, Domenico and Zanoni, Marco},
	year = {2015},
	note = {event-place: Helsinki, Finland},
	keywords = {code smells, refactoring},
}

@inproceedings{nguyen_detection_2012,
	address = {New York, NY, USA},
	series = {{ASE} '12},
	title = {Detection of embedded code smells in dynamic web applications},
	isbn = {978-1-4503-1204-2},
	url = {https://doi.org/10.1145/2351676.2351724},
	doi = {10.1145/2351676.2351724},
	abstract = {In dynamic Web applications, there often exists a type of code smells, called embedded code smells, that violate important principles in software development such as software modularity and separation of concerns, resulting in much maintenance effort. Detecting and fixing those code smells is crucial yet challenging since the code with smells is embedded and generated from the server-side code. We introduce WebScent, a tool to detect such embedded code smells. WebScent first detects the smells in the generated code, and then locates them in the server-side code using the mapping between client-side code fragments and their embedding locations in the server program, which is captured during the generation of those fragments. Our empirical evaluation on real-world Web applications shows that 34\%-81\% of the tested server files contain embedded code smells. We also found that the source files with more embedded code smells are likely to have more defects and scattered changes, thus potentially require more maintenance effort.},
	booktitle = {Proceedings of the 27th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Hung Viet and Nguyen, Hoan Anh and Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Tien N.},
	year = {2012},
	note = {event-place: Essen, Germany},
	keywords = {Code Smells, Dynamic Web Applications, Embedded Code},
	pages = {282--285},
}

@article{yamashita_assessing_2014,
	title = {Assessing the capability of code smells to explain maintenance problems: an empirical study combining quantitative and qualitative data},
	volume = {19},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-013-9250-3},
	doi = {10.1007/s10664-013-9250-3},
	abstract = {Code smells are indicators of deeper design problems that may cause difficulties in the evolution of a software system. This paper investigates the capability of twelve code smells to reflect actual maintenance problems. Four medium-sized systems with equivalent functionality but dissimilar design were examined for code smells. Three change requests were implemented on the systems by six software developers, each of them working for up to four weeks. During that period, we recorded problems faced by developers and the associated Java files on a daily basis. We developed a binary logistic regression model, with "problematic file" as the dependent variable. Twelve code smells, file size, and churn constituted the independent variables. We found that violation of the Interface Segregation Principle (a.k.a. ISP violation) displayed the strongest connection with maintenance problems. Analysis of the nature of the problems, as reported by the developers in daily interviews and think-aloud sessions, strengthened our view about the relevance of this code smell. We observed, for example, that severe instances of problems relating to change propagation were associated with ISP violation. Based on our results, we recommend that code with ISP violation should be considered potentially problematic and be prioritized for refactoring.},
	number = {4},
	journal = {Empirical Softw. Engg.},
	author = {Yamashita, Aiko},
	month = aug,
	year = {2014},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Code smells, Maintenance problems, Refactoring, Software maintenance},
	pages = {1111--1143},
}

@inproceedings{gauthier_semantic_2013,
	series = {{ICSE} '13},
	title = {Semantic smells and errors in access control models: a case study in {PHP}},
	isbn = {978-1-4673-3076-3},
	abstract = {Access control models implement mechanisms to restrict access to sensitive data from unprivileged users. Access controls typically check privileges that capture the semantics of the operations they protect. Semantic smells and errors in access control models stem from privileges that are partially or totally unrelated to the action they protect. This paper presents a novel approach, partly based on static analysis and information retrieval techniques, for the automatic detection of semantic smells and errors in access control models. Investigation of the case study application revealed 31 smells and 2 errors. Errors were reported to developers who quickly confirmed their relevance and took actions to correct them. Based on the obtained results, we also propose three categories of semantic smells and errors to lay the foundations for further research on access control smells in other systems and domains.},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Gauthier, François and Merlo, Ettore},
	year = {2013},
	note = {Place: San Francisco, CA, USA},
	pages = {1169--1172},
}

@inproceedings{soltanifar_software_2016,
	address = {New York, NY, USA},
	series = {{IDEAS} '16},
	title = {Software {Analytics} in {Practice}: {A} {Defect} {Prediction} {Model} {Using} {Code} {Smells}},
	isbn = {978-1-4503-4118-9},
	url = {https://doi.org/10.1145/2938503.2938553},
	doi = {10.1145/2938503.2938553},
	abstract = {In software engineering, maintainability is related to investigating the defects and their causes, correcting the defects and modifying the system to meet customer requirements. Maintenance is a time consuming activity within the software life cycle. Therefore, there is a need for efficiently organizing the software resources in terms of time, cost and personnel for maintenance activity. One way of efficiently managing maintenance resources is to predict defects that may occur after the deployment. Many researchers so far have built defect prediction models using different sets of metrics such as churn and static code metrics. However, hidden causes of defects such as code smells have not been investigated thoroughly. In this study we propose using data science and analytics techniques on software data to build defect prediction models. In order to build the prediction model we used code smells metrics, churn metrics and combination of churn and code smells metrics. The results of our experiments on two different software companies show that code smells is a good indicator of defect proneness of the software product. Therefore, we recommend that code smells metrics should be used to train a defect prediction model to guide the software maintenance team.},
	booktitle = {Proceedings of the 20th {International} {Database} {Engineering} \&amp; {Applications} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Soltanifar, Behjat and Akbarinasaji, Shirin and Caglayan, Bora and Bener, Ayse Basar and Filiz, Asli and Kramer, Bryan M.},
	year = {2016},
	note = {event-place: Montreal, QC, Canada},
	keywords = {Code Smells, Defect Prediction Model, Mining software repositories},
	pages = {148--155},
}

@inproceedings{tahir_can_2018,
	address = {New York, NY, USA},
	series = {{EASE} '18},
	title = {Can you tell me if it smells? {A} study on how developers discuss code smells and anti-patterns in {Stack} {Overflow}},
	isbn = {978-1-4503-6403-4},
	url = {https://doi.org/10.1145/3210459.3210466},
	doi = {10.1145/3210459.3210466},
	abstract = {This paper investigates how developers discuss code smells and anti-patterns over Stack Overflow to understand better their perceptions and understanding of these two concepts. Understanding developers' perceptions of these issues are important in order to inform and align future research efforts and direct tools vendors in the area of code smells and anti-patterns. In addition, such insights could lead the creation of solutions to code smells and anti-patterns that are better fit to the realities developers face in practice. We applied both quantitative and qualitative techniques to analyse discussions containing terms associated with code smells and anti-patterns. Our findings show that developers widely use Stack Overflow to ask for general assessments of code smells or anti-patterns, instead of asking for particular refactoring solutions. An interesting finding is that developers very often ask their peers 'to smell their code' (i.e., ask whether their own code 'smells' or not), and thus, utilize Stack Overflow as an informal, crowd-based code smell/anti-pattern detector. We conjecture that the crowd-based detection approach considers contextual factors, and thus, tends to be more trusted by developers over automated detection tools. We also found that developers often discuss the downsides of implementing specific design patterns, and 'flag' them as potential anti-patterns to be avoided. Conversely, we found discussions on why some anti-patterns previously considered harmful should not be flagged as anti-patterns. Our results suggest that there is a need for: 1) more context-based evaluations of code smells and anti-patterns, and 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2018},
	publisher = {Association for Computing Machinery},
	author = {Tahir, Amjed and Yamashita, Aiko and Licorish, Sherlock and Dietrich, Jens and Counsell, Steve},
	year = {2018},
	note = {event-place: Christchurch, New Zealand},
	keywords = {empirical study, Code smells, anti-patterns, mining software repositories, Stack Overflow},
	pages = {68--78},
}

@article{lewowski_how_2022,
	title = {How far are we from reproducible research on code smell detection? {A} systematic literature review},
	volume = {144},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2021.106783},
	doi = {10.1016/j.infsof.2021.106783},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Lewowski, Tomasz and Madeyski, Lech},
	month = apr,
	year = {2022},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Code smells, Reproducibility, Reproducible research, Software engineering},
}

@article{rubert_effects_2022,
	title = {On the effects of continuous delivery on code quality: {A} case study in industry},
	volume = {81},
	issn = {0920-5489},
	url = {https://doi.org/10.1016/j.csi.2021.103588},
	doi = {10.1016/j.csi.2021.103588},
	number = {C},
	journal = {Comput. Stand. Interfaces},
	author = {Rubert, Maluane and Farias, Kleinner},
	month = apr,
	year = {2022},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Software engineering, Continuous delivery, Enterprise resource planning, Software development},
}

@inproceedings{de_oliveira_junior_calint_2022,
	address = {Berlin, Heidelberg},
	title = {{CALint}: {A} {Tool} for\&nbsp;{Enforcing} the\&nbsp;{Clean} {Architecture}’s {Dependency} {Rule} in\&nbsp;{Python}},
	isbn = {978-3-031-10547-0},
	url = {https://doi.org/10.1007/978-3-031-10548-7_39},
	doi = {10.1007/978-3-031-10548-7_39},
	abstract = {Clean Architecture (CA) aims to address the need for more loosely coupled components and better cohesion. CA focuses on preparing software engineers to write more stable, durable, and flexible applications capable of distinguishing between details (e.g., what framework it uses) and the business logic requirements. A literature review shows that considerable effort has been devoted to cataloging and solving code smells related to code, often called code smells. However, the same does not apply to architecture smells – its software architecture counterpart. Similar research regarding other programming languages such as Java, PHP, or C\# represents noteworthy works in the area, but they do not address Python applications directly. This work directs efforts towards redesigning and adapting existing Python programs to the CA principles by detecting the code smells that break the CA constraints through the developed CALint tool. Moreover, this approach proposes two extended refactoring techniques to solve these smells efficiently by grouping and comparing static code analysis and reuse them to enforce Clean Architecture’s Dependency Rule programmatically. To demonstrate the feasibility of the two refactoring techniques described in this work and the CALint tool, we applied them to three different case studies. The major findings of this work include two extended refactoring techniques and the development of a tool to verify non-conformities related to the Clean Architecture dependency rule. The results show common cases where the dependency rule was violated and highlighted by the CALint tool, which are fixed with the support of refactoring steps.},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2022 {Workshops}: {Malaga}, {Spain}, {July} 4–7, 2022, {Proceedings}, {Part} {V}},
	publisher = {Springer-Verlag},
	author = {de Oliveira Junior, Clevio Orlando and Carvalho, Jonathan and Silveira, Fábio Fagundes and da Silva, Tiago Silva and Guerra, Eduardo Martins},
	year = {2022},
	note = {event-place: Malaga, Spain},
	keywords = {Code smells, Refactoring, Software engineering, Clean architecture},
	pages = {534--549},
}

@inproceedings{fernandes_review-based_2016,
	address = {New York, NY, USA},
	series = {{EASE} '16},
	title = {A review-based comparative study of bad smell detection tools},
	isbn = {978-1-4503-3691-8},
	url = {https://doi.org/10.1145/2915970.2915984},
	doi = {10.1145/2915970.2915984},
	abstract = {Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C\#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Fernandes, Eduardo and Oliveira, Johnatan and Vale, Gustavo and Paiva, Thanis and Figueiredo, Eduardo},
	year = {2016},
	note = {event-place: Limerick, Ireland},
	keywords = {bad smells, comparative study, detection tools, systematic literature review},
}

@article{chen_understanding_2018,
	title = {Understanding metric-based detectable smells in {Python} software},
	volume = {94},
	issn = {0950-5849},
	abstract = {ContextCode smells are supposed to cause potential comprehension and maintenance problems in software development. Although code smells are studied in many languages, e.g. Java and C\#, there is a lack of technique or tool support addressing code smells in Python. ObjectiveDue to the great differences between Python and static languages, the goal of this study is to define and detect code smells in Python programs and to explore the effects of Python smells on software maintainability. MethodIn this paper, we introduced ten code smells and established a metric-based detection method with three different filtering strategies to specify metric thresholds (Experience-Based Strategy, Statistics-Based Strategy, and Tuning Machine Strategy). Then, we performed a comparative study to investigate how three detection strategies perform in detecting Python smells and how these smells affect software maintainability with different detection strategies. This study utilized a corpus of 106 Python projects with most stars on GitHub. ResultsThe results showed that: (1) the metric-based detection approach performs well in detecting Python smells and Tuning Machine Strategy achieves the best accuracy; (2) the three detection strategies discover some different smell occurrences, and Long Parameter List and Long Method are more prevalent than other smells; (3) several kinds of code smells are more significantly related to changes or faults in Python modules. ConclusionThese findings reveal the key features of Python smells and also provide a guideline for the choice of detection strategy in detecting and analyzing Python smells.},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Chen, Zhifei and Chen, Lin and Ma, Wanwangying and Zhou, Xiaoyu and Zhou, Yuming and Xu, Baowen},
	month = feb,
	year = {2018},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Code smell, Detection strategy, Python, Software maintainability},
	pages = {14--29},
}

@inproceedings{shcherban_automatic_2020,
	address = {New York, NY, USA},
	series = {{ESEM} '20},
	title = {Automatic {Identification} of {Code} {Smell} {Discussions} on {Stack} {Overflow}: {A} {Preliminary} {Investigation}},
	isbn = {978-1-4503-7580-1},
	url = {https://doi.org/10.1145/3382494.3422161},
	doi = {10.1145/3382494.3422161},
	abstract = {Background: Code smells indicate potential design or implementation problems that may have a negative impact on programs. Similar to other software artefacts, developers use Stack Overflow (SO) to ask questions about code smells. However, given the high number of questions asked on the platform, and the limitations of the default tagging system, it takes significant effort to extract knowledge about code smells by means of manual approaches. Aim: We utilized supervised machine learning techniques to automatically identify code-smell discussions from SO posts. Method: We conducted an experiment using a manually labeled dataset that contains 3000 code-smell and 3000 non-code-smell posts to evaluate the performance of different classifiers when automatically identifying code smell discussions. Results: Our results show that Logistic Regression (LR) with parameter C=20 (inverse of regularization strength) and Bag of Words (BoW) feature extraction technique achieved the best performance amongst the algorithms we evaluated with a precision of 0.978, a recall of 0.965, and an F1-score of 0.971. Conclusion: Our results show that machine learning approach can effectively locate code-smell posts even if posts' title and/or tags cannot be of help. The technique can be used to extract code smell discussions from other textual artefacts (e.g., code reviews), and promisingly to extract SO discussions of other topics.},
	booktitle = {Proceedings of the 14th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {Association for Computing Machinery},
	author = {Shcherban, Sergei and Liang, Peng and Tahir, Amjed and Li, Xueying},
	year = {2020},
	note = {event-place: Bari, Italy},
	keywords = {Stack Overflow, Automatic Classification, Code Smell, Discussion},
}

@inproceedings{freitas_analyzing_2023,
	address = {New York, NY, USA},
	series = {{SBES} '23},
	title = {Analyzing the {Impact} of {CI} {Sub}-practices on {Continuous} {Code} {Quality} in {Open}-{Source} {Projects}: {An} {Empirical} {Study}},
	isbn = {979-8-4007-0787-2},
	url = {https://doi.org/10.1145/3613372.3613403},
	doi = {10.1145/3613372.3613403},
	abstract = {Continuous Integration (CI) is a widely adopted practice that automates and enhances the frequency of code integration. Previous research has explored the relationship between CI sub-practices (such as frequent commit activity and high test coverage) and software quality. However, limited knowledge exists regarding the impact of specific CI sub-practices on the Continuous Code Quality (CCQ) inspection outcomes of software projects, such as technical debts, bug density, duplicated lines, and code smells. This paper aims to analyze the extent to which the adoption of CI sub-practices improves CCQ outcome metrics in software projects. First, we investigate the association between Travis CI adoption, Travis CI usage maturity, and improved quality inspection outcomes in a set of 75 popular open-source projects from GitHub. Additionally, we use data from other 97 open-source projects to identify specific CI sub-practices, such as maintaining shorter build durations, that exhibit a strong correlation with enhanced quality inspection outcomes. Our findings reveal that the quality inspection outcomes are improved in projects with higher test coverage and shorter build duration. Merely adopting a CI service is not a guarantee for improved quality outcomes. Therefore, it is essential for developers to consistently adhere to recommended CI sub-practices. By doing so, they can effectively harness the benefits of CI in their projects and achieve improved quality outcomes.},
	booktitle = {Proceedings of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Freitas, Guilherme and Bernardo, JoãO Helis and SizíLio, Gustavo and Costa, Daniel Alencar Da and Kulesza, Uirá},
	year = {2023},
	note = {event-place: Campo Grande, Brazil},
	keywords = {software quality, continuous integration, empirical software engineering, software maintenance},
	pages = {1--10},
}

@article{yamashita_what_2013,
	title = {To what extent can maintenance problems be predicted by code smell detection? - {An} empirical study},
	volume = {55},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2013.08.002},
	doi = {10.1016/j.infsof.2013.08.002},
	abstract = {Context: Code smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution. Objective: This study is aimed at a detailed investigation to which extent problems in maintenance projects can be predicted by the detection of currently known code smells. Method: A multiple case study was conducted, in which the problems faced by six developers working on four different Java systems were registered on a daily basis, for a period up to four weeks. Where applicable, the files associated to the problems were registered. Code smells were detected in the pre-maintenance version of the systems, using the tools Borland Together and InCode. In-depth examination of quantitative and qualitative data was conducted to determine if the observed problems could be explained by the detected smells. Results: From the total set of problems, roughly 30\% percent were related to files containing code smells. In addition, interaction effects were observed amongst code smells, and between code smells and other code characteristics, and these effects led to severe problems during maintenance. Code smell interactions were observed between collocated smells (i.e., in the same file), and between coupled smells (i.e., spread over multiple files that were coupled). Conclusions: The role of code smells on the overall system maintainability is relatively minor, thus complementary approaches are needed to achieve more comprehensive assessments of maintainability. Moreover, to improve the explanatory power of code smells, interaction effects amongst collocated smells and coupled smells should be taken into account during analysis.},
	number = {12},
	journal = {Inf. Softw. Technol.},
	author = {Yamashita, Aiko and Moonen, Leon},
	month = dec,
	year = {2013},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Code smells, Empirical study, Maintainability},
	pages = {2223--2242},
}

@inproceedings{mendoza_detecting_2021,
	series = {{MODELS} '19},
	title = {Detecting architectural issues during the continuous integration pipeline},
	isbn = {978-1-7281-5125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00090},
	doi = {10.1109/MODELS-C.2019.00090},
	abstract = {The use of a software reference architecture limits possible deviations and errors in the implementation of software projects, as the code must follow predefined rules that developers must respect to guarantee quality. However, when introducing new code to projects these rules can be violated. As a result, architectural erosion, bad smells, or even bugs that can be difficult to find are introduced to the projects. This paper proposes an approach for reviewing compliance to predefined rules that map architectural decisions to code. During the continuous integration process, the automatic analysis raises an issue for each rule violation. Developers can analyze and correct issues, and trace/visualize improvements, or lack thereof, through time. We present a validation experiment carried out in the context of a Software Development course, and we show how the approach helps developers to write better code1.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {IEEE Press},
	author = {Mendoza, Camilo and Garcés, Kelly and Casallas, Rubby and Bocanegra, José},
	year = {2021},
	note = {Place: Munich, Germany},
	keywords = {continuous integration, architectural rules, issue identification, issue visualization, rule violation},
	pages = {589--597},
}

@inproceedings{baddreddin_impact_2018,
	address = {USA},
	series = {{CASCON} '18},
	title = {The impact of design and {UML} modeling on codebase quality and sustainability},
	abstract = {The general consensus of researchers and practitioners is that up-front and continuous software design using modeling languages such as UML improve code quality and reliability particularly as the software evolves over time. Software designs and models help in managing the underlying code complexities which are crucial for sustainability. Recently, there has been increasing evidence suggesting broader adoption of modeling languages such as UML. However, our understanding of the impact of using such modeling and design languages remains limited. This paper reports on a study that aims to characterize this impact on code quality and sustainability. We identify a sample of open source software repositories with extensive use of designs and modeling and compare their code qualities with similar code-centric repositories. Our evaluation focuses on various code quality attributes such as code smells and technical debt. We also conduct code evolution analysis over five-year period and collect additional data from questionnaires and interviews with active repository contributors. This study finds that repositories with significant use of models and design activities are associated with reduced critical code smells but are also associated with increase in non-critical code smells. The study also finds that modeling and design activities are associated with significant reduction in measures of technical debt. Analyzing code evolution over five year period reveals that UML repositories start with significantly lower technical debt density measures but tend to decline over time.},
	booktitle = {Proceedings of the 28th {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Baddreddin, Omar and Rahad, Khandoker},
	year = {2018},
	note = {event-place: Markham, Ontario, Canada},
	keywords = {code smells, software maintenance, code-centric development, empirical investigation, model driven software development, open source repositories, software design, sustainability, technical debt, UML},
	pages = {236--244},
}

@article{alkharabsheh_software_2019,
	title = {Software {Design} {Smell} {Detection}: a systematic mapping study},
	volume = {27},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-018-9424-8},
	doi = {10.1007/s11219-018-9424-8},
	abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18\&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
	number = {3},
	journal = {Software Quality Journal},
	author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, José A.},
	month = sep,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Antipatterns, DesignSmell, Detection tools, Quality models, Systematic mapping study},
	pages = {1069--1148},
}

@inproceedings{yamashita_exploring_2013,
	series = {{ICSE} '13},
	title = {Exploring the impact of inter-smell relations on software maintainability: an empirical study},
	isbn = {978-1-4673-3076-3},
	abstract = {Code smells are indicators of issues with source code quality that may hinder evolution. While previous studies mainly focused on the effects of individual code smells on maintainability, we conjecture that not only the individual code smells but also the interactions between code smells affect maintenance. We empirically investigate the interactions amongst 12 code smells and analyze how those interactions relate to maintenance problems. Professional developers were hired for a period of four weeks to implement change requests on four medium-sized Java systems with known smells. On a daily basis, we recorded what specific problems they faced and which artifacts were associated with them. Code smells were automatically detected in the pre-maintenance versions of the systems and analyzed using Principal Component Analysis (PCA) to identify patterns of co-located code smells. Analysis of these factors with the observed maintenance problems revealed how smells that were co-located in the same artifact interacted with each other, and affected maintainability. Moreover, we found that code smell interactions occurred across coupled artifacts, with comparable negative effects as same-artifact co-location. We argue that future studies into the effects of code smells on maintainability should integrate dependency analysis in their process so that they can obtain a more complete understanding by including such coupled interactions.},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Yamashita, Aiko and Moonen, Leon},
	year = {2013},
	note = {Place: San Francisco, CA, USA},
	pages = {682--691},
}

@inproceedings{bogner_type_2022,
	address = {New York, NY, USA},
	series = {{MSR} '22},
	title = {To type or not to type? a systematic comparison of the software quality of {JavaScript} and typescript applications on {GitHub}},
	isbn = {978-1-4503-9303-4},
	url = {https://doi.org/10.1145/3524842.3528454},
	doi = {10.1145/3524842.3528454},
	abstract = {JavaScript (JS) is one of the most popular programming languages, and widely used for web apps, mobile apps, desktop clients, and even backend development. Due to its dynamic and flexible nature, however, JS applications often have a reputation for poor software quality. While the type-safe superset TypeScript (TS) offers features to address these prejudices, there is currently insufficient empirical evidence to broadly support the claim that TS applications exhibit better software quality than JS applications.We therefore conducted a repository mining study based on 604 GitHub projects (299 for JS, 305 for TS) with over 16M LoC. Using SonarQube and the GitHub API, we collected and analyzed four facets of software quality: a) code quality (\# of code smells per LoC), b) code understandability (cognitive complexity per LoC), c) bug proneness (bug fix commit ratio), and d) bug resolution time (mean time a bug issue is open). For TS, we also collected how frequently the type-safety ignoring any type was used per project via ESLint.The analysis indicates that TS applications exhibit significantly better code quality and understandability than JS applications. Contrary to expectations, however, bug proneness and bug resolution time of our TS sample were not significantly lower than for JS: the mean bug fix commit ratio of TS projects was more than 60\% larger (0.126 vs. 0.206), and TS projects needed on average more than an additional day to fix bugs (31.86 vs. 33.04 days). Furthermore, reducing the usage of the any type in TS apps appears to be beneficial: its frequency was significantly correlated with all metrics except bug proneness, even though the correlations were of small strengths (Spearman's rho between 0.17 and 0.26).Our results indicate that the perceived positive influence of TypeScript for avoiding bugs in comparison to JavaScript may be more complicated than assumed. While using TS seems to have benefits, it does not automatically lead to less and easier to fix bugs. However, more research is needed in this area, especially concerning the potential influence of project complexity and developer experience.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Bogner, Justus and Merkel, Manuel},
	year = {2022},
	note = {event-place: Pittsburgh, Pennsylvania},
	keywords = {software quality, GitHub, JavaScript, repository mining, TypeScript},
	pages = {658--669},
}

@article{amit_corrective_2021,
	title = {Corrective commit probability: a measure of the effort invested in bug fixing},
	volume = {29},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-021-09564-z},
	doi = {10.1007/s11219-021-09564-z},
	abstract = {The effort invested in software development should ideally be devoted to the implementation of new features. But some of the effort is invariably also invested in corrective maintenance, that is in fixing bugs. Not much is known about what fraction of software development work is devoted to bug fixing, and what factors affect this fraction. We suggest the Corrective Commit Probability (CCP), which measures the probability that a commit reflects corrective maintenance, as an estimate of the relative effort invested in fixing bugs. We identify corrective commits by applying a linguistic model to the commit messages, achieving an accuracy of 93\%, higher than any previously reported model. We compute the CCP of all large active GitHub projects (7,557 projects with 200+ commits in 2019). This leads to the creation of an investment scale, suggesting that the bottom 10\% of projects spend less than 6\% of their total effort on bug fixing, while the top 10\% of projects spend at least 39\% of their effort on bug fixing — more than 6 times more. Being a process metric, CCP is conditionally independent of source code metrics, enabling their evaluation and investigation. Analysis of project attributes shows that lower CCP (that is, lower relative investment in bug fixing) is associated with smaller files, lower coupling, use of languages like JavaScript and C\# as opposed to PHP and C++, fewer code smells, lower project age, better perceived quality, fewer developers, lower developer churn, better onboarding, and better productivity.},
	number = {4},
	journal = {Software Quality Journal},
	author = {Amit, Idan and Feitelson, Dror G.},
	month = dec,
	year = {2021},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Corrective commits, Corrective maintenance, Effort estimate, Process metric},
	pages = {817--861},
}

@article{mariani_systematic_2017,
	title = {A systematic review on search-based refactoring},
	volume = {83},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2016.11.009},
	doi = {10.1016/j.infsof.2016.11.009},
	abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest.Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities.Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation.Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics.Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Mariani, Thainá and Vergilio, Silvia Regina},
	month = mar,
	year = {2017},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Refactoring, Evolutionary algorithms, Search-based software engineering},
	pages = {14--34},
}

@inproceedings{saarimaki_diffuseness_2019,
	series = {{TechDebt} '19},
	title = {On the diffuseness of code technical debt in {Java} projects of the apache ecosystem},
	url = {https://doi.org/10.1109/TechDebt.2019.00028},
	doi = {10.1109/TechDebt.2019.00028},
	abstract = {Background. Companies commonly invest major effort into removing, respectively not introducing, technical debt issues detected by static analysis tools such as SonarQube, Cast, or Coverity. These tools classify technical debt issues into categories according to severity, and developers commonly pay attention to not introducing issues with a high level of severity that could generate bugs or make software maintenance more difficult.Objective. In this work, we aim to understand the diffuseness of Technical Debt (TD) issues and the speed with which developers remove them from the code if they introduced such an issue. The goal is to understand which type of TD is more diffused and how much attention is paid by the developers, as well as to investigate whether TD issues with a higher level of severity are resolved faster than those with a lower level of severity. We conducted a case study across 78K commits of 33 Java projects from the Apache Software Foundation Ecosystem to investigate the distribution of 1.4M TD items.Results. TD items introduced into the code are mostly related to code smells (issues that can increase the maintenance effort). Moreover, developers commonly remove the most severe issues faster than less severe ones. However, the time needed to resolve issues increases when the level of severity increases (minor issues are removed faster that blocker ones).Conclusion. One possible answer to the unexpected issue of resolution time might be that severity is not correctly defined by the tools. Another possible answer is that the rules at an intermediate severity level could be the ones that technically require more time to be removed. The classification of TD items, including their severity and type, require thorough investigation from a research point of view.},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Technical} {Debt}},
	publisher = {IEEE Press},
	author = {Saarimäki, Nyyti and Lenarduzzi, Valentina and Taibi, Davide},
	year = {2019},
	note = {Place: Montreal, Quebec, Canada},
	keywords = {sonarqube, technical debt issues, violations},
	pages = {98--107},
}

@article{caivano_spread_2023,
	title = {On the spread and evolution of dead methods in {Java} desktop applications: an exploratory study},
	volume = {28},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-023-10303-0},
	doi = {10.1007/s10664-023-10303-0},
	abstract = {Background. Dead code is a code smell. It can refer to code blocks, fields, methods, etc. that are unused and/or unreachable—e.g., if a method is unused and/or unreachable, it is a dead method. Past research has shown that the presence of dead code in source code harms its comprehensibility and maintainability. Nevertheless, there is still little empirical evidence on the spread of this code smell in the source code of commercial and open-source software applications.Aims. Our goal is to gather, through an exploratory study, empirical evidence on the spread and evolution of dead methods in open-source Java desktop applications.Method. We quantitatively analyzed the commit histories of 23 open-source Java desktop applications, whose software projects were hosted on GitHub. To investigate the spread and evolution of dead methods, we focused on dead methods detected at a commit level. The total number of analyzed commits in our study is 1,587. The perspective of our exploratory study is that of both practitioners and researchers.Results. We can summarize the most important take-away results as follows: (i) dead methods affect open-source Java desktop applications; (ii) dead methods generally survive for a long time before being “buried” or “revived;” (iii) dead methods that are then revived tend to survive less, as compared to dead methods that are then buried; (iv) dead methods are rarely revived; and (v) most dead methods are stillborn, rather than becoming dead later. Given the exploratory nature of our study, we believe that its results will help researchers to conduct more resource- and time-demanding research on dead methods and, in general, on dead code.Conclusions. We can conclude that developers should carefully handle dead code (and thus dead methods) since it is harmful, widespread, rarely revived, and survives for a long time in software applications.},
	number = {3},
	journal = {Empirical Softw. Engg.},
	author = {Caivano, Danilo and Cassieri, Pietro and Romano, Simone and Scanniello, Giuseppe},
	month = apr,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Code smell, GitHub, Dead code, Exploratory study, Java desktop applications, Open-source, Unused code},
}

@inproceedings{ocariza_detecting_2017,
	series = {{ASE} '17},
	title = {Detecting unknown inconsistencies in web applications},
	isbn = {978-1-5386-2684-9},
	abstract = {Although there has been increasing demand for more reliable web applications, JavaScript bugs abound in web applications. In response to this issue, researchers have proposed automated fault detection tools, which statically analyze the web application code to find bugs. While useful, these tools either only target a limited set of bugs based on predefined rules, or they do not detect bugs caused by cross-language interactions, which occur frequently in web application code. To address this problem, we present an anomaly-based inconsistency detection approach, implemented in a tool called Holocron. The main novelty of our approach is that it does not look for hard-coded inconsistency classes. Instead, it applies subtree pattern matching to infer inconsistency classes and association rule mining to detect inconsistencies that occur both within a single language, and between two languages. We evaluated Holocron, and it successfully detected 51 previously unreported inconsistencies - including 18 bugs and 33 code smells - in 12 web applications.},
	booktitle = {Proceedings of the 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Ocariza, Jr., Frolin S. and Pattabiraman, Karthik and Mesbah, Ali},
	year = {2017},
	note = {Place: Urbana-Champaign, IL, USA},
	keywords = {JavaScript, cross-language interactions, fault detection},
	pages = {566--577},
}

@inproceedings{tamrawi_build_2012,
	series = {{ICSE} '12},
	title = {Build code analysis with symbolic evaluation},
	isbn = {978-1-4673-1067-3},
	abstract = {Build process is crucial in software development. However, the analysis support for build code is still limited. In this paper, we present SYMake, an infrastructure and tool for the analysis of build code in make. Due to the dynamic nature of make language, it is challenging to understand and maintain complex Makefiles. SYMake provides a symbolic evaluation algorithm that processes Makefiles and produces a symbolic dependency graph (SDG), which represents the build dependencies (i.e. rules) among files via commands. During the symbolic evaluation, for each resulting string value in an SDG that represents a part of a file name or a command in a rule, SYMake provides also an acyclic graph (called T-model) to represent its symbolic evaluation trace. We have used SYMake to develop algorithms and a tool 1) to detect several types of code smells and errors in Makefiles, and 2) to support build code refactoring, e.g. renaming a variable/target even if its name is fragmented and built from multiple substrings. Our empirical evaluation for SYMake's renaming on several real-world systems showed its high accuracy in entity renaming. Our controlled experiment showed that with SYMake, developers were able to understand Makefiles better and to detect more code smells as well as to perform refactoring more accurately.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Tamrawi, Ahmed and Nguyen, Hoan Anh and Nguyen, Hung Viet and Nguyen, Tien N.},
	year = {2012},
	note = {Place: Zurich, Switzerland},
	pages = {650--660},
}

@inproceedings{caivano_exploratory_2021,
	address = {New York, NY, USA},
	series = {{ESEM} '21},
	title = {An {Exploratory} {Study} on {Dead} {Methods} in {Open}-source {Java} {Desktop} {Applications}},
	isbn = {978-1-4503-8665-4},
	url = {https://doi.org/10.1145/3475716.3475773},
	doi = {10.1145/3475716.3475773},
	abstract = {Background. Dead code is a code smell. It can refer to code blocks, fields, methods, etc. that are unused and/or unreachable. Empirical evidence shows that dead code harms source code comprehensibility and maintainability in software applications. Researchers have gathered little empirical evidence on the spread of dead code in software applications. Moreover, we know little about the role of this code smell during software evolution.Aims. Our goal is to gather preliminary empirical evidence on the spread and evolution of dead methods in open-source Java desktop applications. Given the exploratory nature of our investigation, we believe that its results can justify more resource- and time-demanding research on dead methods.Method. We quantitatively analyzed the commit histories of 13 open-source Java desktop applications, whose software projects were hosted on GitHub, for a total of 1,044 commits. We focused on dead methods detected at a commit level to investigate the spread and evolution of dead methods in the studied applications. The perspective of our explorative study is that of both practitioners and researchers.Results. The most important take-away results can be summarized as follows: (i) dead methods seems to affect open-source Java desktop applications; (ii) dead methods generally survive for a long time, in terms of commits, before being "buried" or "revived;" (iii) dead methods are rarely revived; and (iv) most dead methods are dead since the creation of the corresponding methods. Conclusions. We conclude that developers should carefully handle dead methods in open-source Java desktop applications since this code smell is harmful, widespread, rarely revived, and survives for a long time in software applications. Our results also justify future research on dead methods.},
	booktitle = {Proceedings of the 15th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {Association for Computing Machinery},
	author = {Caivano, Danilo and Cassieri, Pietro and Romano, Simone and Scanniello, Giuseppe},
	year = {2021},
	note = {event-place: Bari, Italy},
	keywords = {Code smell, GitHub, Java desktop applications, dead code, exploratory study, open-source, unused code},
}

@inproceedings{gadient_security_2021,
	address = {New York, NY, USA},
	series = {{ESEM} '21},
	title = {Security {Smells} {Pervade} {Mobile} {App} {Servers}},
	isbn = {978-1-4503-8665-4},
	url = {https://doi.org/10.1145/3475716.3475780},
	doi = {10.1145/3475716.3475780},
	abstract = {[Background] Web communication is universal in cyberspace, and security risks in this domain are devastating. [Aims] We analyzed the prevalence of six security smells in mobile app servers, and we investigated the consequence of these smells from a security perspective. [Method] We used an existing dataset that includes 9 714 distinct URLs used in 3 376 Android mobile apps. We exercised these URLs twice within 14 months and investigated the HTTP headers and bodies. [Results] We found that more than 69\% of tested apps suffer from three kinds of security smells, and that unprotected communication and misconfigurations are very common in servers. Moreover, source-code and version leaks, or the lack of update policies expose app servers to security risks. [Conclusions] Poor app server maintenance greatly hampers security.},
	booktitle = {Proceedings of the 15th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {Association for Computing Machinery},
	author = {Gadient, Pascal and Tarnutzer, Marc-Andrea and Nierstrasz, Oscar and Ghafari, Mohammad},
	year = {2021},
	note = {event-place: Bari, Italy},
	keywords = {mobile apps, security smells, web communication},
}

@inproceedings{nagy_sqlinspect_2018,
	address = {New York, NY, USA},
	series = {{ICSE} '18},
	title = {{SQLInspect}: a static analyzer to inspect database usage in {Java} applications},
	isbn = {978-1-4503-5663-3},
	url = {https://doi.org/10.1145/3183440.3183496},
	doi = {10.1145/3183440.3183496},
	abstract = {We present SQLInspect, a tool intended to assist developers who deal with SQL code embedded in Java applications. It is integrated into Eclipse as a plug-in that is able to extract SQL queries from Java code through static string analysis. It parses the extracted queries and performs various analyses on them. As a result, one can readily explore the source code which accesses a given part of the database, or which is responsible for the construction of a given SQL query. SQL-related metrics and common coding mistakes are also used to spot inefficiently or defectively performing SQL statements and to identify poorly designed classes, like those that construct many queries via complex control-flow paths. SQLInspect is a novel tool that relies on recent query extraction approaches. It currently supports Java applications working with JDBC and SQL code written for MySQL or Apache Impala. Check out the live demo of SQLInspect at http://perso.unamur.be/ cnagy/sqlinspect.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceeedings}},
	publisher = {Association for Computing Machinery},
	author = {Nagy, Csaba and Cleve, Anthony},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {bad smells, metrics, apache impala, concept location, eclipse, embedded SQL, Java, JDBC, MySQL, static analysis},
	pages = {93--96},
}

@article{jaafar_analyzing_2017,
	title = {Analyzing software evolution and quality by extracting {Asynchrony} change patterns},
	volume = {131},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2017.05.047},
	doi = {10.1016/j.jss.2017.05.047},
	abstract = {An empirical study to identify the more risky part of code in software systems.Cloned files that follow Asynchrony change patterns are more fault-prone.Anti-patterns following Asynchrony change pattern are up to five times faultier. Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones.Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns.},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Jaafar, Fehmi and Lozano, Angela and Guhneuc, Yann-Gal and Mens, Kim},
	month = sep,
	year = {2017},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Anti-patterns, Change patterns, Clones, Fault-proneness, Software quality},
	pages = {311--322},
}

@inproceedings{silva_lccss_2020,
	address = {New York, NY, USA},
	series = {{SBCARS} '20},
	title = {{LCCSS}: {A} {Similarity} {Metric} for {Identifying} {Similar} {Test} {Code}},
	isbn = {978-1-4503-8754-5},
	url = {https://doi.org/10.1145/3425269.3425283},
	doi = {10.1145/3425269.3425283},
	abstract = {Test code maintainability is a common concern in software testing. In order to achieve good maintainability, test methods should be clearly structured, well named, small in size, and, mainly, test code duplication should be avoided. Several strategies exist to avoid test code duplication, such as implicit setup and delegated setup. However, prior to applying these strategies, first it is necessary to identify the duplicate code, which can be a time-consuming task. To address this problem, we automate the identification of duplicate test code through the application of code similarity metrics. We propose a novel similarity metric, called Longest Common Contiguous Start Sub-Sequence (LCCSS), to identify refactoring candidates. LCCSS is a metric used to measure similarity between pairs of tests. The most similar pairs are reported as strong candidates to be refactored through the implicit setup strategy. We also develop a framework, called Ró{\textbackslash}.za, that can use different similarity metrics to identify test code duplication. An experiment shows that LCCSS and Simian, a clone detection tool, have both identified pairs of tests to be refactored through the implicit setup strategy with maximum precision in all the eleven standard recall levels. But, unlike Simian, LCCSS does not need to be calibrated for each project.},
	booktitle = {Proceedings of the 14th {Brazilian} {Symposium} on {Software} {Components}, {Architectures}, and {Reuse}},
	publisher = {Association for Computing Machinery},
	author = {Silva, Lucas Pereira da and Vilain, Patrícia},
	year = {2020},
	note = {event-place: Natal, Brazil},
	keywords = {refactoring, implicit setup, measure, metric, similarity, testing},
	pages = {91--100},
}

@article{borstler_developers_2023,
	title = {Developers talking about code quality},
	volume = {28},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-023-10381-0},
	doi = {10.1007/s10664-023-10381-0},
	abstract = {There are many aspects of code quality, some of which are difficult to capture or to measure. Despite the importance of software quality, there is a lack of commonly accepted measures or indicators for code quality that can be linked to quality attributes. We investigate software developers’ perceptions of source code quality and the practices they recommend to achieve these qualities. We analyze data from semi-structured interviews with 34 professional software developers, programming teachers and students from Europe and the U.S. For the interviews, participants were asked to bring code examples to exemplify what they consider good and bad code, respectively. Readability and structure were used most commonly as defining properties for quality code. Together with documentation, they were also suggested as the most common target properties for quality improvement. When discussing actual code, developers focused on structure, comprehensibility and readability as quality properties. When analyzing relationships between properties, the most commonly talked about target property was comprehensibility. Documentation, structure and readability were named most frequently as source properties to achieve good comprehensibility. Some of the most important source code properties contributing to code quality as perceived by developers lack clear definitions and are difficult to capture. More research is therefore necessary to measure the structure, comprehensibility and readability of code in ways that matter for developers and to relate these measures of code structure, comprehensibility and readability to common software quality attributes.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Börstler, Jürgen and Bennin, Kwabena E. and Hooshangi, Sara and Jeuring, Johan and Keuning, Hieke and Kleiner, Carsten and MacKellar, Bonnie and Duran, Rodrigo and Störrle, Harald and Toll, Daniel and van Assema, Jelle},
	month = sep,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Software development, Code quality, Quality perception, Semi-structured interviews, Source code properties},
}

@article{alomar_preserving_2021,
	title = {On preserving the behavior in software refactoring: {A} systematic mapping study},
	volume = {140},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2021.106675},
	doi = {10.1016/j.infsof.2021.106675},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali},
	month = dec,
	year = {2021},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Refactoring, Systematic mapping study, Behavior preservation},
}

@inproceedings{favato_linuss_2019,
	address = {New York, NY, USA},
	series = {{SBQS} '19},
	title = {Linus's {Law}: {More} {Eyes} {Fewer} {Flaws} in {Open} {Source} {Projects}},
	isbn = {978-1-4503-7282-4},
	url = {https://doi.org/10.1145/3364641.3364650},
	doi = {10.1145/3364641.3364650},
	abstract = {Linus's Law states that "given enough eyeballs, all bugs are shallow". In other words, given a large enough number of developers, almost every programming flaw is characterized and fixed quickly. Although there is much debate about this subject, we still lack empirical evidence to support this law. Given that this theme has, and still is, motivating business decisions in software development, we investigate the implications of Linus's Law in two empirical studies on open source projects mined from GitHub. In the first pilot study, we mined seven popular Java projects from GitHub and investigated the correlation between committers and programming flaws in source code files. Results of this pilot study suggest a positive correlation between the number of developers and programming flaws. We cross-validate these results in a second study with almost one hundred Python projects from GitHub. In this second study, we analyzed the correlation between the number of forks - i.e., a proxy for number of developers - and programming flaws identified in projects. In both studies, programming flaws were detected by using static code analysis tools. As a result of the second study, we could not observe a correlation between the number of developers and the number of programming flaws in Python projects. From both studies we conclude that we were unable to find evidence to support the Linus's Law.},
	booktitle = {Proceedings of the {XVIII} {Brazilian} {Symposium} on {Software} {Quality}},
	publisher = {Association for Computing Machinery},
	author = {Favato, Danilo and Ishitani, Daniel and Oliveira, Johnatan and Figueiredo, Eduardo},
	year = {2019},
	note = {event-place: Fortaleza, Brazil},
	keywords = {software quality, java, Linus's law, python},
	pages = {69--78},
}

@inproceedings{sharma_smelly_2018,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIP} '18},
	title = {Smelly relations: measuring and understanding database schema quality},
	isbn = {978-1-4503-5659-6},
	url = {https://doi.org/10.1145/3183519.3183529},
	doi = {10.1145/3183519.3183529},
	abstract = {Context: Databases are an integral element of enterprise applications. Similarly to code, database schemas are also prone to smells - best practice violations.Objective: We aim to explore database schema quality, associated characteristics and their relationships with other software artifacts.Method: We present a catalog of 13 database schema smells and elicit developers' perspective through a survey. We extract embedded sql statements and identify database schema smells by employing the DbDeo tool which we developed. We analyze 2925 production-quality systems (357 industrial and 2568 well-engineered open-source projects) and empirically study quality characteristics of their database schemas. In total, we analyze 629 million lines of code containing more than 393 thousand sql statements.Results: We find that the index abuse smell occurs most frequently in database code, that the use of an orm framework doesn't immune the application from database smells, and that some database smells, such as adjacency list, are more prone to occur in industrial projects compared to open-source projects. Our co-occurrence analysis shows that whenever the clone table smell in industrial projects and the values in attribute definition smell in open-source projects get spotted, it is very likely to find other database smells in the project.Conclusion: The awareness and knowledge of database smells are crucial for developing high-quality software systems and can be enhanced by the adoption of better tools helping developers to identify database smells early.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {Association for Computing Machinery},
	author = {Sharma, Tushar and Fragkoulis, Marios and Rizou, Stamatia and Bruntink, Magiel and Spinellis, Diomidis},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {code smells, software quality, software maintenance, technical debt, antipatterns, database schema smells},
	pages = {55--64},
}

@inproceedings{siddiq_securityeval_2022,
	address = {New York, NY, USA},
	series = {{MSR4P}\&amp;{S} 2022},
	title = {{SecurityEval} dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques},
	isbn = {978-1-4503-9457-4},
	url = {https://doi.org/10.1145/3549035.3561184},
	doi = {10.1145/3549035.3561184},
	abstract = {Automated source code generation is currently a popular machine-learning-based task. It can be helpful for software developers to write functionally correct code from a given context. However, just like human developers, a code generation model can produce vulnerable code, which the developers can mistakenly use. For this reason, evaluating the security of a code generation model is a must. In this paper, we describe SecurityEval, an evaluation dataset to fulfill this purpose. It contains 130 samples for 75 vulnerability types, which are mapped to the Common Weakness Enumeration (CWE). We also demonstrate using our dataset to evaluate one open-source (i.e., InCoder) and one closed-source code generation model (i.e., GitHub Copilot).},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Mining} {Software} {Repositories} {Applications} for {Privacy} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Siddiq, Mohammed Latif and Santos, Joanna C. S.},
	year = {2022},
	note = {event-place: Singapore, Singapore},
	keywords = {code generation, common weakness enumeration, dataset, security},
	pages = {29--33},
}

@article{aghili_studying_2023,
	title = {Studying the characteristics of {AIOps} projects on {GitHub}},
	volume = {28},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-023-10382-z},
	doi = {10.1007/s10664-023-10382-z},
	abstract = {Artificial Intelligence for IT Operations (AIOps) leverages AI approaches to handle the massive amount of data generated during the operations of software systems. Prior works have proposed various AIOps solutions to support different tasks in system operations and maintenance, such as anomaly detection. In this study, we conduct an in-depth analysis of open-source AIOps projects to understand the characteristics of AIOps in practice. We first carefully identify a set of AIOps projects from GitHub and analyze their repository metrics (e.g., the used programming languages). Then, we qualitatively examine the projects to understand their input data, analysis techniques, and goals. Finally, we assess the quality of these projects using different quality metrics, such as the number of bugs. To provide context, we also sample two sets of baseline projects from GitHub: a random sample of machine learning projects and a random sample of general-purposed projects. By comparing different metrics between our identified AIOps projects and these baselines, we derive meaningful insights. Our results reveal a recent and growing interest in AIOps solutions. However, the quality metrics indicate that AIOps projects suffer from more issues than our baseline projects. We also pinpoint the most common issues in AIOps approaches and discuss potential solutions to address these challenges. Our findings offer valuable guidance to researchers and practitioners, enabling them to comprehend the current state of AIOps practices and shed light on different ways of improving AIOps’ weaker aspects. To the best of our knowledge, this work marks the first attempt to characterize open-source AIOps projects.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Aghili, Roozbeh and Li, Heng and Khomh, Foutse},
	month = oct,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {AIOps, Data mining, Qualitative analysis, Repository mining, Source code analysis, Temporal trends},
}

@book{suryanarayana_refactoring_2014,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {Refactoring for {Software} {Design} {Smells}: {Managing} {Technical} {Debt}},
	isbn = {0-12-801397-4},
	abstract = {Awareness of design smells - indicators of common design problems - helps developers or software engineers understand mistakes made while designing, what design principles were overlooked or misapplied, and what principles need to be applied properly to address those smells through refactoring. Developers and software engineers may "know" principles and patterns, but are not aware of the "smells" that exist in their design because of wrong or mis-application of principles or patterns. These smells tend to contribute heavily to technical debt - further time owed to fix projects thought to be complete - and need to be addressed via proper refactoring. Refactoring for Software Design Smells presents 25 structural design smells, their role in identifying design issues, and potential refactoring solutions. Organized across common areas of software design, each smell is presented with diagrams and examples illustrating the poor design practices and the problems that result, creating a catalog of nuggets of readily usable information that developers or engineers can apply in their projects. The authors distill their research and experience as consultants and trainers, providing insights that have been used to improve refactoring and reduce the time and costs of managing software projects. Along the way they recount anecdotes from actual projects on which the relevant smell helped address a design issue. Contains a comprehensive catalog of 25 structural design smells (organized around four fundamental design principles) that contribute to technical debt in software projects Presents a unique naming scheme for smells that helps understand the cause of a smell as well as points toward its potential refactoring Includes illustrative examples that showcase the poor design practices underlying a smell and the problems that result Covers pragmatic techniques for refactoring design smells to manage technical debt and to create and maintain high-quality software in practice Presents insightful anecdotes and case studies drawn from the trenches of real-world projects},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Suryanarayana, Girish and Samarthyam, Ganesh and Sharma, Tushar},
	year = {2014},
}

@article{bogner_industry_2021,
	title = {Industry practices and challenges for the evolvability assurance of microservices: {An} interview study and systematic grey literature review},
	volume = {26},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-021-09999-9},
	doi = {10.1007/s10664-021-09999-9},
	number = {5},
	journal = {Empirical Softw. Engg.},
	author = {Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred},
	month = sep,
	year = {2021},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Assurance, Evolvability, Grey literature review, Industry, Interviews, Microservices},
}

@book{noauthor_sast_2023,
	address = {New York, NY, USA},
	title = {{SAST} '23: {Proceedings} of the 8th {Brazilian} {Symposium} on {Systematic} and {Automated} {Software} {Testing}},
	isbn = {979-8-4007-1629-4},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{yamashita_software_2017,
	series = {{MSR} '17},
	title = {Software evolution and quality data from controlled, multiple, industrial case studies},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.44},
	doi = {10.1109/MSR.2017.44},
	abstract = {A main difficulty to study the evolution and quality of real-life software systems is the effect of moderator factors, such as: programming skill, type of maintenance task, and learning effect. Experimenters must account for moderator factors to identify the relationships between the variables of interest. In practice, controlling for moderator factors in realistic (industrial) settings is expensive and rather difficult. The data presented in this paper has two particularities: First, it involves six professional developers and four real-life, industrial systems. Second, it was obtained from controlled, multiple case studies where the moderator variables: programming skill, maintenance task, and learning effect were controlled for. This data set is relevant to experimenters studying evolution and quality of reallife systems, in particular those interested in studying industrial systems and replicating empirical studies.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Yamashita, Aiko and Abtahizadeh, S. Amirhossein and Khomh, Foutse and Guéhéneuc, Yann-Gaël},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	keywords = {empirical study, code smells, software quality, case study, industrial data, moderator factors, replication, software defects, software evolution, software replicability},
	pages = {507--510},
}

@inproceedings{cassieri_developers_2022,
	address = {New York, NY, USA},
	series = {{EASE} '22},
	title = {Do {Developers} {Modify} {Dead} {Methods} during the {Maintenance} of {Java} {Desktop} {Applications}?},
	isbn = {978-1-4503-9613-4},
	url = {https://doi.org/10.1145/3530019.3530032},
	doi = {10.1145/3530019.3530032},
	abstract = {Background: Dead code is a code smell. It can refer to code blocks, variables, parameters, fields, methods, classes, etc. that are unused and/or unreachable. Aim: Results from past empirical studies indicate that dead code is widespread in both desktop and web-based software applications. Also, researchers have shown that both comprehensibility and maintainability of source code are negatively affected when dead code is present. Nevertheless, we still know little about maintenance operations involving dead code. Method: We conducted an exploratory empirical study on 13 open-source Java desktop applications, whose software projects were hosted on GitHub, to provide preliminary evidence on whether, and to what extent, developers modify dead code—more specifically, dead methods—when they deal with the maintenance of open-source Java desktop applications. Results: The most important results of our study can be summarized as follows: (i)\&nbsp;developers modify dead methods; (ii)\&nbsp;dead methods are modified to a different extent as compared to alive methods; (iii)\&nbsp;developers spend time modifying dead methods that are removed in subsequent commits; and (iv) developers modify dead methods that are later revived to a different extent as compared to dead methods that are later\&nbsp;removed. Conclusions: One of the conclusions of our study is: developers should remove dead methods, whose presence and purpose are not properly documented, to avoid unnecessary modifications to dead methods during the maintenance of software applications.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Cassieri, Pietro and Romano, Simone and Scanniello, Giuseppe and Tortora, Genoveffa and Caivano, Danilo},
	year = {2022},
	note = {event-place: Gothenburg, Sweden},
	keywords = {Code smell, dead code, unused code, lava flow, unreachable code},
	pages = {120--129},
}

@article{zakeri-nasrabadi_systematic_2023,
	title = {A systematic literature review on source code similarity measurement and clone detection: {Techniques}, applications, and challenges},
	volume = {204},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111796},
	doi = {10.1016/j.jss.2023.111796},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Zakeri-Nasrabadi, Morteza and Parsa, Saeed and Ramezani, Mohammad and Roy, Chanchal and Ekhtiarzadeh, Masoud},
	month = oct,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Code clone, Code recommendation, Plagiarism detection, Source code similarity, Systematic literature review},
}

@article{de_stefano_impacts_2022,
	title = {Impacts of software community patterns on process and product: {An} empirical study},
	volume = {214},
	issn = {0167-6423},
	url = {https://doi.org/10.1016/j.scico.2021.102731},
	doi = {10.1016/j.scico.2021.102731},
	number = {C},
	journal = {Sci. Comput. Program.},
	author = {De Stefano, Manuel and Iannone, Emanuele and Pecorelli, Fabiano and Tamburri, Damian Andrew},
	month = feb,
	year = {2022},
	note = {Place: USA
Publisher: Elsevier North-Holland, Inc.},
	keywords = {Empirical studies, Community patterns, Community smells},
}

@book{noauthor_msr4pamps_2022,
	address = {New York, NY, USA},
	title = {{MSR4P}\&amp;{S} 2022: {Proceedings} of the 1st {International} {Workshop} on {Mining} {Software} {Repositories} {Applications} for {Privacy} and {Security}},
	isbn = {978-1-4503-9457-4},
	abstract = {On behalf of the Program Committee, we are pleased to present the proceedings of the 1st International Workshop on Mining Software Repositories for Privacy and Security (MSR4P\&amp;S 2022). MSR4P\&amp;S is co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). This year, because of the Covid-19 pandemic, MSR4P\&amp;S (as part of ESEC/FSE) is held virtually with an adapted program that will bring together international researchers to exchange ideas, share experiences, investigate problems, and propose promising solutions concerning the application of Mining Software Repositories (MSR) to investigate the different stages of privacy and security. The workshop topics cover a wide range of MSR applications for cybersecurity research, including empirical and mixed-method approaches, as well as datasets and tools.},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_ease_2023,
	address = {New York, NY, USA},
	title = {{EASE} '23: {Proceedings} of the 27th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	isbn = {979-8-4007-0044-6},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_promise_2023,
	address = {New York, NY, USA},
	title = {{PROMISE} 2023: {Proceedings} of the 19th {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	isbn = {979-8-4007-0375-1},
	abstract = {It is our pleasure to welcome you to the 19th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2023), to be held in presence on December 8th, 2023, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2023).},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{lenarduzzi_critical_2023,
	title = {A critical comparison on six static analysis tools: {Detection}, agreement, and precision},
	volume = {198},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2022.111575},
	doi = {10.1016/j.jss.2022.111575},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Lenarduzzi, Valentina and Pecorelli, Fabiano and Saarimaki, Nyyti and Lujan, Savanna and Palomba, Fabio},
	month = apr,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Empirical study, Software quality, Static analysis tools},
}

@book{noauthor_sbqs_2023,
	address = {New York, NY, USA},
	title = {{SBQS} '23: {Proceedings} of the {XXII} {Brazilian} {Symposium} on {Software} {Quality}},
	isbn = {979-8-4007-0786-5},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{azuma_empirical_2022,
	title = {An empirical study on self-admitted technical debt in {Dockerfiles}},
	volume = {27},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-021-10081-7},
	doi = {10.1007/s10664-021-10081-7},
	abstract = {In software development, ad hoc solutions that are intentionally implemented by developers are called self-admitted technical debt (SATD). Because the existence of SATD spreads poor implementations, it is necessary to remove it as soon as possible. Meanwhile, container virtualization has been attracting attention in recent years as a technology to support infrastructure such as servers. Currently, Docker is the de facto standard for container virtualization. In Docker, a file describing how to build a container (Dockerfile) is a set of procedural instructions; thus, it can be considered as a kind of source code. Moreover, because Docker is a relatively new technology, there are few developers who have accumulated good or bad practices for building Docker container. Hence, it is likely that Dockerfiles contain many SATDs, as is the case with general programming language source code analyzed in previous SATD studies. The goal of this paper is to categorize SATDs in Dockerfiles and to share knowledge with developers and researchers. To achieve this goal, we conducted a manual classification for SATDs in Dockerfile. We found that about 3.0\% of the comments in Dockerfile are SATD. In addition, we have classified SATDs into five classes and eleven subclasses. Among them, there are some SATDs specific to Docker, such as SATDs for version fixing and for integrity check. The three most common classes of SATD were related to lowering maintainability, testing, and defects.},
	number = {2},
	journal = {Empirical Softw. Engg.},
	author = {Azuma, Hideaki and Matsumoto, Shinsuke and Kamei, Yasutaka and Kusumoto, Shinji},
	month = mar,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Container virtualization, Docker, Infrastructure as code (IaC), Self-admitted technical debt, Technical debt},
}

@article{bessghaier_what_2023,
	title = {What {Constitutes} the {Deployment} and {Runtime} {Configuration} {System}? {An} {Empirical} {Study} on {OpenStack} {Projects}},
	volume = {33},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3607186},
	doi = {10.1145/3607186},
	abstract = {Modern software systems are designed to be deployed in different configured environments (e.g., permissions, virtual resources, network connections) and adapted at runtime to different situations (e.g., memory limits, enabling/disabling features, database credentials). Such a configuration during the deployment and runtime of a software system is implemented via a set of configuration files, which together constitute what we refer to as a “configuration system.” Recent research efforts investigated the evolution and maintenance of configuration files. However, they merely focused on a limited part of the configuration system (e.g., specific infrastructure configuration files or Dockerfiles), and their results do not generalize to the whole configuration system. To cope with such a limitation, we aim to better capture and understand what files constitute a configuration system. To do so, we leverage an open card sort technique to qualitatively study 1,756 configuration files from OpenStack, a large and widely studied open source software ecosystem. Our investigation reveals the existence of nine types of configuration files, which cover the creation of the infrastructure on top of which OpenStack will be deployed, along with other types of configuration files used to customize OpenStack after its deployment. These configuration files are interconnected while being used at different deployment stages. For instance, we observe specific configuration files used during the deployment stage to create other configuration files that are used in the runtime stage. We also observe that identifying and classifying these types of files is not straightforward, as five out of the nine types can be written in similar programming languages (e.g., Python and Bash) as regular source code files. We also found that the same file extensions (e.g., Yaml) can be used for different configuration types, making it difficult to identify and classify configuration files. Thus, we first leverage a machine learning model to identify configuration from non-configuration files, which achieved a median area under the curve (AUC) of 0.91, a median Brier score of 0.12, a median precision of 0.86, and a median recall of 0.83. Thereafter, we leverage a multi-class classification model to classify configuration files based on the nine configuration types. Our multi-class classification model achieved a median weighted AUC of 0.92, a median Brier score of 0.04, a median weighted precision of 0.84, and a median weighted recall of 0.82. Our analysis also shows that with only 100 labeled configuration and non-configuration files, our model reached a median AUC higher than 0.69. Furthermore, our configuration model requires a minimum of 100 configuration files to reach a median weighted AUC higher than 0.75.},
	number = {1},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Bessghaier, Narjes and Sayagh, Mohammed and Ouni, Ali and Mkaouer, Mohamed Wiem},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Configuration files, files classification, infrastructure-as-code, machine learning models, OpenStack},
}

@book{noauthor_sbes_2022,
	address = {New York, NY, USA},
	title = {{SBES} '22: {Proceedings} of the {XXXVI} {Brazilian} {Symposium} on {Software} {Engineering}},
	isbn = {978-1-4503-9735-3},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{li_survey_2020,
	title = {A {Survey} on {Renamings} of {Software} {Entities}},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3379443},
	doi = {10.1145/3379443},
	abstract = {More than 70\% of characters in the source code are used to label identifiers. Consequently, identifiers are one of the most important source for program comprehension. Meaningful identifiers are crucial to understand and maintain programs. However, for reasons like constrained schedule, inexperience, and unplanned evolution, identifiers may fail to convey the semantics of the entities associated with them. As a result, such entities should be renamed to improve software quality. However, manual renaming and recommendation are fastidious, time consuming, and error prone, whereas automating the process of renamings is challenging: (1) It involves complex natural language processing to understand the meaning of identifers; (2) It also involves difficult semantic analysis to determine the role of software entities. Researchers proposed a number of approaches and tools to facilitate renamings. We present a survey on existing approaches and classify them into identification of renaming opportunities, execution of renamings, and detection of renamings. We find that there is an imbalance between the three type of approaches, and most of implementation of approaches and evaluation dataset are not publicly available. We also discuss the challenges and present potential research directions. To the best of our knowledge, this survey is the first comprehensive study on renamings of software entities.},
	number = {2},
	journal = {ACM Comput. Surv.},
	author = {Li, Guangjie and Liu, Hui and Nyamawe, Ally S.},
	month = apr,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {software quality, identifier, Rename refactoring},
}

@inproceedings{zarras_safari_2023,
	address = {Berlin, Heidelberg},
	title = {A {Safari} for\&nbsp;{Deviating} {GoF} {Pattern} {Definitions} and\&nbsp;{Examples} on\&nbsp;the\&nbsp;{Web}},
	isbn = {978-3-031-47261-9},
	url = {https://doi.org/10.1007/978-3-031-47262-6_10},
	doi = {10.1007/978-3-031-47262-6_10},
	abstract = {The Gang of Four (GoF) patterns have been around for many years now. People use them to solve object-oriented design problems. The main source to consult for the GoF patterns is the seminal book published by Gamma, Helm, Johnson, and Vlissides in 1994. However, today there is also a large amount of information about the GoF patterns on the Web. There, the developers can find pattern definitions and code examples.In this paper, we assess the compliance of pattern definitions and examples found on the Web to the original GoF pattern definitions. We study a corpus of definitions and examples, gathered from 4 well-known sites. According to our findings, most of the provided pattern definitions comply with the original GoF pattern definitions. However, there are some intent deviations that result in incorrect definitions. There are also a few deviations that concern missing and incomplete participants. When it comes to the patterns examples, the situation is quite different. Deviations in the examples are much more frequent and include missing participants, incomplete participants, and erroneous participants. The paper concludes with a discussion of the practical implications of our findings for the developers.},
	booktitle = {Conceptual {Modeling}: 42nd {International} {Conference}, {ER} 2023, {Lisbon}, {Portugal}, {November} 6–9, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Zarras, Apostolos V. and Vassiliadis, Panos},
	year = {2023},
	note = {event-place: Lisbon, Portugal},
	keywords = {deviating definitions, deviating examples, GoF design patterns},
	pages = {181--197},
}

@article{tosch_planalyzer_2019,
	title = {{PlanAlyzer}: assessing threats to the validity of online experiments},
	volume = {3},
	url = {https://doi.org/10.1145/3360608},
	doi = {10.1145/3360608},
	abstract = {Online experiments have become a ubiquitous aspect of design and engineering processes within Internet firms. As the scale of experiments has grown, so has the complexity of their design and implementation. In response, firms have developed software frameworks for designing and deploying online experiments. Ensuring that experiments in these frameworks are correctly designed and that their results are trustworthy—referred to as internal validity—can be difficult. Currently, verifying internal validity requires manual inspection by someone with substantial expertise in experimental design. We present the first approach for statically checking the internal validity of online experiments. Our checks are based on well-known problems that arise in experimental design and causal inference. Our analyses target PlanOut, a widely deployed, open-source experimentation framework that uses a domain-specific language to specify and run complex experiments. We have built a tool called PlanAlyzer that checks PlanOut programs for a variety of threats to internal validity, including failures of randomization, treatment assignment, and causal sufficiency. PlanAlyzer uses its analyses to automatically generate contrasts, a key type of information required to perform valid statistical analyses over the results of these experiments. We demonstrate PlanAlyzer's utility on a corpus of PlanOut scripts deployed in production at Facebook, and we evaluate its ability to identify threats to validity on a mutated subset of this corpus. PlanAlyzer has both precision and recall of 92\% on the mutated corpus, and 82\% of the contrasts it generates match hand-specified data.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	author = {Tosch, Emma and Bakshy, Eytan and Berger, Emery D. and Jensen, David D. and Moss, J. Eliot B.},
	month = oct,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Experimental Design, Online Experiments, Threats to Validity},
}

@article{peruma_how_2022,
	title = {How do i refactor this? {An} empirical study on refactoring trends and topics in {Stack} {Overflow}},
	volume = {27},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-021-10045-x},
	doi = {10.1007/s10664-021-10045-x},
	abstract = {An essential part of software maintenance and evolution, refactoring is performed by developers, regardless of technology or domain, to improve the internal quality of the system, and reduce its technical debt. However, choosing the appropriate refactoring strategy is not always straightforward, resulting in developers seeking assistance. Although research in refactoring is well-established, with several studies altering between the detection of refactoring opportunities and the recommendation of appropriate code changes, little is known about their adoption in practice. Analyzing the perception of developers is critical to understand better what developers consider to be problematic in their code and how they handle it. Additionally, there is a need for bridging the gap between refactoring, as research, and its adoption in practice, by extracting common refactoring intents that are more suitable for what developers face in reality. In this study, we analyze refactoring discussions on Stack Overflow through a series of quantitative and qualitative experiments. Our results show that Stack Overflow is utilized by a diverse set of developers for refactoring assistance for a variety of technologies. Our observations show five areas that developers typically require help with refactoring– Code Optimization, Tools and IDEs, Architecture and Design Patterns, Unit Testing, and Database. We envision our findings better bridge the support between traditional (or academic) aspects of refactoring and their real-world applicability, including better tool support.},
	number = {1},
	journal = {Empirical Softw. Engg.},
	author = {Peruma, Anthony and Simmons, Steven and AlOmar, Eman Abdullah and Newman, Christian D. and Mkaouer, Mohamed Wiem and Ouni, Ali},
	month = jan,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Refactoring, Stack Overflow, Empirical study, Software maintenance and evolution},
}

@article{rosa_what_2023,
	title = {What {Quality} {Aspects} {Influence} the {Adoption} of {Docker} {Images}?},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3603111},
	doi = {10.1145/3603111},
	abstract = {Docker is a containerization technology that allows developers to ship software applications along with their dependencies in Docker images. Developers can extend existing images using them as base images when writing Dockerfiles. However, a lot of alternative functionally equivalent base images are available. Although many studies define and evaluate quality features that can be extracted from Docker artifacts, the criteria on which developers choose a base image over another remain unclear.In this article, we aim to fill this gap. First, we conduct a literature review through which we define a taxonomy of quality features, identifying two main groups: configuration-related features (i.e., mainly related to the Dockerfile and image build process), and externally observable features (i.e., what the Docker image users can observe). Second, we ran an empirical study considering the developers’ preference for 2,441 Docker images in 1,911 open source software projects. We want to understand how the externally observable features influence the developers’ preferences, and how they are related to the configuration-related features. Our results pave the way to the definition of a reliable quality measure for Docker artifacts, along with tools that support developers for a quality-aware development of them.},
	number = {6},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Rosa, Giovanni and Scalabrino, Simone and Bavota, Gabriele and Oliveto, Rocco},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {software maintenance, Docker, container virtualization, Empirical software engineering},
}

@book{noauthor_esem_2022,
	address = {New York, NY, USA},
	title = {{ESEM} '22: {Proceedings} of the 16th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	isbn = {978-1-4503-9427-7},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{zhou_drive_2023,
	title = {{DRIVE}: {Dockerfile} {Rule} {Mining} and {Violation} {Detection}},
	volume = {33},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3617173},
	doi = {10.1145/3617173},
	abstract = {A Dockerfile defines a set of instructions to build Docker images, which can then be instantiated to support containerized applications. Recent studies have revealed a considerable amount of quality issues with Dockerfiles. In this article, we propose a novel approach, Dockerfiles Rule mIning and Violation dEtection (DRIVE), to mine implicit rules and detect potential violations of such rules in Dockerfiles. DRIVE first parses Dockerfiles and transforms them to an intermediate representation. It then leverages an efficient sequential pattern mining algorithm to extract potential patterns. With heuristic-based reduction and moderate human intervention, potential rules are identified, which can then be utilized to detect potential violations of Dockerfiles. DRIVE identifies 34 semantic rules and 19 syntactic rules including 9 new semantic rules that have not been reported elsewhere. Extensive experiments on real-world Dockerfiles demonstrate the efficacy of our approach.},
	number = {2},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Zhou, Yu and Zhan, Weilin and Li, Zi and Han, Tingting and Chen, Taolue and Gall, Harald},
	month = dec,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Docker, configuration files, dockerfile, pattern mining, violation detection},
}

@inproceedings{lipp_empirical_2022,
	address = {New York, NY, USA},
	series = {{ISSTA} 2022},
	title = {An empirical study on the effectiveness of static {C} code analyzers for vulnerability detection},
	isbn = {978-1-4503-9379-9},
	url = {https://doi.org/10.1145/3533767.3534380},
	doi = {10.1145/3533767.3534380},
	abstract = {Static code analysis is often used to scan source code for security vulnerabilities. Given the wide range of existing solutions implementing different analysis techniques, it is very challenging to perform an objective comparison between static analysis tools to determine which ones are most effective at detecting vulnerabilities. Existing studies are thereby limited in that (1) they use synthetic datasets, whose vulnerabilities do not reflect the complexity of security bugs that can be found in practice and/or (2) they do not provide differentiated analyses w.r.t. the types of vulnerabilities output by the static analyzers. Hence, their conclusions about an analyzer's capability to detect vulnerabilities may not generalize to real-world programs. In this paper, we propose a methodology for automatically evaluating the effectiveness of static code analyzers based on CVE reports. We evaluate five free and open-source and one commercial static C code analyzer(s) against 27 software projects containing a total of 1.15 million lines of code and 192 vulnerabilities (ground truth). While static C analyzers have been shown to perform well in benchmarks with synthetic bugs, our results indicate that state-of-the-art tools miss in-between 47\% and 80\% of the vulnerabilities in a benchmark set of real-world programs. Moreover, our study finds that this false negative rate can be reduced to 30\% to 69\% when combining the results of static analyzers, at the cost of 15 percentage points more functions flagged. Many vulnerabilities hence remain undetected, especially those beyond the classical memory-related security bugs.},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Lipp, Stephan and Banescu, Sebastian and Pretschner, Alexander},
	year = {2022},
	note = {event-place: Virtual, South Korea},
	keywords = {empirical study, static code analysis, vulnerability detection},
	pages = {544--555},
}

@book{noauthor_esse_2022,
	address = {New York, NY, USA},
	title = {{ESSE} '22: {Proceedings} of the 2022 {European} {Symposium} on {Software} {Engineering}},
	isbn = {978-1-4503-9730-8},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{alfayez_how_2023,
	title = {How {SonarQube}-identified technical debt is prioritized: {An} exploratory case study},
	volume = {156},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2023.107147},
	doi = {10.1016/j.infsof.2023.107147},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Alfayez, Reem and Winn, Robert and Alwehaibi, Wesam and Venson, Elaine and Boehm, Barry},
	month = apr,
	year = {2023},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Technical debt, Case study, Software, SonarQube, Technical debt prioritization},
}

@article{piantadosi_how_2020,
	title = {How does code readability change during software evolution?},
	volume = {25},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-020-09886-9},
	doi = {10.1007/s10664-020-09886-9},
	abstract = {Code reading is one of the most frequent activities in software maintenance. Such an activity aims at acquiring information from the code and, thus, it is a prerequisite for program comprehension: developers need to read the source code they are going to modify before implementing changes. As the code changes, so does its readability; however, it is not clear yet how code readability changes during software evolution. To understand how code readability changes when software evolves, we studied the history of 25 open source systems. We modeled code readability evolution by defining four states in which a file can be at a certain point of time (non-existing, other-name, readable, and unreadable). We used the data gathered to infer the probability of transitioning from one state to another one. In addition, we also manually checked a significant sample of transitions to compute the performance of the state-of-the-art readability prediction model we used to calculate the transition probabilities. With this manual analysis, we found that the tool correctly classifies all the transitions in the majority of the cases, even if there is a loss of accuracy compared to the single-version readability estimation. Our results show that most of the source code files are created readable. Moreover, we observed that only a minority of the commits change the readability state. Finally, we manually carried out qualitative analysis to understand what makes code unreadable and what developers do to prevent this. Using our results we propose some guidelines (i) to reduce the risk of code readability erosion and (ii) to promote best practices that make code readable.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Piantadosi, Valentina and Fierro, Fabiana and Scalabrino, Simone and Serebrenik, Alexander and Oliveto, Rocco},
	month = nov,
	year = {2020},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Software evolution, Mining software repositories, Code readability},
	pages = {5374--5412},
}

@article{caldeira_unveiling_2022,
	title = {Unveiling process insights from refactoring practices},
	volume = {81},
	issn = {0920-5489},
	url = {https://doi.org/10.1016/j.csi.2021.103587},
	doi = {10.1016/j.csi.2021.103587},
	number = {C},
	journal = {Comput. Stand. Interfaces},
	author = {Caldeira, João and Brito e Abreu, Fernando and Cardoso, Jorge and dos Reis, José Pereira},
	month = apr,
	year = {2022},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Refactoring practices, Software complexity, Software development process mining, Software process complexity},
}

@article{ardito_tool-based_2020,
	title = {A {Tool}-{Based} {Perspective} on {Software} {Code} {Maintainability} {Metrics}: {A} {Systematic} {Literature} {Review}},
	volume = {2020},
	issn = {1058-9244},
	url = {https://doi.org/10.1155/2020/8840389},
	doi = {10.1155/2020/8840389},
	abstract = {Software maintainability is a crucial property of software projects. It can be defined as the ease with which a software system or component can be modified to be corrected, improved, or adapted to its environment. The software engineering literature proposes many models and metrics to predict the maintainability of a software project statically. However, there is no common accordance with the most dependable metrics or metric suites to evaluate such nonfunctional property. The goals of the present manuscript are as follows: (i) providing an overview of the most popular maintainability metrics according to the related literature; (ii) finding what tools are available to evaluate software maintainability; and (iii) linking the most popular metrics with the available tools and the most common programming languages. To this end, we performed a systematic literature review, following Kitchenham’s SLR guidelines, on the most relevant scientific digital libraries. The SLR outcome provided us with 174 software metrics, among which we identified a set of 15 most commonly mentioned ones, and 19 metric computation tools available to practitioners. We found optimal sets of at most five tools to cover all the most commonly mentioned metrics. The results also highlight missing tool coverage for some metrics on commonly used programming languages and minimal coverage of metrics for newer or less popular programming languages. We consider these results valuable for researchers and practitioners who want to find the best selection of tools to evaluate the maintainability of their projects or to bridge the discussed coverage gaps for newer programming languages.},
	journal = {Sci. Program.},
	author = {Ardito, Luca and Coppola, Riccardo and Barbato, Luca and Verga, Diego and Briola, Daniela},
	month = jan,
	year = {2020},
	note = {Place: London, GBR
Publisher: Hindawi Limited},
}

@article{reis_fixing_2021,
	title = {Fixing vulnerabilities potentially hinders maintainability},
	volume = {26},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-021-10019-z},
	doi = {10.1007/s10664-021-10019-z},
	abstract = {Security is a requirement of utmost importance to produce high-quality software. However, there is still a considerable amount of vulnerabilities being discovered and fixed almost weekly. We hypothesize that developers affect the maintainability of their codebases when patching vulnerabilities. This paper evaluates the impact of patches to improve security on the maintainability of open-source software. Maintainability is measured based on the Better Code Hub’s model of 10 guidelines on a dataset, including 1300 security-related commits. Results show evidence of a trade-off between security and maintainability for 41.90\% of the cases, i.e., developers may hinder software maintainability. Our analysis shows that 38.29\% of patches increased software complexity and 37.87\% of patches increased the percentage of LOCs per unit. The implications of our study are that changes to codebases while patching vulnerabilities need to be performed with extra care; tools for patch risk assessment should be integrate into the CI/CD pipeline; computer science curricula needs to be updated; and, more secure programming languages are necessary.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Reis, Sofia and Abreu, Rui and Cruz, Luis},
	month = nov,
	year = {2021},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Software maintenance, Open-source software, Software security},
}

@article{furia_towards_2023,
	title = {Towards {Causal} {Analysis} of {Empirical} {Software} {Engineering} {Data}: {The} {Impact} of {Programming} {Languages} on {Coding} {Competitions}},
	volume = {33},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3611667},
	doi = {10.1145/3611667},
	abstract = {There is abundant observational data in the software engineering domain, whereas running large-scale controlled experiments is often practically impossible. Thus, most empirical studies can only report statistical correlations—instead of potentially more insightful and robust causal relations.To support analyzing purely observational data for causal relations and to assess any differences between purely predictive and causal models of the same data, this article discusses some novel techniques based on structural causal models (such as directed acyclic graphs of causal Bayesian networks). Using these techniques, one can rigorously express, and partially validate, causal hypotheses and then use the causal information to guide the construction of a statistical model that captures genuine causal relations—such that correlation does imply causation.We apply these ideas to analyzing public data about programmer performance in Code Jam, a large world-wide coding contest organized by Google every year. Specifically, we look at the impact of different programming languages on a participant’s performance in the contest. While the overall effect associated with programming languages is weak compared to other variables—regardless of whether we consider correlational or causal links—we found considerable differences between a purely associational and a causal analysis of the very same data.The takeaway message is that even an imperfect causal analysis of observational data can help answer the salient research questions more precisely and more robustly than with just purely predictive techniques—where genuine causal effects may be confounded.},
	number = {1},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Furia, Carlo A. and Torkar, Richard and Feldt, Robert},
	month = nov,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {empirical software engineering, Causality analysis, programming contests, statistical analysis},
}

@article{tosch_planalyzer_2021,
	title = {{PlanAlyzer}: assessing threats to the validity of online experiments},
	volume = {64},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3474385},
	doi = {10.1145/3474385},
	abstract = {Online experiments are an integral part of the design and evaluation of software infrastructure at Internet firms. To handle the growing scale and complexity of these experiments, firms have developed software frameworks for their design and deployment. Ensuring that the results of experiments in these frameworks are trustworthy—referred to as internal validity—can be difficult. Currently, verifying internal validity requires manual inspection by someone with substantial expertise in experimental design.We present the first approach for checking the internal validity of online experiments statically, that is, from code alone. We identify well-known problems that arise in experimental design and causal inference, which can take on unusual forms when expressed as computer programs: failures of randomization and treatment assignment, and causal sufficiency errors. Our analyses target PLANOUT, a popular framework that features a domain-specific language (DSL) to specify and run complex experiments. We have built PLANALYZER, a tool that checks PLANOUT programs for threats to internal validity, before automatically generating important data for the statistical analyses of a large class of experimental designs. We demonstrate PLANALYZER'S utility on a corpus of PLANOUT scripts deployed in production at Facebook, and we evaluate its ability to identify threats on a mutated subset of this corpus. PLANALYZER has both precision and recall of 92\% on the mutated corpus, and 82\% of the contrasts it generates match hand-specified data.},
	number = {9},
	journal = {Commun. ACM},
	author = {Tosch, Emma and Bakshy, Eytan and Berger, Emery D. and Jensen, David D. and Moss, J. Eliot B.},
	month = aug,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {108--116},
}

@article{nuez-varela_source_2017,
	title = {Source code metrics},
	volume = {128},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2017.03.044},
	doi = {10.1016/j.jss.2017.03.044},
	abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
	month = jun,
	year = {2017},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Systematic mapping study, Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics},
	pages = {164--197},
}

@article{olianas_sleepreplacer_2022,
	title = {{SleepReplacer}: a novel tool-based approach for replacing thread sleeps in selenium {WebDriver} test code},
	volume = {30},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-022-09596-z},
	doi = {10.1007/s11219-022-09596-z},
	abstract = {Assuring quality of web applications is fundamental, given their relevance in the today’s world. A possible way to reach this goal is through end-to-end (E2E) testing, an approach in which a web application is automatically tested by performing the actions that a user would do. With modern web applications (for example, single-page applications), it is of great importance to properly handle asynchronous calls in the test suite. In E2E Selenium WebDriver test suites, asynchronous calls are usually managed in two ways: using thread sleeps or explicit waits. The first is easier to use, but is inefficient and can lead to instability (also called flakiness, a problem often present in test suites that makes us\&nbsp;lose confidence in the testing phase), while the second is usually more efficient but harder to use because, if the correct kind of wait is not carefully selected, it can introduce flakiness too. To help Testers, who often opt for the first strategy, we present in this work a tool-based approach to automatically replace thread sleeps with explicit waits in an E2E Selenium WebDriver test suite without introducing new flakiness. We empirically validated our tool named SleepReplacer\&nbsp;on four different test suites, and we found that it can correctly replace in an\&nbsp;automatic way from 81 to 100\% of thread sleeps, leading to a significant reduction of the total execution time of the test suite (i.e., from 13 to 71\%).},
	number = {4},
	journal = {Software Quality Journal},
	author = {Olianas, Dario and Leotta, Maurizio and Ricca, Filippo},
	month = dec,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Explicit waits, Selenium WebDriver, Test code, Thread sleeps, Web testing},
	pages = {1089--1121},
}

@inproceedings{abidi_anti-patterns_2019,
	address = {New York, NY, USA},
	series = {{EuroPLop} '19},
	title = {Anti-patterns for multi-language systems},
	isbn = {978-1-4503-6206-1},
	url = {https://doi.org/10.1145/3361149.3364227},
	doi = {10.1145/3361149.3364227},
	abstract = {Multi-language systems are common nowadays because most of the systems are developed using components written in different programming languages. These systems could arise from three different reasons: (1) to leverage the strengths and take benefits of each language, (2) to reduce the cost by reusing code written in other languages, (3) to include and accommodate legacy code. However, they also introduce additional challenges, including the increase in the complexity and the need for proper interfaces and interactions between the different languages. To address these challenges, the software-engineering research community, as well as the industry, should describe and provide common guidelines, idioms, and patterns to support the development, maintenance, and evolution of these systems. These patterns are an effective means of improving the quality of multi-language systems. They capture good practices to adopt and bad practices to avoid. In order to help to improve the quality of multi-language systems, we analysed open-source systems, developers' documentation, bug reports, and programming language specifications to extract bad practices of multi-language systems usage. We encoded and cataloged these practices in the form of design anti-patterns. We report here six anti-patterns. These results could help not only researchers but also professional developers considering the use of more than one programming language.},
	booktitle = {Proceedings of the 24th {European} {Conference} on {Pattern} {Languages} of {Programs}},
	publisher = {Association for Computing Machinery},
	author = {Abidi, Mouna and Khomh, Foutse and Guéhéneuc, Yann-Gaël},
	year = {2019},
	note = {event-place: Irsee, Germany},
	keywords = {code analysis, multi-language systems, software quality, anti-patterns},
}

@article{stevenson_recognising_2018,
	title = {Recognising object-oriented software design quality: a practitioner-based questionnaire survey},
	volume = {26},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-017-9364-8},
	doi = {10.1007/s11219-017-9364-8},
	abstract = {Design quality is vital if software is to be maintainable. What practices do developers actually use to achieve design quality in their day-to-day work and which of these do they find most useful? To discover the extent to which practitioners concern themselves with object-oriented design quality and the approaches used when determining quality in practice, a questionnaire survey of 102 software practitioners, approximately half from the UK and the remainder from elsewhere around the world was used. Individual and peer experience are major contributors to design quality. Classic design guidelines, well-known lower level practices, tools and metrics all can also contribute positively to design quality. There is a potential relationship between testing practices and design quality. Inexperience, time pressures, novel problems, novel technology, and imprecise or changing requirements may have a negative impact on quality. Respondents with most experience are more confident in their design decisions, place more value on reviews by team leads and are more likely to rate design quality as very important. For practitioners, these results identify the techniques and tools that other practitioners find effective. For researchers, the results highlight a need for more work investigating the role of experience in the design process and the contribution experience makes to quality. There is also the potential for more in-depth studies of how practitioners are actually using design guidance, including Clean Code. Lastly, the potential relationship between testing practices and design quality merits further investigation.},
	number = {2},
	journal = {Software Quality Journal},
	author = {Stevenson, Jamie and Wood, Murray},
	month = jun,
	year = {2018},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Industry, Software, Design, Maintenance, Object-oriented, Practitioner, Quality, Questionnaire, Survey},
	pages = {321--365},
}

@inproceedings{amanatidis_developers_2018,
	address = {New York, NY, USA},
	series = {{TechDebt} '18},
	title = {The developer's dilemma: factors affecting the decision to repay code debt},
	isbn = {978-1-4503-5713-5},
	url = {https://doi.org/10.1145/3194164.3194174},
	doi = {10.1145/3194164.3194174},
	abstract = {The set of concepts collectively known as Technical Debt (TD) assume that software liabilities set up a context that can make a future change more costly or impossible; and therefore repaying the debt should be pursued. However, software developers often disagree with an automatically generated list of improvement suggestions, which they consider not fitting or important for their own code. To shed light into the reasons that drive developers to adopt or reject refactoring opportunities (i.e. TD repayment), we have performed an empirical study on the potential factors that affect the developers' decision to agree with the removal of a specific TD liability. The study has been addressed to the developers of four well-known open-source applications. To increase the response rate, a personalized assessment has first been sent to each developer, summarizing his/her own contribution to the TD of the corresponding project. Responds have been collected through a custom built web application that presented code fragments suffering from violations as identified by SonarQube along with information that could possibly affect their level of agreement to the importance of resolving an issue. These factors include data such as the frequency of past changes in the module under study, the number of bugs, the type and intensity of the violation, the level of involvement of the developer and whether he/she is a contributor in the corresponding project. Multivariate statistical analysis methods have been used to understand the importance and the underlying relationships among these factors and the results are expected to be useful for researchers and practitioners in TD Management.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Technical} {Debt}},
	publisher = {Association for Computing Machinery},
	author = {Amanatidis, Theodoros and Mittas, Nikolaos and Chatzigeorgiou, Alexander and Ampatzoglou, Apostolos and Angelis, Lefteris},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {empirical study, refactoring, technical debt management},
	pages = {62--66},
}

@inproceedings{guaman_systematic-oriented_2020,
	address = {Berlin, Heidelberg},
	title = {A {Systematic}-{Oriented} {Process} for {Tool} {Selection}: {The} {Case} of {Green} and {Technical} {Debt} {Tools} in {Architecture} {Reconstruction}},
	isbn = {978-3-030-64147-4},
	url = {https://doi.org/10.1007/978-3-030-64148-1_15},
	doi = {10.1007/978-3-030-64148-1_15},
	abstract = {Well-established methods in software engineering research, such as Systematic Literature Reviews, Systematic Mappings and Case Studies are effective research methods to explore emerging areas, since they are systematic and replicable, and produce reusable result avoiding bias. Frequently, software engineers have to evaluate and select CASE (Computer Aided Software Engineering) tools that address trending issues with a non-systematic and replicable processes. This work addresses this problem by tailoring the ISO/IEC 14102:2008 to a systematic-oriented process for the evaluation of software engineering CASE tools in order to embrace the advantages of software engineering systematic methods in the exploration of new areas or emerging issues. This tailored ISO/IEC 14102:2008 standard prescribes a process for the preparation, design and conduction of the software engineering CASE tools evaluation and selection. This process is founded in the application of systematic methods and the generation of a pre-established assets to ensure the reusability of knowledge. In this paper, this tailored process has been applied to address two great emerging concerns in architectural reconstruction: technical debt and energy consumption. As result of this adoption, this paper details the reporting analysis and the set of reusable assets that have been generated during the evaluation process. Specifically, this contribution presents a set of tables, statistics and a decision-making tree of the selected tools for technical debt and energy consumption analysis in architecture reconstruction.},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}: 21st {International} {Conference}, {PROFES} 2020, {Turin}, {Italy}, {November} 25–27, 2020, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Guamán, Daniel and Pérez, Jennifer and Garbajosa, Juan and Rodríguez, Germania},
	year = {2020},
	note = {event-place: Turin, Italy},
	keywords = {Technical debt, Architecture reconstruction, Green software, ISO/IEC14102:2008, Systematic process, Tools},
	pages = {237--253},
}

@article{yang_predictive_2022,
	title = {Predictive {Models} in {Software} {Engineering}: {Challenges} and {Opportunities}},
	volume = {31},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3503509},
	doi = {10.1145/3503509},
	abstract = {Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.},
	number = {3},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Yang, Yanming and Xia, Xin and Lo, David and Bi, Tingting and Grundy, John and Yang, Xiaohu},
	month = apr,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, machine learning, Predictive models, software engineering, survey},
}

@article{harrison_this_2019,
	title = {In this issue},
	volume = {27},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-019-09461-6},
	doi = {10.1007/s11219-019-09461-6},
	number = {3},
	journal = {Software Quality Journal},
	author = {Harrison, Rachel},
	month = sep,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	pages = {919--920},
}

@article{li_exploring_2022,
	title = {Exploring multi-programming-language commits and their impacts on software quality: {An} empirical study on {Apache} projects},
	volume = {194},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2022.111508},
	doi = {10.1016/j.jss.2022.111508},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Li, Zengyang and Qi, Xiaoxiao and Yu, Qinyi and Liang, Peng and Mo, Ran and Yang, Chen},
	month = dec,
	year = {2022},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Bug introduction, Bug proneness, Change complexity, Issue reopen, Multi-programming-language commit, Open source software},
}

@book{noauthor_sbes_2023,
	address = {New York, NY, USA},
	title = {{SBES} '23: {Proceedings} of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}},
	isbn = {979-8-4007-0787-2},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{alghamdi_how_2023,
	title = {How are websites used during development and what are the implications for the coding process?},
	volume = {205},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111803},
	doi = {10.1016/j.jss.2023.111803},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Alghamdi, Omar and Clinch, Sarah and Skeva, Rigina and Jay, Caroline},
	month = nov,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Stack Overflow, Computer science education, Human memory, Online code snippets, Problematic code, Professional practice},
}

@inproceedings{catolino_gender_2019,
	series = {{ICSE}-{SEIS} '19},
	title = {Gender diversity and women in software teams: how do they affect community smells?},
	url = {https://doi.org/10.1109/ICSE-SEIS.2019.00010},
	doi = {10.1109/ICSE-SEIS.2019.00010},
	abstract = {As social as software engineers are, there is a known and established gender imbalance in our community structures, regardless of their open- or closed-source nature. To shed light on the actual benefits of achieving such balance, this empirical study looks into the relations between such balance and the occurrence of community smells, that is, sub-optimal circumstances and patterns across the software organizational structure. Examples of community smells are Organizational Silo effects (overly disconnected sub-groups) or Lone Wolves (defiant community members). Results indicate that the presence of women generally reduces the amount of community smells. We conclude that women are instrumental to reducing community smells in software development teams.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society}},
	publisher = {IEEE Press},
	author = {Catolino, Gemma and Palomba, Fabio and Tamburri, Damian A. and Serebrenik, Alexander and Ferrucci, Filomena},
	year = {2019},
	note = {Place: Montreal, Quebec, Canada},
	keywords = {empirical study, community smells, gender balance, software organizational structures},
	pages = {11--20},
}

@article{silva_co-change_2019,
	title = {Co-change patterns: {A} large scale empirical study},
	volume = {152},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2019.03.014},
	doi = {10.1016/j.jss.2019.03.014},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Silva, Luciana L. and Valente, Marco Tulio and Maia, Marcelo A.},
	month = jun,
	year = {2019},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Co-change clusters, Co-change patterns, Modularity},
	pages = {196--214},
}

@article{mastrangelo_casting_2019,
	title = {Casting about in the dark: an empirical study of cast operations in {Java} programs},
	volume = {3},
	url = {https://doi.org/10.1145/3360584},
	doi = {10.1145/3360584},
	abstract = {The main goal of a static type system is to prevent certain kinds of errors from happening at run time. A type system is formulated as a set of constraints that gives any expression or term in a program a well-defined type. Yet mainstream programming languages are endowed with type systems that provide the means to circumvent their constraints through casting. We want to understand how and when developers escape the static type system to use dynamic typing. We empirically study how casting is used by developers in more than seven thousand Java projects. We find that casts are widely used (8.7\% of methods contain at least one cast) and that 50\% of casts we inspected are not guarded locally to ensure against potential run-time errors. To help us better categorize use cases and thus understand how casts are used in practice, we identify 25 cast-usage patterns—recurrent programming idioms using casts to solve a specific issue. This knowledge can be: (a) a recommendation for current and future language designers to make informed decisions (b) a reference for tool builders, e.g., by providing more precise or new refactoring analyses, (c) a guide for researchers to test new language features, or to carry out controlled programming experiments, and (d) a guide for developers for better practices.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	author = {Mastrangelo, Luis and Hauswirth, Matthias and Nystrom, Nathaniel},
	month = oct,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Java, cast, type safety},
}

@inproceedings{detofeno_technical_2021,
	address = {New York, NY, USA},
	series = {{SBQS} '21},
	title = {Technical {Debt} {Guild}: {When} experience and engagement improve {Technical} {Debt} {Management}},
	isbn = {978-1-4503-9553-3},
	url = {https://doi.org/10.1145/3493244.3493271},
	doi = {10.1145/3493244.3493271},
	abstract = {Efficient Technical Debt Management (TDM) requires specialized guidance so that decisions taken are oriented to add value to the business. Because it is a complex problem that involves several variables, TDM requires a systemic look that considers professionals' experiences from different specialties. Guilds have been a means technology companies using the Spotify methodology have found to unite specialized professionals around a common interest. This paper presents the experience in implementing a guild to support TDM's source code activities in a software development organization, using the action research method. The project lasted two years, and approximately 100 developers were involved in updating about 63,300 source-code files. The actions resulting from the TDM guild's efforts impacted the company's culture by introducing new development practices and standards. Besides, they positively influenced the quality of the artifact delivered by the developers. This study shows that, as the company acquires maturity in TDM, it increases the need for professionals dedicated to TDM's activities.},
	booktitle = {Proceedings of the {XX} {Brazilian} {Symposium} on {Software} {Quality}},
	publisher = {Association for Computing Machinery},
	author = {Detofeno, Thober and Reinehr, Sheila and Andreia, Malucelli},
	year = {2021},
	note = {event-place: Virtual Event, Brazil},
	keywords = {Community of Practice, Technical Debt, Technical Debt Guild, Technical Debt Management},
}

@article{di_grazia_code_2023,
	title = {Code {Search}: {A} {Survey} of {Techniques} for {Finding} {Code}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3565971},
	doi = {10.1145/3565971},
	abstract = {The immense amounts of source code provide ample challenges and opportunities during software development. To handle the size of code bases, developers commonly search for code, e.g., when trying to find where a particular feature is implemented or when looking for code examples to reuse. To support developers in finding relevant code, various code search engines have been proposed. This article surveys 30 years of research on code search, giving a comprehensive overview of challenges and techniques that address them. We discuss the kinds of queries that code search engines support, how to preprocess and expand queries, different techniques for indexing and retrieving code, and ways to rank and prune search results. Moreover, we describe empirical studies of code search in practice. Based on the discussion of prior work, we conclude the article with an outline of challenges and opportunities to be addressed in the future.},
	number = {11},
	journal = {ACM Comput. Surv.},
	author = {Di Grazia, Luca and Pradel, Michael},
	month = feb,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {survey, API, code retrieval, Code search, learning},
}

@inproceedings{abidi_behind_2019,
	address = {USA},
	series = {{CASCON} '19},
	title = {Behind the scenes: developers' perception of multi-language practices},
	abstract = {Nowadays, it is common to see software development teams combine multiple programming languages when developing a new software system. Most non-trivial software systems are developed using components written in different languages and technologies. This mixed-language programming approach allows developers to reuse existing code and libraries instead of implementing the code from scratch. It also allows them to leverage the strengths and benefits of each language. However, poor integration of different components of multi-language systems can lead to inconsistencies, dependency issues, and even bugs. To help developers make proper use of components and libraries written in different programming languages, researchers and practitioners have formulated multiple catalogs of design practices for multi-language systems. However, there is no evidence that these design practices are considered relevant by developers. Through this paper, we aim to assess the perception of developers, regarding the relevance of multi-language design practices and their impact on software quality. To achieve this goal, we extracted a set of good and bad practices related to multi-language systems that are discussed in the literature and developers' documentation. We surveyed 93 developers about their usage of the practices and their perception of the benefits of the practices for some quality. Our results show that the proposed design practices are not equally prevalent in the Industry. Among the practices frequently used by developers, some have a positive impact on the understandability, reusability, and simplicity of multi-language systems. We summarise the reported strengths and limitations of the studied design practices and provide guidelines for practitioners. We also formulate recommendations for researchers interested in improving the quality of multi-language systems.},
	booktitle = {Proceedings of the 29th {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Abidi, Mouna and Grichi, Manel and Khomh, Foutse},
	year = {2019},
	note = {event-place: Toronto, Ontario, Canada},
	keywords = {multi-language systems, survey, impact, issues, practices, quality attributes},
	pages = {72--81},
}

@article{antinyan_evaluating_2017,
	title = {Evaluating code complexity triggers, use of complexity measures and the influence of code complexity on maintenance time},
	volume = {22},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-017-9508-2},
	doi = {10.1007/s10664-017-9508-2},
	abstract = {Code complexity has been studied intensively over the past decades because it is a quintessential characterizer of code's internal quality. Previously, much emphasis has been put on creating code complexity measures and applying these measures in practical contexts. To date, most measures are created based on theoretical frameworks, which determine the expected properties that a code complexity measure should fulfil. Fulfilling the necessary properties, however, does not guarantee that the measure characterizes the code complexity that is experienced by software engineers. Subsequently, code complexity measures often turn out to provide rather superficial insights into code complexity. This paper supports the discipline of code complexity measurement by providing empirical insights into the code characteristics that trigger complexity, the use of code complexity measures in industry, and the influence of code complexity on maintenance time. Results of an online survey, conducted in seven companies and two universities with a total of 100 respondents, show that among several code characteristics, two substantially increase code complexity, which subsequently have a major influence on the maintenance time of code. Notably, existing code complexity measures are poorly used in industry.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Antinyan, Vard and Staron, Miroslaw and Sandberg, Anna},
	month = dec,
	year = {2017},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Maintainability, Survey, Complexity, Internal quality, Measure},
	pages = {3057--3087},
}

@inproceedings{cruz_performance-based_2017,
	series = {{MOBILESoft} '17},
	title = {Performance-based guidelines for energy efficient mobile applications},
	isbn = {978-1-5386-2669-6},
	url = {https://doi.org/10.1109/MOBILESoft.2017.19},
	doi = {10.1109/MOBILESoft.2017.19},
	abstract = {Mobile and wearable devices are nowadays the de facto personal computers, while desktop computers are becoming less popular. Therefore, it is important for companies to deliver efficient mobile applications. As an example, Google has published a set of best practices to optimize the performance of Android applications. However, these guidelines fall short to address energy consumption. As mobile software applications operate in resource-constrained environments, guidelines to build energy efficient applications are of utmost importance. In this paper, we studied whether or not eight best performance-based practices have an impact on the energy consumed by Android applications. In an experimental study with six popular mobile applications, we observed that the battery of the mobile device can last up to approximately an extra hour if the applications are developed with energy-aware practices. This work paves the way for a set of guidelines for energy-aware automatic refactoring techniques.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Mobile} {Software} {Engineering} and {Systems}},
	publisher = {IEEE Press},
	author = {Cruz, Luis and Abreu, Rui},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	keywords = {mobile computing, anti patterns, green computing},
	pages = {46--57},
}

@book{noauthor_soict_2022,
	address = {New York, NY, USA},
	title = {{SoICT} '22: {Proceedings} of the 11th {International} {Symposium} on {Information} and {Communication} {Technology}},
	isbn = {978-1-4503-9725-4},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{fabry_aspectj_2016,
	title = {{AspectJ} code analysis and verification with {GASR}},
	volume = {117},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2016.04.014},
	doi = {10.1016/j.jss.2016.04.014},
	abstract = {We argue for a general-purpose source code analysis tool that is aware of aspects.We present the logic program querying tool GASR, the first such tool.We discuss its implementation along with its library of logical predicates.We show how it can automatically verify previously published aspectual assumptions. Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Fabry, Johan and De Roover, Coen and Noguera, Carlos and Zschaler, Steffen and Rashid, Awais and Jonckers, Viviane},
	month = jul,
	year = {2016},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Source code analysis, Aspect oriented programming, Logic program querying},
	pages = {528--544},
}

@inproceedings{wu_accelerating_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Accelerating {Build} {Dependency} {Error} {Detection} via {Virtual} {Build}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3556930},
	doi = {10.1145/3551349.3556930},
	abstract = {Build scripts play an important role in transforming the source code into executable artifacts. However, the development of build scripts is typically error-prone. As one kind of the most prevalent errors in build scripts, the dependency-related errors, including missing dependencies and redundant dependencies, draw the attention of many researchers. A variety of build dependency analysis techniques have been proposed to tackle them. Unfortunately, most of these techniques, even the state-of-the-art ones, suffer from efficiency issues due to the expensive cost of monitoring the complete build process to build dynamic dependencies. Especially for large-scale projects, such the cost would not be affordable. This work presents a new technique to accelerate the build dependency error detection by reducing the time cost of the build monitoring. Our key idea is to reduce the size of a program while still preserving the same dynamic dependencies as the original one. Building the reduced program does not generate a real software artifact, but it yields the same list of dependency errors and meanwhile speeds up the process. We implement the tool VirtualBuild and evaluate it on real-world projects. It is shown that it detects all the dependency errors found by existing tools at a low cost. Compared with the state-of-the-art technique, VirtualBuild\&nbsp; accelerates the build process by 8.74 times, and improves the efficiency of error detection by 6.13 times on average. Specifically, in the large-scale project LLVM that contains 5.67 MLoC, VirtualBuild\&nbsp; reduces the overall time from over four hours to 38.63\&nbsp; minutes.},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Rongxin and Chen, Minglei and Wang, Chengpeng and Fan, Gang and Qiu, Jiguang and Zhang, Charles},
	year = {2023},
	note = {event-place: Rochester, MI, USA},
	keywords = {build maintenance, build system, Dependency error},
}

@article{li_enhancing_2024,
	title = {Enhancing code summarization with action word prediction},
	volume = {563},
	issn = {0925-2312},
	url = {https://doi.org/10.1016/j.neucom.2023.126777},
	doi = {10.1016/j.neucom.2023.126777},
	number = {C},
	journal = {Neurocomput.},
	author = {Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Zhou, Ziyi and Huang, Zijie},
	month = jan,
	year = {2024},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Action word prediction, Code summarization, Deep learning, Multi-task learning},
}

@article{aranega_rotten_2021,
	title = {Rotten green tests in {Java}, {Pharo} and {Python}: {An} empirical study},
	volume = {26},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-021-10016-2},
	doi = {10.1007/s10664-021-10016-2},
	abstract = {Rotten Green Tests are tests that pass, but not because the assertions they contain are true: a rotten test passes because some or all of its assertions are not actually executed. The presence of a rotten green test is a test smell, and a bad one, because the existence of a test gives us false confidence that the code under test is valid, when in fact that code may not have been tested at all. This article reports on an empirical evaluation of the tests in a corpus of projects found in the wild. We selected approximately one hundred mature projects written in each of Java, Pharo, and Python. We looked for rotten green tests in each project, taking into account test helper methods, inherited helpers, and trait composition. Previous work has shown the presence of rotten green tests in Pharo projects; the results reported here show that they are also present in Java and Python projects, and that they fall into similar categories. Furthermore, we found code bugs that were hidden by rotten tests in Pharo and Python. We also discuss two test smells —missed fail and missed skip —that arise from the misuse of testing frameworks, and which we observed in tests written in all three languages.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Aranega, Vincent and Delplanque, Julien and Martinez, Matias and Black, Andrew P. and Ducasse, Stéphane and Etien, Anne and Fuhrman, Christopher and Polito, Guillermo},
	month = nov,
	year = {2021},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Empirical study, Software quality, Rotten Green Tests, Testing},
}

@inproceedings{chen_why_2017,
	series = {{ICSE}-{C} '17},
	title = {Why is it important to measure maintainability, and what are the best ways to do it?},
	isbn = {978-1-5386-1589-8},
	url = {https://doi.org/10.1109/ICSE-C.2017.75},
	doi = {10.1109/ICSE-C.2017.75},
	abstract = {Software both enables and challenges companies to keep up with the increasing rate of changes in technology, competition, organizations, and the marketplace. Although much of the software engineering literature is focused on software development, Koskinens 2009 survey found that 75–90\% of business and command \&amp; control software and 50–80\% of cyber-physical system software costs are incurred during maintenance. Knowing how maintainable a piece of software helps in the following situations:• Deciding whether and what parts of existing software to reuse;• Deciding whether to maintain or redevelop software components;• Estimating the amount of effort involved in maintaining software components;• Determining award fees, acceptance criteria for Turnkey software;• Providing evaluation data for reuse repositories;• Saving a significant fraction of total software ownership costs.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Software} {Engineering} {Companion}},
	publisher = {IEEE Press},
	author = {Chen, Celia and Alfayez, Reem and Srisopha, Kamonphop and Boehm, Barry and Shi, Lin},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	pages = {377--378},
}

@inproceedings{fan_escaping_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Escaping dependency hell: finding build dependency errors with the unified dependency graph},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397388},
	doi = {10.1145/3395363.3397388},
	abstract = {Modern software projects rely on build systems and build scripts to assemble executable artifacts correctly and efficiently. However, developing build scripts is error-prone. Dependency-related errors in build scripts, mainly including missing dependencies and redundant dependencies, are common in various kinds of software projects. These errors lead to build failures, incorrect build results or poor performance in incremental or parallel builds. To detect such errors, various techniques are proposed and suffer from low efficiency and high false positive problems, due to the deficiency of the underlying dependency graphs. In this work, we design a new dependency graph, the unified dependency graph (UDG), which leverages both static and dynamic information to uniformly encode the declared and actual dependencies between build targets and files. The construction of UDG facilitates the efficient and precise detection of dependency errors via simple graph traversals. We implement the proposed approach as a tool, VeriBuild, and evaluate it on forty-two well-maintained open-source projects. The experimental results show that, without losing precision, VeriBuild incurs 58.2\% less overhead than the state-of-the-art approach. By the time of writing, 398 detected dependency issues have been confirmed by the developers.},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Fan, Gang and Wang, Chengpeng and Wu, Rongxin and Xiao, Xiao and Shi, Qingkai and Zhang, Charles},
	year = {2020},
	note = {event-place: Virtual Event, USA},
	keywords = {build maintenance, build tools, dependency verification},
	pages = {463--474},
}

@book{noauthor_sbsi_2023,
	address = {New York, NY, USA},
	title = {{SBSI} '23: {Proceedings} of the {XIX} {Brazilian} {Symposium} on {Information} {Systems}},
	isbn = {979-8-4007-0759-9},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{amanatidis_who_2017,
	address = {New York, NY, USA},
	series = {{XP} '17},
	title = {Who is producing more technical debt? a personalized assessment of {TD} principal},
	isbn = {978-1-4503-5264-2},
	url = {https://doi.org/10.1145/3120459.3120464},
	doi = {10.1145/3120459.3120464},
	abstract = {Technical debt (TD) impedes software projects by reducing the velocity of development teams during software evolution. Although TD is usually assessed on either the entire system or on individual software artifacts, it is the actual craftsmanship of developers that causes the accumulation of TD. In the light of extremely high maintenance costs, efficient software project management cannot occur without recognizing the relation between developer characteristics and the tendency to evoke violations that lead to TD. In this paper, we investigate three research questions related to the distribution of TD among the developers of a software project, the types of violations caused by each developer and the relation between developers' maturity and the tendency to accumulate TD. The study has been performed on four widely employed PHP open-source projects. All developers' personal characteristics have been anonymized in the study.},
	booktitle = {Proceedings of the {XP2017} {Scientific} {Workshops}},
	publisher = {Association for Computing Machinery},
	author = {Amanatidis, Theodoros and Chatzigeorgiou, Alexander and Ampatzoglou, Apostolos and Stamelos, Ioannis},
	year = {2017},
	note = {event-place: Cologne, Germany},
	keywords = {software maintenance, technical debt, project management},
}

@article{sun_energy_2022,
	title = {Energy inefficiency diagnosis for {Android} applications: a literature review},
	volume = {17},
	issn = {2095-2228},
	url = {https://doi.org/10.1007/s11704-021-0532-4},
	doi = {10.1007/s11704-021-0532-4},
	abstract = {Android applications are becoming increasingly powerful in recent years. While their functionality is still of paramount importance to users, the energy efficiency of these applications is also gaining more and more attention. Researchers have discovered various types of energy defects in Android applications, which could quickly drain the battery power of mobile devices. Such defects not only cause inconvenience to users, but also frustrate Android developers as diagnosing the energy inefficiency of a software product is a non-trivial task. In this work, we perform a literature review to understand the state of the art of energy inefficiency diagnosis for Android applications. We identified 55 research papers published in recent years and classified existing studies from four different perspectives, including power estimation method, hardware component, types of energy defects, and program analysis approach. We also did a cross-perspective analysis to summarize and compare our studied techniques. We hope that our review can help structure and unify the literature and shed light on future research, as well as drawing developers’ attention to build energy-efficient Android applications.},
	number = {1},
	journal = {Front. Comput. Sci.},
	author = {Sun, Yuxia and Fang, Jiefeng and Chen, Yanjia and Liu, Yepang and Chen, Zhao and Guo, Song and Chen, Xinkai and Tan, Ziyuan},
	month = aug,
	year = {2022},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Android applications, energy defects, energy inefficiency diagnosis},
}

@inproceedings{tony_conversational_2022,
	address = {New York, NY, USA},
	series = {{EASE} '22},
	title = {Conversational {DevBots} for {Secure} {Programming}: {An} {Empirical} {Study} on {SKF} {Chatbot}},
	isbn = {978-1-4503-9613-4},
	url = {https://doi.org/10.1145/3530019.3535307},
	doi = {10.1145/3530019.3535307},
	abstract = {Conversational agents or chatbots are widely investigated and used across different fields including healthcare, education, and marketing. Still, the development of chatbots for assisting secure coding practices is in its infancy. In this paper, we present the results of an empirical study on SKF chatbot, a software-development bot (DevBot) designed to answer queries about software security. To the best of our knowledge, SKF chatbot is one of the very few of its kind, thus a representative instance of conversational DevBots aiding secure software development. In this study, we collect and analyse empirical evidence on the effectiveness of SKF chatbot, while assessing the needs and expectations of its users (i.e., software developers). Furthermore, we explore the factors that may hinder the elaboration of more sophisticated conversational security DevBots and identify features for improving the efficiency of state-of-the-art solutions. All in all, our findings provide valuable insights pointing towards the design of more context-aware and personalized conversational DevBots for security engineering.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tony, Catherine and Balasubramanian, Mohana and Díaz Ferreyra, Nicolás E. and Scandariato, Riccardo},
	year = {2022},
	note = {event-place: Gothenburg, Sweden},
	keywords = {Empirical study, DevBot, Secure programming, Software Chatbot},
	pages = {276--281},
}

@inproceedings{basak_what_2023,
	series = {{ICSE} '23},
	title = {What {Challenges} {Do} {Developers} {Face} about {Checked}-in {Secrets} in {Software} {Artifacts}?},
	isbn = {978-1-6654-5701-9},
	url = {https://doi.org/10.1109/ICSE48619.2023.00141},
	doi = {10.1109/ICSE48619.2023.00141},
	abstract = {Throughout 2021, GitGuardian's monitoring of public GitHub repositories revealed a two-fold increase in the number of secrets (database credentials, API keys, and other credentials) exposed compared to 2020, accumulating more than six million secrets. To our knowledge, the challenges developers face to avoid checked-in secrets are not yet characterized. The goal of our paper is to aid researchers and tool developers in understanding and prioritizing opportunities for future research and tool automation for mitigating checked-in secrets through an empirical investigation of challenges and solutions related to checked-in secrets. We extract 779 questions related to checked-in secrets on Stack Exchange and apply qualitative analysis to determine the challenges and the solutions posed by others for each of the challenges. We identify 27 challenges and 13 solutions. The four most common challenges, in ranked order, are: (i) store/version of secrets during deployment; (ii) store/version of secrets in source code; (iii) ignore/hide of secrets in source code; and (iv) sanitize VCS history. The three most common solutions, in ranked order, are: (i) move secrets out of source code/version control and use template config file; (ii) secret management in deployment; and (iii) use local environment variables. Our findings indicate that the same solution has been mentioned to mitigate multiple challenges. However, our findings also identify an increasing trend in questions lacking accepted solutions substantiating the need for future research and tool automation on managing secrets.},
	booktitle = {Proceedings of the 45th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Basak, Setu Kumar and Neil, Lorenzo and Reaves, Bradley and Williams, Laurie},
	year = {2023},
	note = {Place: Melbourne, Victoria, Australia},
	pages = {1635--1647},
}

@inproceedings{li_comparison_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {Comparison and {Evaluation} on {Static} {Application} {Security} {Testing} ({SAST}) {Tools} for {Java}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3616262},
	doi = {10.1145/3611643.3616262},
	abstract = {Static application security testing (SAST) takes a significant role in the software development life cycle (SDLC). However, it is challenging to comprehensively evaluate the effectiveness of SAST tools to determine which is the better one for detecting vulnerabilities. In this paper, based on well-defined criteria, we first selected seven free or open-source SAST tools from 161 existing tools for further evaluation. Owing to the synthetic and newly-constructed real-world benchmarks, we evaluated and compared these SAST tools from different and comprehensive perspectives such as effectiveness, consistency, and performance. While SAST tools perform well on synthetic benchmarks, our results indicate that only 12.7\% of real-world vulnerabilities can be detected by the selected tools. Even combining the detection capability of all tools, most vulnerabilities (70.9\%) remain undetected, especially those beyond resource control and insufficiently neutralized input/output vulnerabilities. The fact is that although they have already built the corresponding detecting rules and integrated them into their capabilities, the detection result still did not meet the expectations. All useful findings unveiled in our comprehensive study indeed help to provide guidance on tool development, improvement, evaluation, and selection for developers, researchers, and potential users.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Li, Kaixuan and Chen, Sen and Fan, Lingling and Feng, Ruitao and Liu, Han and Liu, Chengwei and Liu, Yang and Chen, Yixiang},
	year = {2023},
	note = {event-place: San Francisco, CA, USA},
	keywords = {Empirical study, Benchmarks, Static application security testing},
	pages = {921--933},
}

@article{sotiropoulos_additional_2022,
	title = {The additional testsuite framework: facilitating software testing and test management},
	volume = {17},
	issn = {1476-1289},
	url = {https://doi.org/10.1504/ijwet.2022.127876},
	doi = {10.1504/ijwet.2022.127876},
	abstract = {In this paper, we present the additional testsuite framework, a novel test suite management approach, which provides structures and instrumentation for the creation, maintenance, evolution and use of test suites for software programs. In particular, the tests can be maintained in a centralised repository, and are developed and maintained independently of specific versions of the associated software. Through the use of annotations, tests are categorised and distributed to the desired versions of the software. The presented framework also supports test-based development, dynamic/selective program builds, feature-based builds, testing in different environments and source code analysis. The additional testsuite framework concept has been implemented and extensively evaluated, with the test cases notably including the JBoss EAP CE and OpenLiberty servers.},
	number = {3},
	journal = {Int. J. Web Eng. Technol.},
	author = {Sotiropoulos, Panagiotis and Vassilakis, Costas},
	month = jan,
	year = {2022},
	note = {Place: Geneva 15, CHE
Publisher: Inderscience Publishers},
	keywords = {additional testsuite framework, dynamic testing, feature-based testing, internet application testing, multiple environment testing, multiversion testing, test program repository, test-driven development, testsuite management},
	pages = {296--334},
}

@inproceedings{nguyen_stitch_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {A {Stitch} in {Time}: {Supporting} {Android} {Developers} in {WritingSecure} {Code}},
	isbn = {978-1-4503-4946-8},
	url = {https://doi.org/10.1145/3133956.3133977},
	doi = {10.1145/3133956.3133977},
	abstract = {Despite security advice in the official documentation and an extensive body of security research about vulnerabilities and exploits, many developers still fail to write secure Android applications. Frequently, Android developers fail to adhere to security best practices, leaving applications vulnerable to a multitude of attacks. We point out the advantage of a low-time-cost tool both to teach better secure coding and to improve app security. Using the FixDroid IDE plug-in, we show that professional and hobby app developers can work with and learn from an in-environment tool without it impacting their normal work; and by performing studies with both students and professional developers, we identify key UI requirements and demonstrate that code delivered with such a tool by developers previously inexperienced in security contains significantly less security problems. Perfecting and adding such tools to the Android development environment is an essential step in getting both security and privacy for the next generation of apps.},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Duc Cuong and Wermke, Dominik and Acar, Yasemin and Backes, Michael and Weir, Charles and Fahl, Sascha},
	year = {2017},
	note = {event-place: Dallas, Texas, USA},
	keywords = {android security, cryptographic api, support developers, usable security},
	pages = {1065--1077},
}

@article{spadini_mock_2019,
	title = {Mock objects for testing java systems},
	volume = {24},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-018-9663-0},
	doi = {10.1007/s10664-018-9663-0},
	abstract = {When testing software artifacts that have several dependencies, one has the possibility of either instantiating these dependencies or using mock objects to simulate the dependencies' expected behavior. Even though recent quantitative studies showed that mock objects are widely used both in open source and proprietary projects, scientific knowledge is still lacking on how and why practitioners use mocks. An empirical understanding of the situations where developers have (and have not) been applying mocks, as well as the impact of such decisions in terms of coupling and software evolution can be used to help practitioners adapt and improve their future usage. To this aim, we study the usage of mock objects in three OSS projects and one industrial system. More specifically, we manually analyze more than 2,000 mock usages. We then discuss our findings with developers from these systems, and identify practices, rationales, and challenges. These results are supported by a structured survey with more than 100 professionals. Finally, we manually analyze how the usage of mock objects in test code evolve over time as well as the impact of their usage on the coupling between test and production code. Our study reveals that the usage of mocks is highly dependent on the responsibility and the architectural concern of the class. Developers report to frequently mock dependencies that make testing difficult (e.g., infrastructure-related dependencies) and to not mock classes that encapsulate domain concepts/rules of the system. Among the key challenges, developers report that maintaining the behavior of the mock compatible with the behavior of original class is hard and that mocking increases the coupling between the test and the production code. Their perceptions are confirmed by our data, as we observed that mocks mostly exist since the very first version of the test class, and that they tend to stay there for its whole lifetime, and that changes in production code often force the test code to also change.},
	number = {3},
	journal = {Empirical Softw. Engg.},
	author = {Spadini, Davide and Aniche, Maurício and Bruntink, Magiel and Bacchelli, Alberto},
	month = jun,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Empirical software engineering, Mocking practices, Mockito, Software testing},
	pages = {1461--1498},
}

@inproceedings{li_vulnerability_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {On the vulnerability proneness of multilingual code},
	isbn = {978-1-4503-9413-0},
	url = {https://doi.org/10.1145/3540250.3549173},
	doi = {10.1145/3540250.3549173},
	abstract = {Software construction using multiple languages has long been a norm, yet it is still unclear if multilingual code construction has significant security implications and real security consequences. This paper aims to address this question with a large-scale study of popular multi-language projects on GitHub and their evolution histories, enabled by our novel techniques for multilingual code characterization. We found statistically significant associations between the proneness of multilingual code to vulnerabilities (in general and of specific categories) and its language selection. We also found this association is correlated with that of the language interfacing mechanism, not that of individual languages. We validated our statistical findings with in-depth case studies on actual vulnerabilities, explained via the mechanism and language selection. Our results call for immediate actions to assess and defend against multilingual vulnerabilities, for which we provide practical recommendations.},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Li, Wen and Li, Li and Cai, Haipeng},
	year = {2022},
	note = {event-place: Singapore, Singapore},
	keywords = {cross-language vulnerability, language interfacing, multi-language software, multilingual code, regression analysis, software security},
	pages = {847--859},
}

@inproceedings{boussaa_automatic_2016,
	address = {New York, NY, USA},
	series = {{GPCE} 2016},
	title = {Automatic non-functional testing of code generators families},
	isbn = {978-1-4503-4446-3},
	url = {https://doi.org/10.1145/2993236.2993256},
	doi = {10.1145/2993236.2993256},
	abstract = {The intensive use of generative programming techniques provides an elegant engineering solution to deal with the heterogeneity of platforms and technological stacks. The use of domain-specific languages for example, leads to the creation of numerous code generators that automatically translate highlevel system specifications into multi-target executable code. Producing correct and efficient code generator is complex and error-prone. Although software designers provide generally high-level test suites to verify the functional outcome of generated code, it remains challenging and tedious to verify the behavior of produced code in terms of non-functional properties. This paper describes a practical approach based on a runtime monitoring infrastructure to automatically check the potential inefficient code generators. This infrastructure, based on system containers as execution platforms, allows code-generator developers to evaluate the generated code performance. We evaluate our approach by analyzing the performance of Haxe, a popular high-level programming language that involves a set of cross-platform code generators. Experimental results show that our approach is able to detect some performance inconsistencies that reveal real issues in Haxe code generators.},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {Boussaa, Mohamed and Barais, Olivier and Baudry, Benoit and Sunyé, Gerson},
	year = {2016},
	note = {event-place: Amsterdam, Netherlands},
	keywords = {testing, code generator, code quality, non-functional properties},
	pages = {202--212},
}

@inproceedings{khan_automatic_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Automatic {Code} {Documentation} {Generation} {Using} {GPT}-3},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3559548},
	doi = {10.1145/3551349.3559548},
	abstract = {Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2\% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Khan, Junaed Younus and Uddin, Gias},
	year = {2023},
	note = {event-place: Rochester, MI, USA},
	keywords = {code documentation, GPT-3, Machine Learning.},
}

@inproceedings{detofeno_priortd_2022,
	address = {New York, NY, USA},
	series = {{SBES} '22},
	title = {{PriorTD}: {A} {Method} for {Prioritization} {Technical} {Debt}},
	isbn = {978-1-4503-9735-3},
	url = {https://doi.org/10.1145/3555228.3555238},
	doi = {10.1145/3555228.3555238},
	abstract = {Advances in Technical Debt (TD) have enabled the identification of numerous immature artifacts and their impacts on software maintenance and evolution. However, organizations can identify and understand the impacts of TD, but challenges arise when deciding which TD item should be paid for and which should be deferred. These decisions are influenced by many contextual factors that cannot be determined by considering source code alone. Prioritization strategies must consider the business objectives, the scope of the projects, and the needs of the development teams. This paper presents the PriorTD method, which prioritizes TD from the most relevant source code for the project and provides scenarios to meet business needs. It was built throughout two cycles of action research, over two years and eight months, in a software development company. PriorTD analyzes the source code and TD from the perspective of the importance of the source code for the project, team, and managers and provides guidance and support for all involved to understand the decisions related to the prioritization of TD.},
	booktitle = {Proceedings of the {XXXVI} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Detofeno, Thober and Malucelli, Andreia and Reinehr, Sheila},
	year = {2022},
	note = {event-place: Virtual Event, Brazil},
	keywords = {Technical Debt, Technical Debt Management, Software Refactoring, Technical Debt Prioritization},
	pages = {230--240},
}

@inproceedings{naiakshina_if_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {"{If} you want, {I} can store the encrypted password": {A} {Password}-{Storage} {Field} {Study} with {Freelance} {Developers}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300370},
	doi = {10.1145/3290605.3300370},
	abstract = {In 2017 and 2018, Naiakshina et al. (CCS'17, SOUPS'18) studied in a lab setting whether computer science students need to be told to write code that stores passwords securely. The authors' results showed that, without explicit prompting, none of the students implemented secure password storage. When asked about this oversight, a common answer was that they would have implemented secure storage - if they were creating code for a company. To shed light on this possible confusion, we conducted a mixed-methods field study with developers. We hired freelance developers online and gave them a similar password storage task followed by a questionnaire to gain additional insights into their work. From our research, we offer two contributions. First of all, we reveal that, similar to the students, freelancers do not store passwords securely unless prompted, they have misconceptions about secure password storage, and they use outdated methods. Secondly, we discuss the methodological implications of using freelancers and students in developer studies.},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Naiakshina, Alena and Danilova, Anastasia and Gerlitz, Eva and von Zezschwitz, Emanuel and Smith, Matthew},
	year = {2019},
	note = {event-place: Glasgow, Scotland Uk},
	keywords = {developer password study, field study, security developer study, usable security and privacy},
	pages = {1--12},
}

@inproceedings{benats_empirical_2021,
	address = {Berlin, Heidelberg},
	title = {An {Empirical} {Study} of ({Multi}-) {Database} {Models} in {Open}-{Source} {Projects}},
	isbn = {978-3-030-89021-6},
	url = {https://doi.org/10.1007/978-3-030-89022-3_8},
	doi = {10.1007/978-3-030-89022-3_8},
	abstract = {Managing data-intensive systems has long been recognized as an expensive and error-prone process. This is mainly due to the often implicit consistency relationships that hold between applications and their database. As new technologies emerged for specialized purposes (e.g., graph databases, document stores), the joint use of database models has also become popular. There are undeniable benefits of such multi-database models where developers combine various technologies. However, the side effects on design, querying, and maintenance are not well-known yet. In this paper, we study multi-database models in software systems by mining major open-source repositories. We consider four years of history, from 2017 to 2020, of a total number of 40,609 projects with databases. Our results confirm the emergence of hybrid data-intensive systems as we found (multi-) database models (e.g., relational and non-relational) used together in 16\% of all database-dependent projects. One percent of the systems added, deleted, or changed a database during the four years. The majority (62\%) of these systems had a single database before becoming hybrid, and another significant part (19\%) became “mono-database” after initially using multiple databases. We examine the evolution of these systems to understand the rationale of the design choices of the developers. Our study aims to guide future research towards new challenges posed by those emerging data management architectures.},
	booktitle = {Conceptual {Modeling}: 40th {International} {Conference}, {ER} 2021, {Virtual} {Event}, {October} 18–21, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Benats, Pol and Gobert, Maxime and Meurice, Loup and Nagy, Csaba and Cleve, Anthony},
	year = {2021},
	keywords = {Empirical study, Data models, Open-source projects},
	pages = {87--101},
}

@article{sheikhaei_study_2023,
	title = {A study of update request comments in {Stack} {Overflow} answer posts},
	volume = {198},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2022.111590},
	doi = {10.1016/j.jss.2022.111590},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Sheikhaei, Mohammad Sadegh and Tian, Yuan and Wang, Shaowei},
	month = apr,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Answer quality, Classification, Commenting, Crowd-sourced knowledge sharing, Knowledge maintenance and update, Stack overflow},
}

@inproceedings{alfayez_exploratory_2018,
	address = {New York, NY, USA},
	series = {{TechDebt} '18},
	title = {An exploratory study on the influence of developers in technical debt},
	isbn = {978-1-4503-5713-5},
	url = {https://doi.org/10.1145/3194164.3194165},
	doi = {10.1145/3194164.3194165},
	abstract = {Software systems are often developed by many developers who have a varying range of skills and habits. These developers have a big impact on software quality. Understanding how different developers and developer characteristics impact the quality of a software is crucial to properly deploy human resources and help managers improve quality outcomes which is essential for software systems success. Addressing this concern, we conduct a study on how different developers and developer characteristics such as developer seniority in a system, frequency of commits, and interval between commits relate to Technical Debt (TD). We performed a large-scale analysis on 19,088 commits from 38 Apache Java systems and applied multiple statistical analysis tests to evaluate our hypotheses. Our empirical evaluation suggests that developers unequally increase and decrease TD, a developer seniority in a software system and frequency of commits are negatively correlated with the TD the developer induces, and a developer commit interval has a positive correlation with the TD the developer induces.},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Technical} {Debt}},
	publisher = {Association for Computing Machinery},
	author = {Alfayez, Reem and Behnamghader, Pooyan and Srisopha, Kamonphop and Boehm, Barry},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {software maintenance, technical debt, software engineering, project management, developer contribution, developer experience, human factors},
	pages = {1--10},
}

@inproceedings{yamashita_assembling_2014,
	address = {New York, NY, USA},
	series = {{EASE} '14},
	title = {Assembling multiple-case studies: potential, principles and practical considerations},
	isbn = {978-1-4503-2476-2},
	url = {https://doi.org/10.1145/2601248.2601286},
	doi = {10.1145/2601248.2601286},
	abstract = {Case studies are a research method aimed at holistically analyzing a phenomenon in its context. Despite the fact that they cannot be used to answer the same precise research questions as, e.g., can be addressed by controlled experiments, case studies can cope much better with situations having several variables of interest, multiple sources of evidence, or rich contexts that cannot be controlled or isolated. As such, case studies are a promising instrument to study the complex phenomena at play in Software Engineering.However, the use of case studies as research methodology entails certain challenges. We argue that one of the biggest challenges is the case selection bias when conducting multiple-case studies. In practice, cases are frequently selected based on their availability, without appropriate control over moderator factors. This hinders the level of comparability across cases, leading to internal validity issues.In this paper, we discuss the notion of assembling cases as a plausible alternative to selecting cases to overcome the selection bias problem when conducting multiple-case studies. In addition, we present and discuss our experiences from applying this approach in a study designed to investigate the impact of software design on maintainability.},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yamashita, Aiko and Moonen, Leon},
	year = {2014},
	note = {event-place: London, England, United Kingdom},
	keywords = {case study, empirical studies, internal validity, methodology},
}

@article{akiki_var_2019,
	title = {To var or not to var: how do {C}\# developers use and misuse implicit and explicit typing?},
	volume = {27},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-018-9426-6},
	doi = {10.1007/s11219-018-9426-6},
	abstract = {When implicit typing with the “var” keyword was introduced into C\#, it prompted contradictory opinions among developers. This paper starts by explaining the difference between implicit and explicit typing and then provides an overview of developers’ opinions and guidelines that are available online. This paper then reports on the results of a study that investigated how C\# developers use and misuse implicit and explicit typing. This study involved analyzing the source code of 10 different open-source software projects including more than 16,500,000 lines of code and more than 930,000 variables. This study investigated to what extent developers use a form of typing that affects the readability of a variable’s type and the length of its declaration. It also investigated whether or not there is an adoption of a consistent set of guidelines in general and across each software project. A tool called “Code Analysis and Refactoring Engine for C\#” (Care\#) was developed and used to conduct the code analysis for this study.},
	number = {3},
	journal = {Software Quality Journal},
	author = {Akiki, Pierre A.},
	month = sep,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {C\#, Code analysis, Consistency, Implicit and explicit typing, Readability},
	pages = {1175--1207},
}

@article{doernhoefer_surfing_2012,
	title = {Surfing the net for software engineering notes},
	volume = {37},
	issn = {0163-5948},
	url = {https://doi.org/10.1145/2382756.2382780},
	doi = {10.1145/2382756.2382780},
	number = {6},
	journal = {SIGSOFT Softw. Eng. Notes},
	author = {Doernhoefer, Mark},
	month = nov,
	year = {2012},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {10--18},
}

@inproceedings{hassan_rudsea_2018,
	address = {New York, NY, USA},
	series = {{ASE} '18},
	title = {{RUDSEA}: recommending updates of {Dockerfiles} via software environment analysis},
	isbn = {978-1-4503-5937-5},
	url = {https://doi.org/10.1145/3238147.3240470},
	doi = {10.1145/3238147.3240470},
	abstract = {Dockerfiles are configuration files of docker images which package all dependencies of a software to enable convenient software deployment and porting. In other words, dockerfiles list all environment assumptions of a software application's build and / or execution, so they need to be frequently updated when the environment assumptions change during fast software evolution. In this paper, we propose RUDSEA, a novel approach to recommend updates of dockerfiles to developers based on analyzing changes on software environment assumptions and their impacts. Our evaluation on 1,199 real-world instruction updates shows that RUDSEA can recommend correct update locations for 78.5\% of the updates, and correct code changes for 44.1\% of the updates.},
	booktitle = {Proceedings of the 33rd {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hassan, Foyzul and Rodriguez, Rodney and Wang, Xiaoyin},
	year = {2018},
	note = {event-place: Montpellier, France},
	keywords = {Dockerfile, Software Environment, String Analysis},
	pages = {796--801},
}

@article{sultana_code_2023,
	title = {Code reviews in open source projects : how do gender biases affect participation and outcomes?},
	volume = {28},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-023-10324-9},
	doi = {10.1007/s10664-023-10324-9},
	number = {4},
	journal = {Empirical Softw. Engg.},
	author = {Sultana, Sayma and Turzo, Asif Kamal and Bosu, Amiangshu},
	month = jun,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Code review, Diversity and inclusion, Gender bias, Pull requests},
}

@article{flint_pitfalls_2022,
	title = {Pitfalls and guidelines for using time-based {Git} data},
	volume = {27},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-022-10200-y},
	doi = {10.1007/s10664-022-10200-y},
	abstract = {Many software engineering research papers rely on time-based data (e.g., commit timestamps, issue report creation/update/close dates, release dates). Like most real-world data however, time-based data is often dirty. To date, there are no studies that quantify how frequently such data is used by the software engineering research community, or investigate sources of and quantify how often such data is dirty. Depending on the research task and method used, including such dirty data could affect the research results. This paper presents an extended survey of papers that utilize time-based data, published in the Mining Software Repositories (MSR) conference series. Out of the 754 technical track and data papers published in MSR 2004–2021, we saw at least 290 (38\%) papers utilized time-based data. We also observed that most time-based data used in research papers comes in the form of Git commits, often from GitHub. Based on those results, we then used the Boa and Software Heritage infrastructures to help identify and quantify several sources of dirty Git timestamp data. Finally we provide guidelines/best practices for researchers utilizing time-based data from Git repositories.},
	number = {7},
	journal = {Empirical Softw. Engg.},
	author = {Flint, Samuel W. and Chauhan, Jigyasa and Dyer, Robert},
	month = dec,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Mining software repositories, Literature review, Time data},
}

@inproceedings{wang_if_2021,
	series = {{ICSE} '21},
	title = {If {It}'s {Not} {Secure}, {It} {Should} {Not} {Compile}: {Preventing} {DOM}-{Based} {XSS} in {Large}-{Scale} {Web} {Development} with {API} {Hardening}},
	isbn = {978-1-4503-9085-9},
	url = {https://doi.org/10.1109/ICSE43902.2021.00123},
	doi = {10.1109/ICSE43902.2021.00123},
	abstract = {With tons of efforts spent on its mitigation, Cross-site scripting (XSS) remains one of the most prevalent security threats on the internet. Decades of exploitation and remediation demonstrated that code inspection and testing alone does not eliminate XSS vulnerabilities in complex web applications with a high degree of confidence.This paper introduces Google's secure-by-design engineering paradigm that effectively prevents DOM-based XSS vulnerabilities in large-scale web development. Our approach, named API hardening, enforces a series of company-wide secure coding practices. We provide a set of secure APIs to replace native DOM APIs that are prone to XSS vulnerabilities. Through a combination of type contracts and appropriate validation and escaping, the secure APIs ensure that applications based thereon are free of XSS vulnerabilities. We deploy a simple yet capable compile-time checker to guarantee that developers exclusively use our hardened APIs to interact with the DOM. We make various of efforts to scale this approach to tens of thousands of engineers without significant productivity impact. By offering rigorous tooling and consultant support, we help developers adopt the secure coding practices as seamlessly as possible. We present empirical results showing how API hardening has helped reduce the occurrences of XSS vulnerabilities in Google's enormous code base over the course of two-year deployment.},
	booktitle = {Proceedings of the 43rd {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Wang, Pei and Bangert, Julian and Kern, Christoph},
	year = {2021},
	note = {Place: Madrid, Spain},
	keywords = {empirical software engineering, cross-site scripting, language-based security, Web security},
	pages = {1360--1372},
}

@article{nam_bug_2019,
	title = {A bug finder refined by a large set of open-source projects},
	volume = {112},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2019.04.014},
	doi = {10.1016/j.infsof.2019.04.014},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Nam, Jaechang and Wang, Song and Xi, Yuan and Tan, Lin},
	month = aug,
	year = {2019},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {bug detection rules, bug patterns, Static bug finder},
	pages = {164--175},
}

@inproceedings{reis_leveraging_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Leveraging {Practitioners}’ {Feedback} to {Improve} a {Security} {Linter}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3560419},
	doi = {10.1145/3551349.3560419},
	abstract = {Infrastructure-as-Code (IaC) is a technology that enables the management and distribution of infrastructure through code instead of manual processes. In 2020, Palo Alto Network’s Unit 42 announced the discovery of over 199K vulnerable IaC templates through their “Cloud Threat” Report. This report highlights the importance of tools to prevent vulnerabilities from reaching production. Unfortunately, we observed through a comprehensive study that a security linter for IaC scripts is not reliable yet—high false positive rates. Our approach to tackling this problem was to leverage community expertise to improve the precision of this tool. More precisely, we interviewed professional developers to collect their feedback on the root causes of imprecision of the state-of-the-art security linter for Puppet. From that feedback, we developed a linter adjusting 7 rules of an existing linter ruleset and adding 3 new rules. We conducted a new study with 131 practitioners, which helped us improve the tool’s precision significantly and achieve a final precision of . An important takeaway from this paper is that obtaining professional feedback is fundamental to improving the rules’ precision and extending the rulesets, which is critical for the usefulness and adoption of lightweight tools, such as IaC security linters.},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Reis, Sofia and Abreu, Rui and d'Amorim, Marcelo and Fortunato, Daniel},
	year = {2023},
	note = {event-place: Rochester, MI, USA},
	keywords = {Infrastructure, Linter, Security},
}

@inproceedings{bao_v-szz_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {V-{SZZ}: automatic identification of version ranges affected by {CVE} vulnerabilities},
	isbn = {978-1-4503-9221-1},
	url = {https://doi.org/10.1145/3510003.3510113},
	doi = {10.1145/3510003.3510113},
	abstract = {Vulnerabilities publicly disclosed in the National Vulnerability Database (NVD) are assigned with CVE (Common Vulnerabilities and Exposures) IDs and associated with specific software versions. Many organizations, including IT companies and government, heavily rely on the disclosed vulnerabilities in NVD to mitigate their security risks. Once a software is claimed as vulnerable by NVD, these organizations would examine the presence of the vulnerable versions of the software and assess the impact on themselves. However, the version information about vulnerable software in NVD is not always reliable. Nguyen et al. find that the version information of many CVE vulnerabilities is spurious and propose an approach based on the original SZZ algorithm (i.e., an approach to identify bug-introducing commits) to assess the software versions affected by CVE vulnerabilities.However, SZZ algorithms are designed for common bugs, while vulnerabilities and bugs are different. Many bugs are introduced by a recent bug-fixing commit, but vulnerabilities are usually introduced in their initial versions. Thus, the current SZZ algorithms often fail to identify the inducing commits for vulnerabilities. Therefore, in this study, we propose an approach based on an improved SZZ algorithm to refine software versions affected by CVE vulnerabilities. Our proposed SZZ algorithm leverages the line mapping algorithms to identify the earliest commit that modified the vulnerable lines, and then considers these commits to be the vulnerability-inducing commits, as opposed to the previous SZZ algorithms that assume the commits that last modified the buggy lines as the inducing commits. To evaluate our proposed approach, we manually annotate the true inducing commits and verify the vulnerable versions for 172 CVE vulnerabilities with fixing commits from two publicly available datasets with five C/C++ and 41 Java projects, respectively. We find that 99 out of 172 vulnerabilities whose version information is spurious. The experiment results show that our proposed approach can identify more vulnerabilities with the true inducing commits and correct vulnerable versions than the previous SZZ algorithms. Our approach outperforms the previous SZZ algorithms in terms of F1-score for identifying vulnerability-inducing commits on both C/C++ and Java projects (0.736 and 0.630, respectively). For refining vulnerable versions, our approach also achieves the best performance on the two datasets in terms of F1-score (0.928 and 0.952).},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Bao, Lingfeng and Xia, Xin and Hassan, Ahmed E. and Yang, Xiaohu},
	year = {2022},
	note = {event-place: Pittsburgh, Pennsylvania},
	keywords = {CVE, SZZ, vulnerability},
	pages = {2352--2364},
}

@article{li_gbgallery_2022,
	title = {{GBGallery} : {A} benchmark and framework for game testing},
	volume = {27},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-022-10158-x},
	doi = {10.1007/s10664-022-10158-x},
	abstract = {Software bug database and benchmark are the wheels of advancing automated software testing. In practice, real bugs often occur sparsely relative to the amount of software code, the extraction and curation of which are quite labor-intensive but can be essential to facilitate the innovation of testing techniques. Over the past decade, several milestones have been made to construct bug databases, pushing the progress of automated software testing research. However, up to the present, it still lacks a real bug database and benchmark for game software, making current game testing research mostly stagnant. The missing of bug database and framework greatly limits the development of automated game testing techniques. To bridge this gap, we first perform large-scale real bug collection and manual analysis from 5 large commercial games, with a total of more than 250,000 lines of code. Based on this, we propose GBGallery, a game bug database and an extensible framework, to enable automated game testing research. In its initial version, GBGallery contains 76 real bugs from 5 games and incorporates 5 state-of-the-art testing techniques for comparative study as a baseline for further research. With GBGallery, we perform large-scale empirical studies and find that the current automated game testing is still at an early stage, where new testing techniques for game software should be extensively investigated. We make GBGallery publicly available, hoping to facilitate the game testing research.},
	number = {6},
	journal = {Empirical Softw. Engg.},
	author = {Li, Zhuo and Wu, Yuechen and Ma, Lei and Xie, Xiaofei and Chen, Yingfeng and Fan, Changjie},
	month = nov,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Automated testing, Bug database, Deep reinforcement learning, Game testing},
}

@article{unger_reliability_2023,
	title = {Reliability and {Performance} of the {Online} {Literature} {Database} {CAMbase} after {Changing} from a {Semantic} {Search} to a {Score} {Ranking} {Algorithm}},
	volume = {4},
	url = {https://doi.org/10.1007/s42979-023-02146-9},
	doi = {10.1007/s42979-023-02146-9},
	abstract = {Despite the increase in scientific publications in the field of integrative medicine over the past decades, a valid overview of published evidence remains challenging to get. The online literature database CAMbase (available at ) is one of the established databases designed to provide such an overview. In 2020, the database was migrated from a 32-bit to a 64-bit operating system, which resulted in unexpected, technical issues and forced the replacement of the semantic search algorithm with Solr, an open-source platform that uses a score ranking algorithm. Although semantic search was replaced, the goal was to create a literature database that is essentially no different from the legacy system. Therefore, a before-after analysis was conducted to compare first the number of retrieved documents and then their titles, while the titles were syntactically compared using two Sentence-Bidirectional Encoder Representations from Transformers (SBERT) models. Analysis with a paired t-test revealed no significant overall differences between the legacy system and the final system in the number of documents (t =−\&nbsp;1.41, df = 35, p = 0.17), but an increase in performance (t = 4.13, df = 35, p \&lt; 0.01). Analysis with a t-test for independent samples of the values from the models also revealed a high degree of consistency between the retrieved documents. The results show that an equivalent search can be provided by using Solr, while improving the performance, making this technical report a viable blueprint for projects with similar contexts.},
	number = {5},
	journal = {SN Comput. Sci.},
	author = {Unger, Sebastian and Raak, Christa K. and Ostermann, Thomas},
	month = sep,
	year = {2023},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Deep learning, Database management systems, Evaluation, Information storage and retrieval, Search engine, Semantics},
}

@inproceedings{wu_understanding_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Understanding and {Predicting} {Docker} {Build} {Duration}: {An} {Empirical} {Study} of {Containerized} {Workflow} of {OSS} {Projects}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3556940},
	doi = {10.1145/3551349.3556940},
	abstract = {Docker building is a critical component of containerized workflow, which automates the process by which sources are packaged and transformed into container images. If not run properly, Docker builds can bring long durations (i.e., slow builds), which increases the cost in human and computing resources, and thus inevitably affect the software development. However, the current status and remedy for the duration cost in Docker builds remain unclear and need an in-depth study. To fill this gap, this paper provides the first empirical investigation on 171,439 Docker builds from 5,833 open source software (OSS) projects. Starting with an exploratory study, the Docker build durations can be characterized in real-world projects, and the developers’ perceptions of slow builds are obtained via a comprehensive survey. Driven by the results of our exploratory study, we propose a prediction modeling of Docker build duration, leveraging 27 handcrafted features from build-related context and configuration and 8 regression algorithms for the prediction task. Our results demonstrate that Random Forest model provides the superior performance with a Spearman’s correlation of 0.781, outperforming the baseline random model by 82.9\% in RMSE, 90.6\% in MAE, and 94.4\% in MAPE, respectively. The implications of this study will facilitate research and assist practitioners in improving the Docker build process.},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Yiwen and Zhang, Yang and Xu, Kele and Wang, Tao and Wang, Huaimin},
	year = {2023},
	note = {event-place: Rochester, MI, USA},
	keywords = {Docker, Build duration, Containerization, Duration prediction},
}

@inproceedings{kallingal_joshy_validating_2021,
	address = {New York, NY, USA},
	series = {{ISSTA} 2021},
	title = {Validating static warnings via testing code fragments},
	isbn = {978-1-4503-8459-9},
	url = {https://doi.org/10.1145/3460319.3464832},
	doi = {10.1145/3460319.3464832},
	abstract = {Static analysis is an important approach for finding bugs and vulnerabilities in software. However, inspecting and confirming static warnings are challenging and time-consuming. In this paper, we present a novel solution that automatically generates test cases based on static warnings to validate true and false positives. We designed a syntactic patching algorithm that can generate syntactically valid, semantic preserving executable code fragments from static warnings. We developed a build and testing system to automatically test code fragments using fuzzers, KLEE and Valgrind. We evaluated our techniques using 12 real-world C projects and 1955 warnings from two commercial static analysis tools. We successfully built 68.5\% code fragments and generated 1003 test cases. Through automatic testing, we identified 48 true positives and 27 false positives, and 205 likely false positives. We matched 4 CVE and real-world bugs using Helium, and they are only triggered by our tool but not other baseline tools. We found that testing code fragments is scalable and useful; it can trigger bugs that testing entire programs or testing procedures failed to trigger.},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Kallingal Joshy, Ashwin and Chen, Xueyuan and Steenhoek, Benjamin and Le, Wei},
	year = {2021},
	note = {event-place: Virtual, Denmark},
	keywords = {Code Fragments, Syntactic Patching, Testing Static Warnings},
	pages = {540--552},
}

@inproceedings{wan_you_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {You see what {I} want you to see: poisoning vulnerabilities in neural code search},
	isbn = {978-1-4503-9413-0},
	url = {https://doi.org/10.1145/3540250.3549153},
	doi = {10.1145/3540250.3549153},
	abstract = {Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far. In this paper, we aim to study and understand the security and robustness of code search models by answering the following question: Can we inject backdoors into deep-learning-based code search models? If so, can we detect poisoned data and remove these backdoors? This work studies and develops a series of backdoor attacks on the deep-learning-based models for code search, through data poisoning. We first show that existing models are vulnerable to data-poisoning-based backdoor attacks. We then introduce a simple yet effective attack on neural code search models by poisoning their corresponding training dataset. Moreover, we demonstrate that attacks can also influence the ranking of the code search results by adding a few specially-crafted source code files to the training corpus. We show that this type of backdoor attack is effective for several representative deep-learning-based code search systems, and can successfully manipulate the ranking list of searching results. Taking the bidirectional RNN-based code search system as an example, the normalized ranking of the target candidate can be significantly raised from top 50\% to top 4.43\%, given a query containing an attacker targeted word, e.g., file. To defend a model against such attack, we empirically examine an existing popular defense strategy and evaluate its performance. Our results show the explored defense strategy is not yet effective in our proposed backdoor attack for code search systems.},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wan, Yao and Zhang, Shijie and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Yao, Dezhong and Jin, Hai and Sun, Lichao},
	year = {2022},
	note = {event-place: Singapore, Singapore},
	keywords = {deep learning, Code search, backdoor attack, data poisoning, software vulnerability},
	pages = {1233--1245},
}

@article{mahdavi-hezaveh_feature_2022,
	title = {Feature toggles as code: {Heuristics} and metrics for structuring feature toggles},
	volume = {145},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2021.106813},
	doi = {10.1016/j.infsof.2021.106813},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Mahdavi-Hezaveh, Rezvan and Ajmeri, Nirav and Williams, Laurie},
	month = may,
	year = {2022},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Continuous development, Continuous integration, Feature toggle, Heuristic, Metric, Open source repository},
}

@inproceedings{bibaev_all_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {All you need is logs: improving code completion by learning from anonymous {IDE} usage logs},
	isbn = {978-1-4503-9413-0},
	url = {https://doi.org/10.1145/3540250.3558968},
	doi = {10.1145/3540250.3558968},
	abstract = {In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832. The approach adheres to privacy requirements and legal constraints, since it does not require collecting personal information, performing all the necessary anonymization on the client's side. Importantly, it can be improved continuously: implementing new features, collecting new data, and evaluating new models - this way, we have been using it in production since the end of 2020.},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Bibaev, Vitaliy and Kalina, Alexey and Lomshakov, Vadim and Golubev, Yaroslav and Bezzubov, Alexander and Povarov, Nikita and Bryksin, Timofey},
	year = {2022},
	note = {event-place: Singapore, Singapore},
	keywords = {machine learning, A/B-testing, anonymous usage logs, code completion, integrated development environment},
	pages = {1269--1279},
}

@inproceedings{van_tonder_tailoring_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {Tailoring programs for static analysis via program transformation},
	isbn = {978-1-4503-7121-6},
	url = {https://doi.org/10.1145/3377811.3380343},
	doi = {10.1145/3377811.3380343},
	abstract = {Static analysis is a proven technique for catching bugs during software development. However, analysis tooling must approximate, both theoretically and in the interest of practicality. False positives are a pervading manifestation of such approximations—tool configuration and customization is therefore crucial for usability and directing analysis behavior. To suppress false positives, developers readily disable bug checks or insert comments that suppress spurious bug reports. Existing work shows that these mechanisms fall short of developer needs and present a significant pain point for using or adopting analyses. We draw on the insight that an analysis user always has one notable ability to influence analysis behavior regardless of analyzer options and implementation: modifying their program. We present a new technique for automated, generic, and temporary code changes that tailor to suppress spurious analysis errors. We adopt a rule-based approach where simple, declarative templates describe general syntactic changes for code patterns that are known to be problematic for the analyzer. Our technique promotes program transformation as a general primitive for improving the fidelity of analysis reports (we treat any given analyzer as a black box). We evaluate using five different static analyzers supporting three different languages (C, Java, and PHP) on large, real world programs (up to 800KLOC). We show that our approach is effective in sidestepping long-standing and complex issues in analysis implementations.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {van Tonder, Rijnard and Le Goues, Claire},
	year = {2020},
	note = {event-place: Seoul, South Korea},
	pages = {824--834},
}

@article{wessel_quality_2022,
	title = {Quality gatekeepers: investigating the effects of code review bots on pull request activities},
	volume = {27},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-022-10130-9},
	doi = {10.1007/s10664-022-10130-9},
	abstract = {Software bots have been facilitating several development activities in Open Source Software (OSS) projects, including code review. However, these bots may bring unexpected impacts to group dynamics, as frequently occurs with new technology adoption. Understanding and anticipating such effects is important for planning and management. To analyze these effects, we investigate how several activity indicators change after the adoption of a code review bot. We employed a regression discontinuity design on 1,194 software projects from GitHub. We also interviewed 12 practitioners, including open-source maintainers and contributors. Our results indicate that the adoption of code review bots increases the number of monthly merged pull requests, decreases monthly non-merged pull requests, and decreases communication among developers. From the developers’ perspective, these effects are explained by the transparency and confidence the bot comments introduce, in addition to the changes in the discussion focused on pull requests. Practitioners and maintainers may leverage our results to understand, or even predict, bot effects on their projects.},
	number = {5},
	journal = {Empirical Softw. Engg.},
	author = {Wessel, Mairieli and Serebrenik, Alexander and Wiese, Igor and Steinmacher, Igor and Gerosa, Marco A.},
	month = sep,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Software engineering, Open source software, Code review, Automation, GitHub bots, Software bots},
}

@inproceedings{chen_studying_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {Studying the use of {Java} logging utilities in the wild},
	isbn = {978-1-4503-7121-6},
	url = {https://doi.org/10.1145/3377811.3380408},
	doi = {10.1145/3377811.3380408},
	abstract = {Software logging is widely used in practice. Logs have been used for a variety of purposes like debugging, monitoring, security compliance, and business analytics. Instead of directly invoking the standard output functions, developers usually prefer to use logging utilities (LUs) (e.g., SLF4J), which provide additional functionalities like thread-safety and verbosity level support, to instrument their source code. Many of the previous research works on software logging are focused on the log printing code. There are very few works studying the use of LUs, although new LUs are constantly being introduced by companies and researchers. In this paper, we conducted a large-scale empirical study on the use of Java LUs in the wild. We analyzed the use of 3, 856 LUs from 11,194 projects in GitHub and found that many projects have complex usage patterns for LUs. For example, 75.8\% of the large-sized projects have implemented their own LUs in their projects. More than 50\% of these projects use at least three LUs. We conducted further qualitative studies to better understand and characterize the complex use of LUs. Our findings show that different LUs are used for a variety of reasons (e.g., internationalization of the log messages). Some projects develop their own LUs to satisfy project-specific logging needs (e.g., defining the logging format). Multiple uses of LUs in one project are pretty common for large and very largesized projects mainly for context like enabling and configuring the logging behavior for the imported packages. Interviewing with 13 industrial developers showed that our findings are also generally true for industrial projects and are considered as very helpful for them to better configure and manage the logging behavior for their projects. The findings and the implications presented in this paper will be useful for developers and researchers who are interested in developing and maintaining LUs.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Boyuan and Jiang, Zhen Ming (Jack)},
	year = {2020},
	note = {event-place: Seoul, South Korea},
	keywords = {empirical software engineering, logging code, logging practices},
	pages = {397--408},
}

@inproceedings{dai_automatically_2020,
	address = {New York, NY, USA},
	series = {{SoCC} '20},
	title = {Automatically detecting risky scripts in infrastructure code},
	isbn = {978-1-4503-8137-6},
	url = {https://doi.org/10.1145/3419111.3421303},
	doi = {10.1145/3419111.3421303},
	abstract = {Infrastructure code supports embedded scripting languages such as Shell and PowerShell to manage the infrastructure resources and conduct life-cycle operations. Risky patterns in the embedded scripts have widespread of negative impacts across the whole infrastructure, causing disastrous consequences. In this paper, we propose an analysis framework, which can automatically extract and compose the embedded scripts from infrastructure code before detecting their risky code patterns with correlated severity levels and negative impacts. We implement SecureCode based on the proposed framework to check infrastructure code supported by Ansible, i.e., Ansible playbooks. We integrate SecureCode with the DevOp pipeline deployed in IBM cloud and test Secure-Code on 45 IBM Services community repositories. Our evaluation shows that SecureCode can efficiently and effectively identify 3419 true issues with 116 false positives in minutes. Among the 3419 true issues, 1691 have high severity levels.},
	booktitle = {Proceedings of the 11th {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Dai, Ting and Karve, Alexei and Koper, Grzegorz and Zeng, Sai},
	year = {2020},
	note = {event-place: Virtual Event, USA},
	keywords = {performance, static analysis, security, infrastructure-as-code, ansible, availability, powershell, reliability, shell},
	pages = {358--371},
}

@inproceedings{goncharenko_language_2016,
	address = {New York, NY, USA},
	series = {{SLE} 2016},
	title = {Language design and implementation for the domain of coding conventions},
	isbn = {978-1-4503-4447-0},
	url = {https://doi.org/10.1145/2997364.2997386},
	doi = {10.1145/2997364.2997386},
	abstract = {Coding conventions are lexical, syntactic or semantic restrictions enforced on top of a software language for the sake of consistency within the source base. Specifying coding conventions is currently an open problem in software language engineering, addressed in practice by resorting to natural language descriptions which complicate conformance verification. In this paper we present an endeavour to solve this problem for the case of CSS — a ubiquitous software language used for specifying appearance of hypertextual content separately from the content itself. The paper contains the results of domain analysis, a short report on an empirically obtained catalogue of 143 unique CSS coding conventions, the domain-specific ontology for the domain of detecting violations, the design of CssCoco, a language for expressing coding conventions of CSS, as well as a description of the tool we developed to detect violations of conventions specified in this DSL.},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Goncharenko, Boryana and Zaytsev, Vadim},
	year = {2016},
	note = {event-place: Amsterdam, Netherlands},
	keywords = {conventions, software language design},
	pages = {90--104},
}

@inproceedings{giannopoulos_pythia_2019,
	address = {New York, NY, USA},
	series = {{EuroSec} '19},
	title = {Pythia: {Identifying} {Dangerous} {Data}-flows in {Django}-based {Applications}},
	isbn = {978-1-4503-6274-0},
	url = {https://doi.org/10.1145/3301417.3312497},
	doi = {10.1145/3301417.3312497},
	abstract = {Web frameworks that allow developers to create applications based on design patterns such as the Model View Controller (MVC), provide by default a number of security checks. Nevertheless, by using specific constructs, developers may disable these checks thus re-introducing classic application vulnerabilities such as Cross-site Scripting (XSS) and Cross-Site Request Forgery (CSRF). Framework-specific elements including (1) the complex nature of these applications, (2) the different features that they involve (e.g. templates), and (3) the inheritance mechanisms that governs them, make the identification of such issues very difficult.To tackle this problem, we have developed Pythia, a scheme that analyzes applications based on the Django framework. To identify potentially dangerous data flows that can lead to XSS and CSRF defects, Pythia takes into account all the aforementioned elements and employs ideas coming from standard data-flow analysis and taint tracking schemes. To the best of our knowledge, Pythia is the first mechanism to consider framework-specific elements in its analysis. We have evaluated our scheme with positive results. Specifically, we used Pythia to examine five open-source applications that are currently in production and have thousands of users including an e-voting service, and a web-based translation management system. In four cases we have identified dangerous paths that in turn led to vulnerabilities. Notably, in many cases the paths involved the particular features of Django-based applications e.g. templates.},
	booktitle = {Proceedings of the 12th {European} {Workshop} on {Systems} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Giannopoulos, Linos and Degkleri, Eirini and Tsanakas, Panayiotis and Mitropoulos, Dimitris},
	year = {2019},
	note = {event-place: Dresden, Germany},
	keywords = {Application Security, Cross-Site Request Forgery, Cross-site Scripting, Data-flow Analysis, Django, Templates, Unsanitized Output},
}

@inproceedings{rosa_evaluating_2021,
	series = {{ICSE} '21},
	title = {Evaluating {SZZ} {Implementations} {Through} a {Developer}-informed {Oracle}},
	isbn = {978-1-4503-9085-9},
	url = {https://doi.org/10.1109/ICSE43902.2021.00049},
	doi = {10.1109/ICSE43902.2021.00049},
	abstract = {The SZZ algorithm for identifying bug-inducing changes has been widely used to evaluate defect prediction techniques and to empirically investigate when, how, and by whom bugs are introduced. Over the years, researchers have proposed several heuristics to improve the SZZ accuracy, providing various implementations of SZZ. However, fairly evaluating those implementations on a reliable oracle is an open problem: SZZ evaluations usually rely on (i) the manual analysis of the SZZ output to classify the identified bug-inducing commits as true or false positives; or (ii) a golden set linking bug-fixing and bug-inducing commits. In both cases, these manual evaluations are performed by researchers with limited knowledge of the studied subject systems. Ideally, there should be a golden set created by the original developers of the studied systems.We propose a methodology to build a "developer-informed" oracle for the evaluation of SZZ variants. We use Natural Language Processing (NLP) to identify bug-fixing commits in which developers explicitly reference the commit(s) that introduced a fixed bug. This was followed by a manual filtering step aimed at ensuring the quality and accuracy of the oracle. Once built, we used the oracle to evaluate several variants of the SZZ algorithm in terms of their accuracy. Our evaluation helped us to distill a set of lessons learned to further improve the SZZ algorithm.},
	booktitle = {Proceedings of the 43rd {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Rosa, Giovanni and Pascarella, Luca and Scalabrino, Simone and Tufano, Rosalia and Bavota, Gabriele and Lanza, Michele and Oliveto, Rocco},
	year = {2021},
	note = {Place: Madrid, Spain},
	keywords = {SZZ, Defect Prediction, Empirical Study},
	pages = {436--447},
}

@article{alfayez_what_2023,
	title = {What is asked about technical debt ({TD}) on {Stack} {Exchange} question-and-answer ({Q}\&amp;{A}) websites? {An} observational study},
	volume = {28},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-022-10269-5},
	doi = {10.1007/s10664-022-10269-5},
	abstract = {Technical debt (TD) is a term coined by agile software pioneer Ward Cunningham to account for the added software system effort or cost resulting from taking early software project shortcuts. Previous research on TD has extensively outlined and discussed the various consequences derived from accumulating TD and the difficulty in managing it. A review of the software engineering literature revealed that Stack Exchange question-and-answer (Q\&amp;A) websites can provide valuable, real world perspectives on a number of software engineering topics. Therefore, this study aims to observe how the TD term is utilized on Stack Exchange Q\&amp;A websites. Specifically, this study utilizes a dataset derived from three Stack Exchange Q\&amp;A websites, which are Stack Overflow (SO), Software Engineering (SE), and Project Management (PM), to retrieve and analyze 578 TD-related questions. The results unveiled that TD-related questions can be categorized into 14 different categories, a total of 636 unique tags are utilized in the acquired set of TD-related questions, and a few TD-related categories both lack accepted answers and have a longer median time to receive an accepted answer than other categories. This study’s findings highlight the TD-related challenges that are addressed by Stack Exchange Q\&amp;A website users, which may prove beneficial in steering future TD-related efforts.},
	number = {2},
	journal = {Empirical Softw. Engg.},
	author = {Alfayez, Reem and Ding, Yunyan and Winn, Robert and Alfayez, Ghaida and Harman, Christopher and Boehm, Barry},
	month = jan,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Software engineering, Technical debt, A) websites, Question-and-answer (Q\&amp, Stackoverflow, Technical debt management},
}

@article{rosa_comprehensive_2023,
	title = {A comprehensive evaluation of {SZZ} {Variants} through a developer-informed oracle},
	volume = {202},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111729},
	doi = {10.1016/j.jss.2023.111729},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Rosa, Giovanni and Pascarella, Luca and Scalabrino, Simone and Tufano, Rosalia and Bavota, Gabriele and Lanza, Michele and Oliveto, Rocco},
	month = aug,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Empirical study, SZZ, Defect prediction},
}

@inproceedings{keuning_student_2020,
	address = {New York, NY, USA},
	series = {Koli {Calling} '20},
	title = {Student {Refactoring} {Behaviour} in a {Programming} {Tutor}},
	isbn = {978-1-4503-8921-1},
	url = {https://doi.org/10.1145/3428029.3428043},
	doi = {10.1145/3428029.3428043},
	abstract = {Producing high-quality code is essential for professionals working on maintainable software. However, awareness of code quality is also important for novices. In addition to writing programs meeting functional requirements, teachers would like to see their students write understandable, concise and efficient code. Unfortunately, time to address these qualitative aspects is limited. We have developed a tutoring system for programming that teaches students to refactor functionally correct code, focussing on the method-level. The tutoring system provides automated feedback and layered hints. This paper describes the results of a study of 133 students working with the tutoring system. We analyse log data to see how they approach the exercises, and how they use the hints and feedback to refactor code. In addition, we analyse the results of a student survey. We found that students with some background in programming were generally able to identify issue in code and solve them (on average 92\%), that they used hints at various levels, and we noticed occasional learning in recurring issues. They struggled most with simplifying complex control flow. Students generally valued the topic of code quality and working with the tutor. Finally, we derive improvements for the tutoring system to strengthen students’ comprehension of refactoring.},
	booktitle = {Proceedings of the 20th {Koli} {Calling} {International} {Conference} on {Computing} {Education} {Research}},
	publisher = {Association for Computing Machinery},
	author = {Keuning, Hieke and Heeren, Bastiaan and Jeuring, Johan},
	year = {2020},
	note = {event-place: Koli, Finland},
}

@inproceedings{lyu_sand_2021,
	address = {New York, NY, USA},
	series = {{ISSTA} 2021},
	title = {{SAND}: a static analysis approach for detecting {SQL} antipatterns},
	isbn = {978-1-4503-8459-9},
	url = {https://doi.org/10.1145/3460319.3464818},
	doi = {10.1145/3460319.3464818},
	abstract = {Local databases underpin important features in many mobile applications, such as responsiveness in the face of poor connectivity. However, failure to use such databases correctly can lead to high resource consumption or even security vulnerabilities. We present SAND, an extensible static analysis approach that checks for misuse of local databases, also known as SQL antipatterns, in mobile apps. SAND features novel abstractions for common forms of application/database interactions, which enables concise and precise specification of the antipatterns that SAND checks for. To validate the efficacy of SAND, we have experimented with a diverse suite of 1,000 Android apps. We show that the abstractions that power SAND allow concise specification of all the known antipatterns from the literature (12-74 LOC), and that the antipatterns are modeled accurately (99.4-100\% precision). As for performance, SAND requires on average 41 seconds to complete a scan on a mobile app.},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Lyu, Yingjun and Volokh, Sasha and Halfond, William G. J. and Tripp, Omer},
	year = {2021},
	note = {event-place: Virtual, Denmark},
	keywords = {performance, security, database, Mobile applications},
	pages = {270--282},
}

@article{arshad_analysis_2021,
	title = {Analysis of security and privacy challenges for {DNA}-genomics applications and databases},
	volume = {119},
	issn = {1532-0464},
	url = {https://doi.org/10.1016/j.jbi.2021.103815},
	doi = {10.1016/j.jbi.2021.103815},
	number = {C},
	journal = {J. of Biomedical Informatics},
	author = {Arshad, Saadia and Arshad, Junaid and Khan, Muhammad Mubashir and Parkinson, Simon},
	month = jul,
	year = {2021},
	note = {Place: San Diego, CA, USA
Publisher: Elsevier Science},
	keywords = {Bioinformatics, Cyber-attacks, Cyberbiosecurity, DNA, Genomics, Vulnerabilities},
}

@inproceedings{barnaby_exempla_2020,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Exempla gratis ({E}.{G}.): code examples for free},
	isbn = {978-1-4503-7043-1},
	url = {https://doi.org/10.1145/3368089.3417052},
	doi = {10.1145/3368089.3417052},
	abstract = {Modern software engineering often involves using many existing APIs, both open source and – in industrial coding environments– proprietary. Programmers reference documentation and code search tools to remind themselves of proper common usage patterns of APIs. However, high-quality API usage examples are computationally expensive to curate and maintain, and API usage examples retrieved from company-wide code search can be tedious to review. We present a tool, EG, that mines codebases and shows the common, idiomatic us-age examples for API methods. EG was integrated into Facebook’s internal code search tool for the Hack language and evaluated on open-source GitHub projects written in Python. EG was also compared against code search results and hand-written examples from a popular programming website called ProgramCreek. Compared with these two baselines, examples generated by EG are more succinct and representative with less extraneous statements. In addition, a survey with Facebook developers shows that EG examples are preferred in 97\% of cases.},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Barnaby, Celeste and Sen, Koushik and Zhang, Tianyi and Glassman, Elena and Chandra, Satish},
	year = {2020},
	note = {event-place: Virtual Event, USA},
	keywords = {API examples, big code, software tools},
	pages = {1353--1364},
}

@article{mahadi_conclusion_2022,
	title = {Conclusion stability for natural language based mining of design discussions},
	volume = {27},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-021-10009-1},
	doi = {10.1007/s10664-021-10009-1},
	abstract = {Developer discussions range from in-person hallway chats to comment chains on bug reports. Being able to identify discussions that touch on software design would be helpful in documentation and refactoring software. Design mining is the application of machine learning techniques to correctly label a given discussion artifact, such as a pull request, as pertaining (or not) to design. In this paper we demonstrate a simple example of how design mining works. We then show how conclusion stability is poor on different artifact types and different projects. We show two techniques—augmentation and context specificity—that greatly improve the conclusion stability and cross-project relevance of design mining. Our new approach achieves AUC of 0.88 on within dataset classification and 0.80 on the cross-dataset classification task.},
	number = {1},
	journal = {Empirical Softw. Engg.},
	author = {Mahadi, Alvi and Ernst, Neil A. and Tongay, Karan},
	month = jan,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Conclusion stability, Mining software design, Supervised learning},
}

@article{savic_language-independent_2014,
	title = {A language-independent approach to the extraction of dependencies between source code entities},
	volume = {56},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2014.04.011},
	doi = {10.1016/j.infsof.2014.04.011},
	abstract = {Context: Software networks are directed graphs of static dependencies between source code entities (functions, classes, modules, etc.). These structures can be used to investigate the complexity and evolution of large-scale software systems and to compute metrics associated with software design. The extraction of software networks is also the first step in reverse engineering activities. Objective: The aim of this paper is to present SNEIPL, a novel approach to the extraction of software networks that is based on a language-independent, enriched concrete syntax tree representation of the source code. Method: The applicability of the approach is demonstrated by the extraction of software networks representing real-world, medium to large software systems written in different languages which belong to different programming paradigms. To investigate the completeness and correctness of the approach, class collaboration networks (CCNs) extracted from real-world Java software systems are compared to CCNs obtained by other tools. Namely, we used Dependency Finder which extracts entity-level dependencies from Java bytecode, and Doxygen which realizes language-independent fuzzy parsing approach to dependency extraction. We also compared SNEIPL to fact extractors present in language-independent reverse engineering tools. Results: Our approach to dependency extraction is validated on six real-world medium to large-scale software systems written in Java, Modula-2, and Delphi. The results of the comparative analysis involving ten Java software systems show that the networks formed by SNEIPL are highly similar to those formed by Dependency Finder and more precise than the comparable networks formed with the help of Doxygen. Regarding the comparison with language-independent reverse engineering tools, SNEIPL provides both language-independent extraction and representation of fact bases. Conclusion: SNEIPL is a language-independent extractor of software networks and consequently enables language-independent network-based analysis of software systems, computation of design software metrics, and extraction of fact bases for reverse engineering activities.},
	number = {10},
	journal = {Inf. Softw. Technol.},
	author = {Savić, Miloš and Rakić, Gordana and Budimac, Zoran and Ivanović, Mirjana},
	month = oct,
	year = {2014},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Software metrics, Dependency extraction, Enriched concrete syntax tree, Fact extraction, Reverse engineering, Software networks},
	pages = {1268--1288},
}

@article{szalay_practical_2021,
	title = {Practical heuristics to improve precision for erroneous function argument swapping detection in {C} and {C}++},
	volume = {181},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2021.111048},
	doi = {10.1016/j.jss.2021.111048},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Szalay, Richárd and Sinkovics, Ábel and Porkoláb, Zoltán},
	month = nov,
	year = {2021},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Argument selection defect, Error-prone constructs, Function parameters, Static analysis, Strong typing, Type safety},
}

@article{tamburri_discovering_2019,
	title = {Discovering community patterns in open-source: a systematic approach and its evaluation},
	volume = {24},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-018-9659-9},
	doi = {10.1007/s10664-018-9659-9},
	abstract = {"There can be no vulnerability without risk; there can be no community without vulnerability; there can be no peace, and ultimately no life, without community." - [M. Scott Peck]The open-source phenomenon has reached the point in which it is virtually impossible to find large applications that do not rely on it. Such grand adoption may turn into a risk if the community regulatory aspects behind open-source work (e.g., contribution guidelines or release schemas) are left implicit and their effect untracked. We advocate the explicit study and automated support of such aspects and propose Yoshi (Y ielding O pen-S ource H ealth I nformation), a tool able to map open-source communities onto community patterns, sets of known organisational and social structure types and characteristics with measurable core attributes. This mapping is beneficial since it allows, for example, (a) further investigation of community health measuring established characteristics from organisations research, (b) reuse of pattern-specific best-practices from the same literature, and (c) diagnosis of organisational anti-patterns specific to open-source, if any. We evaluate the tool in a quantitative empirical study involving 25 open-source communities from GitHub, finding that the tool offers a valuable basis to monitor key community traits behind open-source development and may form an effective combination with web-portals such as OpenHub or Bitergia. We made the proposed tool open source and publicly available.},
	number = {3},
	journal = {Empirical Softw. Engg.},
	author = {Tamburri, Damian A. and Palomba, Fabio and Serebrenik, Alexander and Zaidman, Andy},
	month = jun,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Community patterns, Empirical software engineering, Community types, Open source systems and community analysis},
	pages = {1369--1417},
}

@book{noauthor_icse_2023,
	address = {Melbourne, Victoria, Australia},
	title = {{ICSE} '23: {Proceedings} of the 45th {International} {Conference} on {Software} {Engineering}},
	isbn = {978-1-6654-5701-9},
	abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
	publisher = {IEEE Press},
	year = {2023},
}

@inproceedings{tavares_systematic_2018,
	address = {New York, NY, USA},
	series = {{SBSI} '18},
	title = {A {Systematic} {Mapping} of {Literature} on {Software} {Refactoring} {Tools}},
	isbn = {978-1-4503-6559-8},
	url = {https://doi.org/10.1145/3229345.3229357},
	doi = {10.1145/3229345.3229357},
	abstract = {Refactoring consists of improving the internal structure of the code without changing the external behavior of a software system. However, the task of refactoring is very costly in the development of an information system. Thus, many tools have been proposed to support refactoring the source code. In order to find tools cited in the literature, this work presents a Systematic Literature Mapping about refactoring. As a result, this paper summarizes the refactoring tools that have been published in the last 5 years in terms of the tool profiles developed, which programming languages have support for refactoring and which are the main refactoring strategies that are handled by tools. It has been identified that publications on refactoring have remained constant over the past 5 years. Also, most of the refactoring works describe tools, being they for systems written in the Java language, that perform code refactoring automatically and the main refactorings are: Move Method, Pull Up Method, Extract Class and Code Clone. Finally, we performed an analysis of the data returned by the DBLP library. As a result, it was observed that the papers returned by the DBLP have a high level of similarity with the other research bases studied.},
	booktitle = {Proceedings of the {XIV} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tavares, Cleiton Silva and Ferreira, Fischer and Figueiredo, Eduardo},
	year = {2018},
	note = {event-place: Caxias do Sul, Brazil},
}

@inproceedings{zhang_answer_2020,
	address = {New York, NY, USA},
	series = {{SIGIR} '20},
	title = {Answer {Ranking} for {Product}-{Related} {Questions} via {Multiple} {Semantic} {Relations} {Modeling}},
	isbn = {978-1-4503-8016-4},
	url = {https://doi.org/10.1145/3397271.3401166},
	doi = {10.1145/3397271.3401166},
	abstract = {Many E-commerce sites now offer product-specific question answering platforms for users to communicate with each other by posting and answering questions during online shopping. However, the multiple answers provided by ordinary users usually vary diversely in their qualities and thus need to be appropriately ranked for each question to improve user satisfaction. It can be observed that product reviews usually provide useful information for a given question, and thus can assist the ranking process. In this paper, we investigate the answer ranking problem for product-related questions, with the relevant reviews treated as auxiliary information that can be exploited for facilitating the ranking. We propose an answer ranking model named MUSE which carefully models multiple semantic relations among the question, answers, and relevant reviews. Specifically, MUSE constructs a multi-semantic relation graph with the question, each answer, and each review snippet as nodes. Then a customized graph convolutional neural network is designed for explicitly modeling the semantic relevance between the question and answers, the content consistency among answers, and the textual entailment between answers and reviews. Extensive experiments on real-world E-commerce datasets across three product categories show that our proposed model achieves superior performance on the concerned answer ranking task.},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Wenxuan and Deng, Yang and Lam, Wai},
	year = {2020},
	note = {event-place: Virtual Event, China},
	keywords = {answer ranking, e-commerce, product question answering, question answering},
	pages = {569--578},
}

@book{noauthor_esecfse_2022,
	address = {New York, NY, USA},
	title = {{ESEC}/{FSE} 2022: {Proceedings} of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	isbn = {978-1-4503-9413-0},
	abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{fox_crossing_2012,
	title = {Crossing the software education chasm},
	volume = {55},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2160718.2160732},
	doi = {10.1145/2160718.2160732},
	abstract = {An Agile approach that exploits cloud computing.},
	number = {5},
	journal = {Commun. ACM},
	author = {Fox, Armando and Patterson, David},
	month = may,
	year = {2012},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {44--49},
}

@article{soetens_changes_2017,
	title = {Changes as {First}-{Class} {Citizens}: {A} {Research} {Perspective} on {Modern} {Software} {Tooling}},
	volume = {50},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3038926},
	doi = {10.1145/3038926},
	abstract = {Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions.},
	number = {2},
	journal = {ACM Comput. Surv.},
	author = {Soetens, Quinten David and Robbes, Romain and Demeyer, Serge},
	month = apr,
	year = {2017},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Systematic mapping study, atomic change operations, change distilling, change recording, fine-grained changes, fine-grained edit operations},
}

@article{nierstrasz_agile_2012,
	title = {Agile software assessment with {Moose}},
	volume = {37},
	issn = {0163-5948},
	url = {https://doi.org/10.1145/2180921.2180925},
	doi = {10.1145/2180921.2180925},
	abstract = {During software maintenance, much time is spent reading and assessing existing code. Unfortunately most of the tools available for exploring and assessing code, such as browsers, debuggers and profilers, focus on development tasks, and offer little to support program understanding. We present a platform for software and data analysis, called Moose, which enables the rapid development of custom tools for software assessment. We demonstrate how Moose supports agile software assessment through a series of demos, we illustrate some of the custom tools that have been developed, and we draw various lessons learned for future work in this domain},
	number = {3},
	journal = {SIGSOFT Softw. Eng. Notes},
	author = {Nierstrasz, Oscar},
	month = may,
	year = {2012},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {1--5},
}

@inproceedings{zieris_explaining_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {Explaining pair programming session dynamics from knowledge gaps},
	isbn = {978-1-4503-7121-6},
	url = {https://doi.org/10.1145/3377811.3380925},
	doi = {10.1145/3377811.3380925},
	abstract = {Background: Despite a lot of research on the effectiveness of Pair Programming (PP), the question when it is useful or less useful remains unsettled.Method: We analyze recordings of many industrial PP sessions with Grounded Theory Methodology and build on prior work that identified various phenomena related to within-session knowledge build-up and transfer. We validate our findings with practitioners.Result: We identify two fundamentally different types of required knowledge and explain how different constellations of knowledge gaps in these two respects lead to different session dynamics. Gaps in project-specific systems knowledge are more hampering than gaps in general programming knowledge and are dealt with first and foremost in a PP session.Conclusion: Partner constellations with complementary knowledge make PP a particularly effective practice. In PP sessions, differences in system understanding are more important than differences in general software development knowledge.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zieris, Franz and Prechelt, Lutz},
	year = {2020},
	note = {event-place: Seoul, South Korea},
	pages = {421--432},
}

@book{noauthor_ase_2016,
	address = {New York, NY, USA},
	title = {{ASE} '16: {Proceedings} of the 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	isbn = {978-1-4503-3845-5},
	publisher = {Association for Computing Machinery},
	year = {2016},
}

@book{noauthor_dls_2022,
	address = {New York, NY, USA},
	title = {{DLS} 2022: {Proceedings} of the 18th {ACM} {SIGPLAN} {International} {Symposium} on {Dynamic} {Languages}},
	isbn = {978-1-4503-9908-1},
	abstract = {Welcome to the 18th edition of the Dynamic Language Symposium (DLS), co-located with SPLASH 2022. DLS is the premier forum for researchers and practitioners to share research results and experience on all aspects on dynamic languages by which we mean languages like Clojure, Dart, Elixir, Erlang, JavaScript, Julia, Lisp, Lua, Perl, Python, Ruby, R, Racket, Scheme, Smalltalk, and more.},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@inproceedings{ahmed_empirical_2015,
	series = {{MSR} '15},
	title = {An empirical study of the copy and paste behavior during development},
	isbn = {978-0-7695-5594-2},
	abstract = {Developers frequently employ Copy and Paste. However, little is known about the copy and paste behavior during development. To better understand the copy and paste behavior, automated approaches are proposed to identify cloned code. However, such automated approaches can only identify the location of the code that has been copied and pasted, but little is known about the context of the copy and paste. On the other hand, prior research studying actual copy and paste behavior is based on a small number of users in an experimental setup.In this paper, we study the behavior of developers copying and pasting code while using the Eclipse IDE. We mine the usage data of over 20,000 Eclipse users. We aim to explore the different patterns of Copy and Paste (C\&amp;P) that are used by Eclipse users during development. We compare such usage patterns to the regular users' usage of copy and paste during non-development tasks reported in earlier studies. Our findings instruct builders of future IDEs. We find that developers' C\&amp;P behavior is considerably different from the behavior of regular users. For example, developers tend to perform more frequent C\&amp;P in the same file contrary to regular users, who tend to perform C\&amp;P across different windows. Moreover, we find that C\&amp;P across different programming languages is a common behavior as we extracted more than 75,000 C\&amp;P incidents across different programming languages. Such a finding highlights the need for clone detection techniques that can detect code clones across different programming languages.},
	booktitle = {Proceedings of the 12th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Ahmed, Tarek M. and Shang, Weiyi and Hassan, Ahmed E.},
	year = {2015},
	note = {Place: Florence, Italy},
	pages = {99--110},
}

@inproceedings{cito_empirical_2017,
	series = {{MSR} '17},
	title = {An empirical analysis of the docker container ecosystem on {GitHub}},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.67},
	doi = {10.1109/MSR.2017.67},
	abstract = {Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6\%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34\% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Cito, Jürgen and Schermann, Gerald and Wittern, John Erik and Leitner, Philipp and Zumberi, Sali and Gall, Harald C.},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	keywords = {empirical software engineering, GitHub, docker},
	pages = {323--333},
}

@inproceedings{kuiter_getting_2018,
	address = {New York, NY, USA},
	series = {{SPLC} '18},
	title = {Getting rid of clone-and-own: moving to a software product line for temperature monitoring},
	isbn = {978-1-4503-6464-5},
	url = {https://doi.org/10.1145/3233027.3233050},
	doi = {10.1145/3233027.3233050},
	abstract = {Due to its fast and simple applicability, clone-and-own is widely used in industry to develop software variants. In cooperation with different companies for thermoelectric products, we implemented multiple variants of a heat monitoring tool based on clone-and-own. After encountering redundancy-related problems during development and maintenance, we decided to migrate towards a software product line. Within this paper, we describe this case study of migrating cloned variants to a software product line based on the extractive approach. The resulting software product line encapsulates variability on several levels, including the underlying hardware systems, interfaces, and use cases. Currently, we support monitoring hardware from three different companies that use the same core system and provide a configurable front-end. We share our experiences and encountered problems with cloning and migration towards a software product line—focusing on feature extraction and modeling in particular. Furthermore, we provide a lightweight, web-based tool for modeling, configuring, and implementing software product lines, which we use to migrate and manage features. Besides this experience report, we contribute most of the created artifacts as open-source and freely available for the research community.},
	booktitle = {Proceedings of the 22nd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Kuiter, Elias and Krüger, Jacob and Krieter, Sebastian and Leich, Thomas and Saake, Gunter},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {case study, extraction, feature modeling, software product line},
	pages = {179--189},
}

@book{noauthor_ase_2022,
	address = {New York, NY, USA},
	title = {{ASE} '22: {Proceedings} of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	isbn = {978-1-4503-9475-8},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{ghaffarian_software_2017,
	title = {Software {Vulnerability} {Analysis} and {Discovery} {Using} {Machine}-{Learning} and {Data}-{Mining} {Techniques}: {A} {Survey}},
	volume = {50},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3092566},
	doi = {10.1145/3092566},
	abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
	month = aug,
	year = {2017},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {survey, software security, data-mining, machine-learning, review, Software vulnerability analysis, software vulnerability discovery},
}

@inproceedings{danilova_replication_2020,
	address = {USA},
	series = {{SOUPS}'20},
	title = {Replication: on the ecological validity of online security developer studies: exploring deception in a password-storage study with freelancers},
	isbn = {978-1-939133-16-8},
	abstract = {Recruiting professional developers for studies can be challenging and one major concern for studies examining security development issues is their ecological validity–does the study adequately reflect the real world? Naiakshina et al. [28] examined the ecological validity of a password storage study conducted with students [29, 30] by hiring freelancers from Freelancer.com. In the hope of increasing the ecologically validity, Naiakshina et al. used a deception study design wherein freelance developers were hired for a regular job using a company front created for the study, instead of openly telling the freelancers that they were taking part in a study. Based on their results, Naiakshina et al. propose the use of online freelancers to be examined further, to supplement other recruitment channels such as CS students and GitHub users. The deception in their study was used with the aim that results would reflect the real work of online freelancers. However, deception needs to be used with careful consideration, which can entail additional study design work and negotiations with ethical oversight bodies. In this paper, we take a closer look at the deception used in Naiakshina et al.'s study. Therefore, we replicate Naiakshina et al.'s work but announce and run it as a study on Freelancer.com. Our findings suggest that for this password storage study deception did not have a large effect and the open recruitment without deception was a viable recruitment method.},
	booktitle = {Proceedings of the {Sixteenth} {USENIX} {Conference} on {Usable} {Privacy} and {Security}},
	publisher = {USENIX Association},
	author = {Danilova, Anastasia and Naiakshina, Alena and Deuter, Johanna and Smith, Matthew},
	year = {2020},
}

@inproceedings{pinto_what_2013,
	address = {New York, NY, USA},
	series = {{WRT} '13},
	title = {What programmers say about refactoring tools? an empirical investigation of stack overflow},
	isbn = {978-1-4503-2604-9},
	url = {https://doi.org/10.1145/2541348.2541357},
	doi = {10.1145/2541348.2541357},
	abstract = {Programmers often use forums, such as StackOverflow, to easily and quickly solve their issues. Researchers then investigate those questions to better understand the state-of-use of software engineering techniques. Also, due to the quality and the great number questions and answers, the results found using such method might be difficult, or even impossible, to find using common survey techniques. In this study, we conducted a qualitative and quantitative research in order to categorize questions about refactoring tools. As a result, we presented a comprehensive classification of flaws and desirable features in refactoring tools. We also reported that programmers do not often rely on refactoring tools, but, at the same time, they are desiring number of unimplemented features.},
	booktitle = {Proceedings of the 2013 {ACM} {Workshop} on {Workshop} on {Refactoring} {Tools}},
	publisher = {Association for Computing Machinery},
	author = {Pinto, Gustavo H. and Kamei, Fernando},
	year = {2013},
	note = {event-place: Indianapolis, Indiana, USA},
	keywords = {programming knowledge, question-answer websites, refactoring tools},
	pages = {33--36},
}

@article{raza_assisting_2019,
	title = {Assisting software engineering students in analyzing their performance in software development},
	volume = {27},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-018-9433-7},
	doi = {10.1007/s11219-018-9433-7},
	abstract = {Collecting product and process measures in software development projects, particularly in education and training environments, is important as a basis for assessing current performance and opportunities for improvement. However, analyzing the collected data manually is challenging because of the expertise required, the lack of benchmarks for comparison, the amount of data to analyze, and the time required to do the analysis. ProcessPAIR is a novel tool for automated performance analysis and improvement recommendation; based on a performance model calibrated from the performance data of many developers, it automatically identifies and ranks potential performance problems and root causes of individual developers. In education and training environments, it increases students’ autonomy and reduces instructors’ effort in grading and feedback. In this article, we present the results of a controlled experiment involving 61 software engineering master students, half of whom used ProcessPAIR in a Personal Software Process (PSP) performance analysis assignment, and the other half used a traditional PSP support tool (Process Dashboard) for performing the same assignment. The results show significant benefits in terms of students’ satisfaction (average score of 4.78 in a 1–5 scale for ProcessPAIR users, against 3.81 for Process Dashboard users), quality of the analysis outcomes (average grades achieved of 88.1 in a 0–100 scale for ProcessPAIR users, against 82.5 for Process Dashboard users), and time required to do the analysis (average of 252 min for ProcessPAIR users, against 262 min for Process Dashboard users, but with much room for improvement).},
	number = {3},
	journal = {Software Quality Journal},
	author = {Raza, Mushtaq and Faria, João Pascoal and Salazar, Rafael},
	month = sep,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Controlled experiment, Performance analysis, Personal software process, Software engineering education},
	pages = {1209--1237},
}

@inproceedings{horschig_java_2018,
	address = {New York, NY, USA},
	series = {Programming '18},
	title = {Do {Java} programmers write better {Python}? {Studying} off-language code quality on {GitHub}},
	isbn = {978-1-4503-5513-1},
	url = {https://doi.org/10.1145/3191697.3214341},
	doi = {10.1145/3191697.3214341},
	abstract = {There are style guides and best practices for many programming languages. Their goal is to promote uniformity and readability of code, consequentially reducing the chance of errors. While programmers who are frequently using the same programming language tend to internalize most of its best practices eventually, little is known about what happens when they casually switch languages and write code in a less familiar language. Insights into the factors that lead to coding convention violations could help to improve tutorials for programmers switching languages, make teachers aware of mistakes they might expect depending on what language students have been using before, or influence the order in which programming languages are taught. To approach this question, we make use of a large-scale data set representing a major part of the open source development activity happening on GitHub. In this data set, we search for Java and C++ programmers that occasionally program Python and study their Python code quality using a lint tool. Comparing their defect rates to those from Python programmers reveals significant effects in both directions: We observe that some of Python's best practices have more widespread adoption among Java and C++ programmers than Python experts. At the same time, python-specific coding conventions, especially indentation, scoping, and the use of semicolons, are violated more frequently. We conclude that programming off-language is not generally associated with better or worse code quality, but individual coding conventions are violated more or less frequently depending on whether they are more universal or language-specific. We intend to motivate a discussion and more research on what causes these effects, how we can mitigate or use them for good, and which related effects can be studied using the presented data set.},
	booktitle = {Companion {Proceedings} of the 2nd {International} {Conference} on the {Art}, {Science}, and {Engineering} of {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Horschig, Siegfried and Mattis, Toni and Hirschfeld, Robert},
	year = {2018},
	note = {event-place: Nice, France},
	keywords = {code quality, best practices, explorative study, github, lint},
	pages = {127--134},
}

@article{stevens_querying_2019,
	title = {Querying distilled code changes to extract executable transformations},
	volume = {24},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-018-9644-3},
	doi = {10.1007/s10664-018-9644-3},
	abstract = {Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study.},
	number = {1},
	journal = {Empirical Softw. Engg.},
	author = {Stevens, Reinout and Molderez, Tim and Roover, Coen},
	month = feb,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Change distilling, Change querying, Logic meta-programming},
	pages = {491--535},
}

@inproceedings{gazzillo_kmax_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {Kmax: finding all configurations of {Kbuild} makefiles statically},
	isbn = {978-1-4503-5105-8},
	url = {https://doi.org/10.1145/3106237.3106283},
	doi = {10.1145/3106237.3106283},
	abstract = {Feature-oriented software design is a useful paradigm for building and reasoning about highly-configurable software. By making variability explicit, feature-oriented tools and languages make program analysis tasks easier, such as bug-finding, maintenance, and more. But critical software, such as Linux, coreboot, and BusyBox rely instead on brittle tools, such as Makefiles, to encode variability, impeding variability-aware tool development. Summarizing Makefile behavior for all configurations is difficult, because Makefiles have unusual semantics, and exhaustive enumeration of all configurations is intractable in practice. Existing approaches use ad-hoc heuristics, missing much of the encoded variability in Makefiles. We present Kmax, a new static analysis algorithm and tool for Kbuild Makefiles. It is a family-based variability analysis algorithm, where paths are Boolean expressions of configuration options, called reaching configurations, and its abstract state enumerates string values for all configurations. Kmax localizes configuration explosion to the statement level, making precise analysis tractable. The implementation analyzes Makefiles from the Kbuild build system used by several low-level systems projects. Evaluation of Kmax on the Linux and BusyBox build systems shows it to be accurate, precise, and fast. It is the first tool to collect all source files and their configurations from Linux. Compared to previous approaches, Kmax is far more accurate and precise, performs with little overhead, and scales better.},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Gazzillo, Paul},
	year = {2017},
	note = {event-place: Paderborn, Germany},
	keywords = {Configuration, Kbuild, Kmax, Makefiles, Static Analysis, Variability},
	pages = {279--290},
}

@article{sultana_study_2019,
	title = {A study examining relationships between micro patterns and security vulnerabilities},
	volume = {27},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-017-9397-z},
	doi = {10.1007/s11219-017-9397-z},
	abstract = {Software security is an integral part of software quality and reliability. Software vulnerabilities make the software susceptible to attacks which violates software security. Metric-based software vulnerability prediction is one way to evaluate vulnerabilities beforehand so that developers can take preventative measures against attacks. In this study, we explore the correlation between software vulnerabilities and code-level constructs called micro patterns. These code patterns characterize class-level object-oriented program features. Existing research addressed micro pattern correlation with software defects. We analyzed the correlation between vulnerabilities and micro patterns from different viewpoints and explored whether they are related. We studied the distribution of micro patterns and their associations with vulnerable classes in 42 versions of the Apache Tomcat and three Java web applications. This study shows that certain micro patterns are frequently present in vulnerable classes. We also show that there is a high correlation between certain patterns that coexist in a vulnerable class.},
	number = {1},
	journal = {Software Quality Journal},
	author = {Sultana, Kazi Zakia and Williams, Byron J. and Bhowmik, Tanmay},
	month = mar,
	year = {2019},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Software quality, Software security, Micro patterns, Software vulnerabilities},
	pages = {5--41},
}

@inproceedings{tan_anti-patterns_2016,
	address = {New York, NY, USA},
	series = {{FSE} 2016},
	title = {Anti-patterns in search-based program repair},
	isbn = {978-1-4503-4218-6},
	url = {https://doi.org/10.1145/2950290.2950295},
	doi = {10.1145/2950290.2950295},
	abstract = {Search-based program repair automatically searches for a program fix within a given repair space. This may be accomplished by retrofitting a generic search algorithm for program repair as evidenced by the GenProg tool, or by building a customized search algorithm for program repair as in SPR. Unfortunately, automated program repair approaches may produce patches that may be rejected by programmers, because of which past works have suggested using human-written patches to produce templates to guide program repair. In this work, we take the position that we will not provide templates to guide the repair search because that may unduly restrict the repair space and attempt to overfit the repairs into one of the provided templates. Instead, we suggest the use of a set of anti-patterns — a set of generic forbidden transformations that can be enforced on top of any search-based repair tool. We show that by enforcing our anti-patterns, we obtain repairs that localize the correct lines or functions, involve less deletion of program functionality, and are mostly obtained more efficiently. Since our set of anti-patterns are generic, we have integrated them into existing search based repair tools, including GenProg and SPR, thereby allowing us to obtain higher quality program patches with minimal effort.},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Shin Hwei and Yoshida, Hiroaki and Prasad, Mukul R. and Roychoudhury, Abhik},
	year = {2016},
	note = {event-place: Seattle, WA, USA},
	keywords = {and repair, Debugging, fault localization},
	pages = {727--738},
}

@inproceedings{bernabe_faat_2015,
	address = {New York, NY, USA},
	series = {{TEEM} '15},
	title = {Faat: freelance as a team},
	isbn = {978-1-4503-3442-6},
	url = {https://doi.org/10.1145/2808580.2808685},
	doi = {10.1145/2808580.2808685},
	abstract = {Agile methodologies are reliable engineering and management practices, capable of helping in the development of quality and successful software in business environments. However, most of these methodologies are centered on a development team and its internal communication. Moreover, for simplicity, a single product development is taken into account with its successive releases. There is another scenario: that of a single programmer working alone and often in much smaller projects and in several at the same time. Also in this scenario the client proximity is not as described by the agile environment ideal. In that case, the priorities and needs change, communication takes on another meaning and working mechanisms are not always comparable to that of a team. This paper introduces Faat (Freelance as a Team), a methodology specifically designed for those professionals. Integrating existing practices to the needs and possibilities of an individual programmer. However, it has been frequently considered the possible application of this methodology to small teams and/or other more general scenarios. This methodology has been tested in the web-based learning applications.},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Technological} {Ecosystems} for {Enhancing} {Multiculturality}},
	publisher = {Association for Computing Machinery},
	author = {Bernabé, Rodrigo Borrego and Navia, Iván Álvarez and García-Peñalvo, Francisco José},
	year = {2015},
	note = {event-place: Porto, Portugal},
	keywords = {agile methodology, development process, personal software process},
	pages = {687--694},
}

@article{ying_refactoring_2012,
	title = {Refactoring {Flash} {Embedding} {Methods}},
	volume = {3},
	issn = {1947-3052},
	url = {https://doi.org/10.4018/jssoe.2012070102},
	doi = {10.4018/jssoe.2012070102},
	abstract = {Flash and Ajax are currently two popular Rich Internet Application RIA technologies, integrating Flash and Ajax will further enhance Internet users' experiences. To communicate Flash written in ActionScript with Ajax written in JavaScript, the first step is to embed Flash content into a web page. Two methods can be used: markup-based Flash embedding methods and JavaScript-based Flash embedding methods. However, the drawbacks of markup-based Flash embedding methods make JavaScript-based Flash embedding methods a better solution. To automatically convert markup-based Flash embedding methods into a JavaScript-based method, this paper presents a refactoring tool, called FlashembedRT, to assist programmers with the transformation. This tool refactors the five different markup-based Flash embedding methods to the JavaScript-based Flash embedding method called flashembed.},
	number = {3},
	journal = {Int. J. Syst. Serv.-Oriented Eng.},
	author = {Ying, Ming and Miller, James},
	month = jul,
	year = {2012},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Refactoring, JavaScript, ActionScript, Asynchronous JavaScript and XML Ajax, Flash, Rich Internet Application RIA},
	pages = {26--40},
}

@inproceedings{moazeni_software_2014,
	address = {New York, NY, USA},
	series = {{ICSSP} 2014},
	title = {Software domains in incremental development productivity decline},
	isbn = {978-1-4503-2754-1},
	url = {https://doi.org/10.1145/2600821.2600830},
	doi = {10.1145/2600821.2600830},
	abstract = {This research paper expands on a previously introduced phenomenon called Incremental Development Productivity Decline (IDPD) that is presumed to be present in all incremental software projects to some extent. Incremental models are now being used by many organizations in order to reduce development risks. Incremental development has become the most common method of software development. Therefore its characteristics inevitably influence the productivity of projects. Based on their observed IDPD, incrementally developed projects are split into several major IDPD categories. Different ways of measuring productivity are presented and evaluated in order to come to a definition or set of definitions that is suitable to these categories of projects. Data has been collected and analyzed, indicating the degree of IDPD associated with each category. Several hypotheses have undergone preliminary evaluations regarding the existence, stability and category-dependence of IDPD with encouraging results. Further data collection and hypothesis testing is underway.},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Software} and {System} {Process}},
	publisher = {Association for Computing Machinery},
	author = {Moazeni, Ramin and Link, Daniel and Chen, Celia and Boehm, Barry},
	year = {2014},
	note = {event-place: Nanjing, China},
	keywords = {Software engineering, incremental development, productivity decline, statistics},
	pages = {75--83},
}

@inproceedings{almorsy_supporting_2012,
	address = {New York, NY, USA},
	series = {{ASE} '12},
	title = {Supporting automated vulnerability analysis using formalized vulnerability signatures},
	isbn = {978-1-4503-1204-2},
	url = {https://doi.org/10.1145/2351676.2351691},
	doi = {10.1145/2351676.2351691},
	abstract = {Adopting publicly accessible platforms such as cloud computing model to host IT systems has become a leading trend. Although this helps to minimize cost and increase availability and reachability of applications, it has serious implications on applications’ security. Hackers can easily exploit vulnerabilities in such publically accessible services. In addition to, 75\% of the total reported application vulnerabilities are web application specific. Identifying such known vulnerabilities as well as newly discovered vulnerabilities is a key challenging security requirement. However, existing vulnerability analysis tools cover no more than 47\% of the known vulnerabilities. We introduce a new solution that supports automated vulnerability analysis using formalized vulnerability signatures. Instead of depending on formal methods to locate vulnerability instances where analyzers have to be developed to locate specific vulnerabilities, our approach incorporates a formal vulnerability signature described using OCL. Using this formal signature, we perform program analysis of the target system to locate signature matches (i.e. signs of possible vulnerabilities). A newly–discovered vulnerability can be easily identified in a target program provided that a formal signature for it exists. We have developed a prototype static vulnerability analysis tool based on our formalized vulnerability signatures specification approach. We have validated our approach in capturing signatures of the OWSAP Top10 vulnerabilities and applied these signatures in analyzing a set of seven benchmark applications.},
	booktitle = {Proceedings of the 27th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Almorsy, Mohamed and Grundy, John and Ibrahim, Amani S.},
	year = {2012},
	note = {event-place: Essen, Germany},
	keywords = {Software security, Common weaknesses enumeration (CWE), Formal vulnerability specification, Vulnerability analysis},
	pages = {100--109},
}

@article{rhein_variability-aware_2018,
	title = {Variability-{Aware} {Static} {Analysis} at {Scale}: {An} {Empirical} {Study}},
	volume = {27},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3280986},
	doi = {10.1145/3280986},
	abstract = {The advent of variability management and generator technology enables users to derive individual system variants from a configurable code base by selecting desired configuration options. This approach gives rise to the generation of possibly billions of variants, which, however, cannot be efficiently analyzed for bugs and other properties with classic analysis techniques. To address this issue, researchers and practitioners have developed sampling heuristics and, recently, variability-aware analysis techniques. While sampling reduces the analysis effort significantly, the information obtained is necessarily incomplete, and it is unknown whether state-of-the-art sampling techniques scale to billions of variants. Variability-aware analysis techniques process the configurable code base directly, exploiting similarities among individual variants with the goal of reducing analysis effort. However, while being promising, so far, variability-aware analysis techniques have been applied mostly only to small academic examples. To learn about the mutual strengths and weaknesses of variability-aware and sample-based static-analysis techniques, we compared the two by means of seven concrete control-flow and data-flow analyses, applied to five real-world subject systems: Busybox, OpenSSL, SQLite, the x86 Linux kernel, and uClibc. In particular, we compare the efficiency (analysis execution time) of the static analyses and their effectiveness (potential bugs found). Overall, we found that variability-aware analysis outperforms most sample-based static-analysis techniques with respect to efficiency and effectiveness. For example, checking all variants of OpenSSL with a variability-aware static analysis is faster than checking even only two variants with an analysis that does not exploit similarities among variants.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Rhein, Alexander Von and Liebig, JöRG and Janker, Andreas and Kästner, Christian and Apel, Sven},
	month = nov,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {configuration sampling, Highly configurable systems, TypeChef, variability-aware analysis},
}

@inproceedings{weitz_type_2014,
	address = {New York, NY, USA},
	series = {{ISSTA} 2014},
	title = {A type system for format strings},
	isbn = {978-1-4503-2645-2},
	url = {https://doi.org/10.1145/2610384.2610417},
	doi = {10.1145/2610384.2610417},
	abstract = {Most programming languages support format strings, but their use is error-prone. Using the wrong format string syntax, or passing the wrong number or type of arguments, leads to unintelligible text output, program crashes, or security vulnerabilities. This paper presents a type system that guarantees that calls to format string APIs will never fail. In Java, this means that the API will not throw exceptions. In C, this means that the API will not return negative values, corrupt memory, etc. We instantiated this type system for Java’s Formatter API, and evaluated it on 6 large and well-maintained open-source projects. Format string bugs are common in practice (our type system found 104 bugs), and the annotation burden on the user of our type system is low (on average, for every bug found, only 1.0 annotations need to be written).},
	booktitle = {Proceedings of the 2014 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Weitz, Konstantin and Kim, Gene and Srisakaokul, Siwakorn and Ernst, Michael D.},
	year = {2014},
	note = {event-place: San Jose, CA, USA},
	keywords = {static analysis, Format string, printf, type system},
	pages = {127--137},
}

@article{lopez-fernandez_combining_2016,
	title = {Combining unit and specification-based testing for meta-model validation and verification},
	volume = {62},
	issn = {0306-4379},
	url = {https://doi.org/10.1016/j.is.2016.06.008},
	doi = {10.1016/j.is.2016.06.008},
	abstract = {Meta-models play a cornerstone role in Model-Driven Engineering as they are used to define the abstract syntax of modelling languages, and so models and all sorts of model transformations depend on them. However, there are scarce tools and methods supporting their Validation and Verification (V\&amp;V), which are essential activities for the proper engineering of meta-models.In order to fill this gap, we propose two complementary meta-model V\&amp;V languages. The first one has similar philosophy to the xUnit framework, as it enables the definition of meta-model unit test suites comprising model fragments and assertions on their (in-)correctness. The second one is directed to express and verify expected properties of a meta-model, including domain and design properties, quality criteria and platform-specific requirements.As a proof of concept, we have developed tooling for both languages in the Eclipse platform, and illustrate its use within an example-driven approach for meta-model construction. The expressiveness of our languages is demonstrated by their application to build a library of meta-model quality issues, which has been evaluated over the ATL zoo of meta-models and some OMG specifications. The results show that integrated support for meta-model V\&amp;V (as the one we propose here) is urgently needed in meta-modelling environments. HighlightsWe propose two domain-specific languages for meta-model validation and verification.mmUnit enables meta-model unit test definitions with model fragments and assertions.mmSpec is directed to express and verify expected properties of a meta-model.We have developed tooling for both languages in the Eclipse platform.The languages have been evaluated over large real world meta-model repositories.},
	number = {C},
	journal = {Inf. Syst.},
	author = {López-Fernández, Jesús J. and Guerra, Esther and de Lara, Juan},
	month = dec,
	year = {2016},
	note = {Place: GBR
Publisher: Elsevier Science Ltd.},
	keywords = {Domain-specific modelling languages, Meta-model quality, Meta-modelling, Model-driven engineering, Validation \&amp, verification},
	pages = {104--135},
}

@article{pereira_learning_2021,
	title = {Learning software configuration spaces: {A} systematic literature review},
	volume = {182},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2021.111044},
	doi = {10.1016/j.jss.2021.111044},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and Jézéquel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
	month = dec,
	year = {2021},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Systematic literature review, Configurable systems, Machine learning, Software product lines},
}

@article{sulieman_toward_2016,
	title = {Toward {Social}-{Semantic} {Recommender} {Systems}},
	volume = {7},
	issn = {1941-868X},
	url = {https://doi.org/10.4018/IJISSC.2016010101},
	doi = {10.4018/IJISSC.2016010101},
	abstract = {In this article, the authors consider the basic problem of recommender systems that is identifying a set of users to whom a given item is to be recommended. In practice recommender systems are run against huge sets of users, and the problem is then to avoid scanning the whole user set in order to produce the recommendation list. To cope with problem, they consider that users are connected through a social network and that taxonomy over the items has been defined. These two kinds of information are respectively called social and semantic information. In their contribution the authors suggest combining social information with semantic information in one algorithm in order to compute recommendation lists by visiting a limited part of the social network. In their experiments, the authors use two real data sets, namely Amazon.com and MovieLens, and they compare their algorithms with the standard item-based collaborative filtering and hybrid recommendation algorithms. The results show satisfying accuracy values and a very significant improvement of performance, by exploring a small part of the graph instead of exploring the whole graph.},
	number = {1},
	journal = {Int. J. Inf. Syst. Soc. Chang.},
	author = {Sulieman, Dalia and Malek, Maria and Kadima, Hubert and Laurent, Dominique},
	month = jan,
	year = {2016},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Collaborative Filtering, Information Retrieval, Ontology, Recommender Systems, Social Network Analysis},
	pages = {1--30},
}

@book{noauthor_iticse_2015,
	address = {New York, NY, USA},
	title = {{ITiCSE} '15: {Proceedings} of the 2015 {ACM} {Conference} on {Innovation} and {Technology} in {Computer} {Science} {Education}},
	isbn = {978-1-4503-3440-2},
	abstract = {Welcome to ITiCSE 2015 in Vilnius!The ITiCSE conference celebrates its 20th anniversary in Vilnius, the capital of Lithuania and the geographical center of Europe, so declared in 1989 by scientists of the French National Institute of Geography.ITiCSE will be held on July 6–8, starting on Lithuania's Statehood Day (July 6). This is an annual public holiday that commemorates the coronation in 1253 of Mindaugas as the first and only King of Lithuania. The conference venue is the Parliament buildings (Seimas) of the Republic of Lithuania, and the conference dinner is to served in the reconstructed Palace of the Grand Dukes of Lithuania, one of the most famous in Europe in the 15–17th centuries. ITiCSE 2015 is hosted by Vilnius University, one of the oldest and most famous establishments of higher education in Eastern and Central Europe, founded in 1579. The conference organizers represent the Lithuanian research group of Informatics and Informatics Engineering Didactics at the Institute of Mathematics and Informatics of Vilnius University.This conference brings together delegates from all over the world to address pressing issues in computing education. In addition to invited lectures, papers, panels, posters, and tips, techniques \&amp; courseware sessions, the conference provides facilities and exposure for working groups and exhibitions.The conference continues to be truly international with a total of 170 submissions from 40 countries on six continents, with authors from Africa (4), Asia (50), Europe (151), North America (119), Oceania (51), and South America (17). These submissions consisted of 124 research papers, 1 panel, 9 working group proposals, and 36 proposals for posters or for tips, techniques \&amp; courseware.All research papers were double blind reviewed by at least four reviewers, though most papers received five or six reviews. A meta-review was conducted by the members of the conference committee to ensure the reliability of the reviews and to make recommendations to the chairs. A final selection phase was conducted by the program chairs who reviewed all reviews and meta-review recommendations before making their final decisions. As a result of this process, 54 research papers (43.5\%) were selected for presentation and inclusion in the proceedings. The authors of the accepted papers come from 17 different countries on five continents.All poster submissions were blind reviewed by two members of the conference committee, and tips, techniques \&amp; courseware submissions were blind reviewed by three members of the conference committee. Submissions in these categories were then reviewed by the conference chair before selection by the program chairs for final inclusion in the conference. Twenty-four were accepted, representing authors from 15 countries.The two keynote speakers address the learning of programming and computational thinking. Professor Mordechai (Moti) Ben-Ari from the Weizmann Institute of Science, Israel, will give a talk titled In Defense of Programming, which defends the (perhaps controversial) position that programming is the fundamental activity of CS. In the other keynote talk Professor Maciej M. Syslo from Nicolaus Copernicus University and University of Wrocław, Poland, will address algorithmic nand computational thinking as the way to computing for all students.ITiCSE is famous for its working groups. Participating in a working group provides a unique opportunity to work with people from different countries who are interested and knowledgeable in the area of the working group. It is also one of the best ways to become part of the ITiCSE community. Seven working groups have been accepted over a broad spectrum of topics. The working groups range from general topics, such as computing education terminology, CS education in K-9 and K-12 schools, and designing an IT curriculum framework for graduates in 2025, to more specific topics such as developing a repository for high school CS questions, visual assessment tools and metadata annotations, and how students construct solutions to programming problems. The leaders of the accepted working groups come from over 13 countries.Welcome to Vilnius and enjoy the vicennial ITiCSE conference and Lithuania's Statehood Day!},
	publisher = {Association for Computing Machinery},
	year = {2015},
}

@book{noauthor_sbqs_2022,
	address = {New York, NY, USA},
	title = {{SBQS} '22: {Proceedings} of the {XXI} {Brazilian} {Symposium} on {Software} {Quality}},
	isbn = {978-1-4503-9999-9},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_esecfse_2023,
	address = {New York, NY, USA},
	title = {{ESEC}/{FSE} 2023: {Proceedings} of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	isbn = {979-8-4007-0327-0},
	abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_ace_2023,
	address = {New York, NY, USA},
	title = {{ACE} '23: {Proceedings} of the 25th {Australasian} {Computing} {Education} {Conference}},
	isbn = {978-1-4503-9941-8},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_icse_2023-1,
	address = {Melbourne, Victoria, Australia},
	title = {{ICSE} '23: {Proceedings} of the 45th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	isbn = {979-8-3503-2263-7},
	abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
	publisher = {IEEE Press},
	year = {2023},
}

@article{chahal_open_2016,
	title = {Open {Source} {Software} {Evolution}: {A} {Systematic} {Literature} {Review} {Part} 2},
	volume = {7},
	issn = {1942-3926},
	url = {https://doi.org/10.4018/IJOSSP.2016010102},
	doi = {10.4018/IJOSSP.2016010102},
	abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
	number = {1},
	journal = {Int. J. Open Source Softw. Process.},
	author = {Chahal, Kuljit Kaur and Saini, Munish},
	month = jan,
	year = {2016},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {ARIMA Modelling, Automation Support, Co-Evolution, OSS Prediction, Programming Languages, Software Evolution Theory, Software Reuse},
	pages = {28--48},
}

@article{oliveira_how_2022,
	title = {How do developers collaborate? {Investigating} {GitHub} heterogeneous networks},
	volume = {31},
	issn = {0963-9314},
	url = {https://doi.org/10.1007/s11219-022-09598-x},
	doi = {10.1007/s11219-022-09598-x},
	abstract = {Assessing the collaboration among developers is important to understand different aspects of software lifecycle including code smell intensity, bug fixes, and software quality. This kind of collaboration can be obtained from social networks, which represent interactions between individuals in different contexts. In this paper, we model GitHub developers’ collaborations in a heterogeneous network by considering three aspects: social collaboration, collaboration time in a repository and technical features. Then, we explore the GitHub network from different perspectives: size, relevance, and potential applications. The results show the considered metrics are not correlated, bringing new information about the collaborations. We also show that such information is useful for social developer ranking, an actual task which is often part of different applications, such as team formation, community detection and pair programming. Finally, as software quality is intrinsic to the people who code it, our methodology and analyses represent initial steps towards people-centered software quality analysis, as further discussed throughout this article.},
	number = {1},
	journal = {Software Quality Journal},
	author = {Oliveira, Gabriel P. and Moura, Ana Flávia C. and Batista, Natércia A. and Brandão, Michele A. and Hora, Andre and Moro, Mirella M.},
	month = sep,
	year = {2022},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Mining software repositories, Software quality, Collaborative software development, Social coding, Social network metrics},
	pages = {211--241},
}

@inproceedings{kellner_algorithms_2012,
	address = {New York, NY, USA},
	series = {i-{KNOW} '12},
	title = {Algorithms for the verification of the semantic relation between a compound and a given lexeme},
	isbn = {978-1-4503-1242-4},
	url = {https://doi.org/10.1145/2362456.2362463},
	doi = {10.1145/2362456.2362463},
	abstract = {Text mining on a lexical basis is quite well developed for the English language. In compounding languages, however, lexicalized words are often a combination of two or more semantic units. New words can be built easily by concatenating existing ones, without putting any white spaces in between.That poses a problem to existing search algorithms: Such compounds could be of high interest for a search request, but how can be examined whether a compound comprises a given lexeme? A string match can be considered as an indication, but does not prove semantic relation. The same problem is faced when using lexicon based approaches where signal words are defined as lexemes only and need to be identified in all forms of appearance, and hence also as component of a compound. This paper explores the characteristics of compounds and their constituent elements for German, and compares seven algorithms with regard to runtime and error rates. The results of this study are relevant to query analysis and term weighting approaches in information retrieval system design.},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Knowledge} {Management} and {Knowledge} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Kellner, Gudrun and Grünauer, Johannes},
	year = {2012},
	note = {event-place: Graz, Austria},
	keywords = {compound, information retrieval, semantic relation, stemming},
}

@article{zaytsev_grammar_2015,
	title = {Grammar {Zoo}},
	volume = {98},
	issn = {0167-6423},
	url = {https://doi.org/10.1016/j.scico.2014.07.010},
	doi = {10.1016/j.scico.2014.07.010},
	abstract = {In this paper we describe composition of a corpus of grammars in a broad sense in order to enable reuse of knowledge accumulated in the field of grammarware engineering. The Grammar Zoo displays the results of grammar hunting for big grammars of mainstream languages, as well as collecting grammars of smaller DSLs and extracting grammatical knowledge from other places. It is already operational and publicly supplies its users with grammars that have been recovered from different sources of grammar knowledge, varying from official language standards to community-created wiki pages.We summarise recent achievements in the discipline of grammarware engineering, that made the creation of such a corpus possible. We also describe in detail the technology that is used to build and extend such a corpus. The current contents of the Grammar Zoo are listed, as well as some possible future uses for them. The field of grammarware engineering has advanced enough to need a corpus.We composed such a corpus and accumulated many grammars from different sources.Overview of techniques on which our endeavour is based, is given.Current contents of the Grammar Zoo are explained.The general data model for a repository of grammars, is provided.},
	number = {P1},
	journal = {Sci. Comput. Program.},
	author = {Zaytsev, Vadim},
	month = feb,
	year = {2015},
	note = {Place: USA
Publisher: Elsevier North-Holland, Inc.},
	keywords = {Curated corpus, Experimental infrastructure, Grammar recovery, Grammarware engineering},
	pages = {28--51},
}

@article{sampaio_exploring_2016,
	title = {Exploring context-sensitive data flow analysis for early vulnerability detection},
	volume = {113},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2015.12.021},
	doi = {10.1016/j.jss.2015.12.021},
	abstract = {Context-sensitive data-flow analysis improves vulnerability detection.Early detection encourages programmers to promptly fix security vulnerabilities.We built a benchmark for 11 vulnerabilities in order to promote study replications.Our early detector of security vulnerabilities is available at Eclipse marketplace. Secure programming is the practice of writing programs that are resistant to attacks by malicious people or programs. Programmers of secure software have to be continuously aware of security vulnerabilities when writing their program statements. In order to improve programmers' awareness, static analysis techniques have been devised to find vulnerabilities in the source code. However, most of these techniques are built to encourage vulnerability detection a posteriori, only when developers have already fully produced (and compiled) one or more modules of a program. Therefore, this approach, also known as late detection, does not support secure programming but rather encourages posterior security analysis. The lateness of vulnerability detection is also influenced by the high rate of false positives yielded by pattern matching, the underlying mechanism used by existing static analysis techniques. The goal of this paper is twofold. First, we propose to perform continuous detection of security vulnerabilities while the developer is editing each program statement, also known as early detection. Early detection can leverage his knowledge on the context of the code being created, contrary to late detection when developers struggle to recall and fix the intricacies of the vulnerable code they produced from hours to weeks ago. Second, we explore context-sensitive data flow analysis (DFA) for improving vulnerability detection and mitigate the limitations of pattern matching. DFA might be suitable for finding if an object has a vulnerable path. To this end, we have implemented a proof-of-concept Eclipse plugin for continuous DFA-based detection of vulnerabilities in Java programs. We also performed two empirical studies based on several industry-strength systems to evaluate if the code security can be improved through DFA and early vulnerability detection. Our studies confirmed that: (i) the use of context-sensitive DFA significantly reduces the rate of false positives when compared to existing techniques, without being detrimental to the detector performance, and (ii) early detection improves the awareness among developers and encourages programmers to fix security vulnerabilities promptly.},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Sampaio, Luciano and Garcia, Alessandro},
	month = mar,
	year = {2016},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Secure programming, Data flow analysis, Early detection},
	pages = {337--361},
}

@book{noauthor_icse-nier_2023,
	address = {Melbourne, Australia},
	title = {{ICSE}-{NIER} '23: {Proceedings} of the 45th {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	isbn = {979-8-3503-0039-0},
	abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
	publisher = {IEEE Press},
	year = {2023},
}

@inproceedings{barros_editing_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Editing support for software languages: implementation practices in language server protocols},
	isbn = {978-1-4503-9466-6},
	url = {https://doi.org/10.1145/3550355.3552452},
	doi = {10.1145/3550355.3552452},
	abstract = {Effectively using software languages, be it programming or domain-specific languages, requires effective editing support. Modern IDEs, modeling tools, and code editors typically provide sophisticated support to create, comprehend, or modify instances—programs or models—of particular languages. Unfortunately, building such editing support is challenging. While the engineering of languages is well understood and supported by modern model-driven techniques, there is a lack of engineering principles and best practices for realizing their editing support. Especially domain-specific languages—often created by smaller organizations or individual developers, sometimes even for single projects—would benefit from better methods and tools to create proper editing support.We study practices for implementing editing support in 30 so-called language servers—implementations of the language server protocol (LSP). The latter is a recent de facto standard to realize editing support for languages, separated from the editing tools (e.g., IDEs or modeling tools), enhancing the reusability and quality of the editing support. Witnessing the LSP's popularity—a whopping 121 language servers are in existence today—we take this opportunity to analyze the implementations of 30 language servers, some of which support multiple languages. We identify concerns that developers need to take into account when developing editing support, and we synthesize implementation practices to address them, based on a systematic analysis of the servers' source code. We hope that our results shed light on an important technology for software language engineering, that facilitates language-oriented programming and systems development, including model-driven engineering.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Barros, Djonathan and Peldszus, Sven and Assunção, Wesley K. G. and Berger, Thorsten},
	year = {2022},
	note = {event-place: Montreal, Quebec, Canada},
	keywords = {code assistance, implementation practices, language engineering, source code editor},
	pages = {232--243},
}

@book{noauthor_ares_2023,
	address = {New York, NY, USA},
	title = {{ARES} '23: {Proceedings} of the 18th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	isbn = {979-8-4007-0772-8},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_ares_2022,
	address = {New York, NY, USA},
	title = {{ARES} '22: {Proceedings} of the 17th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	isbn = {978-1-4503-9670-7},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_isec_2023,
	address = {New York, NY, USA},
	title = {{ISEC} '23: {Proceedings} of the 16th {Innovations} in {Software} {Engineering} {Conference}},
	isbn = {979-8-4007-0064-4},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{henkel_learning_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {Learning from, understanding, and supporting {DevOps} artifacts for docker},
	isbn = {978-1-4503-7121-6},
	url = {https://doi.org/10.1145/3377811.3380406},
	doi = {10.1145/3377811.3380406},
	abstract = {With the growing use of DevOps tools and frameworks, there is an increased need for tools and techniques that support more than code. The current state-of-the-art in static developer assistance for tools like Docker is limited to shallow syntactic validation. We identify three core challenges in the realm of learning from, understanding, and supporting developers writing DevOps artifacts: (i) nested languages in DevOps artifacts, (ii) rule mining, and (iii) the lack of semantic rule-based analysis. To address these challenges we introduce a toolset, binnacle, that enabled us to ingest 900,000 GitHub repositories.Focusing on Docker, we extracted approximately 178,000 unique Dockerfiles, and also identified a Gold Set of Dockerfiles written by Docker experts. We addressed challenge (i) by reducing the number of effectively uninterpretable nodes in our ASTs by over 80\% via a technique we call phased parsing. To address challenge (ii), we introduced a novel rule-mining technique capable of recovering two-thirds of the rules in a benchmark we curated. Through this automated mining, we were able to recover 16 new rules that were not found during manual rule collection. To address challenge (iii), we manually collected a set of rules for Dockerfiles from commits to the files in the Gold Set. These rules encapsulate best practices, avoid docker build failures, and improve image size and build latency. We created an analyzer that used these rules, and found that, on average, Dockerfiles on GitHub violated the rules five times more frequently than the Dockerfiles in our Gold Set. We also found that industrial Dockerfiles fared no better than those sourced from GitHub.The learned rules and analyzer in binnacle can be used to aid developers in the IDE when creating Dockerfiles, and in a post-hoc fashion to identify issues in, and to improve, existing Dockerfiles.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Henkel, Jordan and Bird, Christian and Lahiri, Shuvendu K. and Reps, Thomas},
	year = {2020},
	note = {event-place: Seoul, South Korea},
	keywords = {docker, DevOps, mining, static checking},
	pages = {38--49},
}

@book{noauthor_icse-seip_2023,
	address = {Melbourne, Australia},
	title = {{ICSE}-{SEIP} '23: {Proceedings} of the 45th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	isbn = {979-8-3503-0037-6},
	publisher = {IEEE Press},
	year = {2023},
}

@inproceedings{hawkins_blackbox_2016,
	address = {New York, NY, USA},
	series = {{CGO} '16},
	title = {{BlackBox}: lightweight security monitoring for {COTS} binaries},
	isbn = {978-1-4503-3778-6},
	url = {https://doi.org/10.1145/2854038.2854062},
	doi = {10.1145/2854038.2854062},
	abstract = {After a software system is compromised, it can be difficult to understand what vulnerabilities attackers exploited. Any information residing on that machine cannot be trusted as attackers may have tampered with it to cover their tracks. Moreover, even after an exploit is known, it can be difficult to determine whether it has been used to compromise a given machine. Aviation has long-used black boxes to better understand the causes of accidents, enabling improvements that reduce the likelihood of future accidents. Many attacks introduce abnormal control flows to compromise systems. In this paper, we present BlackBox, a monitoring system for COTS software. Our techniques enable BlackBox to efficiently monitor unexpected and potentially harmful control flow in COTS binaries. BlackBox constructs dynamic profiles of an application's typical control flows to filter the vast majority of expected control flow behavior, leaving us with a manageable amount of data that can be logged across the network to remote devices. Modern applications make extensive use of dynamically generated code, some of which varies greatly between executions. We introduce support for code generators that can detect security-sensitive behaviors while allowing BlackBox to avoid logging the majority of ordinary behaviors. We have implemented BlackBox in DynamoRIO. We evaluate the runtime overhead of BlackBox, and show that it can effectively monitor recent versions of Microsoft Office and Google Chrome. We show that in ROP, COOP, and state- of-the-art JIT injection attacks, BlackBox logs the pivotal actions by which the attacker takes control, and can also blacklist those actions to prevent repeated exploits.},
	booktitle = {Proceedings of the 2016 {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Hawkins, Byron and Demsky, Brian and Taylor, Michael B.},
	year = {2016},
	note = {event-place: Barcelona, Spain},
	keywords = {Binary Rewriting, Control Flow Integrity, Dynamic Code Generation, Program Monitoring, Software Security},
	pages = {261--272},
}

@book{noauthor_issta_2023,
	address = {New York, NY, USA},
	title = {{ISSTA} 2023: {Proceedings} of the 32nd {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	isbn = {979-8-4007-0221-1},
	abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{ghaedi_heidari_automatic_2023,
	title = {Automatic pattern-based consistency checking in model refactoring: introducing a formal behavioral preserving method},
	volume = {20},
	issn = {1614-5046},
	url = {https://doi.org/10.1007/s11334-022-00525-8},
	doi = {10.1007/s11334-022-00525-8},
	abstract = {Evolution is one of the most important parts of the software development process. One of the negative consequences of software development is design erosion. Refactoring is a technique that aims to prevent this issue. Therefore, refactoring is an important software development process to promote software quality without changing its external behavior. Refactoring at the model level is the same as refactoring at the code level and has similar advantages, and the only difference is that refactoring at the model level, due to its formation over the initial steps of software development process, has a greater impact on cost reduction and efficiency improvement. Timely and consistent utilization of this procedure in a software project has extremely positive long-term impacts, especially when this is done by its technical tools. Then, refactoring will be a rapid, easy, and safe way to promote software system quality. The main idea of this study is the automatic checking of consistency in model refactoring in order to retain model behavior using Alloy modeling language. Thus, by employing structural and behavioral patterns as a reusable and well-defined component, as well as consistency rules, this objective can be achieved.},
	number = {1},
	journal = {Innov. Syst. Softw. Eng.},
	author = {Ghaedi Heidari, Saeedeh and Ajoudanian, Shohreh},
	month = feb,
	year = {2023},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Behavior preservation, Consistency, Alloy modeling language, Design patterns, Model refactoring},
	pages = {65--84},
}

@article{sintaha_katana_2023,
	title = {Katana: {Dual} {Slicing} {Based} {Context} for {Learning} {Bug} {Fixes}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3579640},
	doi = {10.1145/3579640},
	abstract = {Contextual information plays a vital role for software developers when understanding and fixing a bug. Consequently, deep learning based program repair techniques leverage context for bug fixes. However, existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy statement within the enclosing file, class, or method, without any analysis to find actual relations with the bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context. We present a program slicing based approach, in which instead of arbitrarily including code as context, we analyze statements that have a control or data dependency on the buggy statement. We propose a novel concept called dual slicing, which leverages the context of both buggy and fixed versions of the code to capture relevant repair ingredients. We present our technique and tool called Katana, the first to apply slicing-based context for a program repair task. The results show that Katana effectively preserves sufficient information for a model to choose contextual information while reducing noise. We compare against four recent state-of-the-art context-aware program repair techniques. Our results show that Katana fixes between 1.5 and 3.7 times more bugs than existing techniques.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Sintaha, Mifta and Nashid, Noor and Mesbah, Ali},
	month = may,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, contextual information, graph neural networks, program repair, Program slicing},
}

@book{noauthor_fse_2016,
	address = {New York, NY, USA},
	title = {{FSE} 2016: {Proceedings} of the 2016 24th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	isbn = {978-1-4503-4218-6},
	publisher = {Association for Computing Machinery},
	year = {2016},
}

@article{de_dieu_characterizing_2023,
	title = {Characterizing architecture related posts and their usefulness in {Stack} {Overflow}},
	volume = {198},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111608},
	doi = {10.1016/j.jss.2023.111608},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {de Dieu, Musengamana Jean and Liang, Peng and Shahin, Mojtaba and Khan, Arif Ali},
	month = apr,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Stack Overflow, Architectural knowledge, Architectural level element, Architecture solution, Usefulness},
}

@inproceedings{madsen_principles_2022,
	address = {New York, NY, USA},
	series = {Onward! 2022},
	title = {The {Principles} of the {Flix} {Programming} {Language}},
	isbn = {978-1-4503-9909-8},
	url = {https://doi.org/10.1145/3563835.3567661},
	doi = {10.1145/3563835.3567661},
	abstract = {We present the design values and design principles of the Flix programming language, a functional-first, imperative, and logic programming language. We explain how these values and principles came into being and how they have influenced the design of Flix over the last several years. The principles cover most facets of the Flix language and its ecosystem, including its syntax, semantics, static type and effect system, and standard library. We present each principle in detail, including its origin, rationale, and how it has shaped Flix. We believe that codifying a language's design values and principles can serve as a powerful medium for discussing and comparing programming language designs and we hope our presentation will inspire future language designers to document their languages' design values and principles.},
	booktitle = {Proceedings of the 2022 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {Association for Computing Machinery},
	author = {Madsen, Magnus},
	year = {2022},
	note = {event-place: Auckland, New Zealand},
	keywords = {design values and principles, Flix, programming language design},
	pages = {112--127},
}

@inproceedings{muralee_argus_2023,
	address = {USA},
	series = {{SEC} '23},
	title = {{ARGUS}: a framework for staged static taint analysis of {GitHub} workflows and actions},
	isbn = {978-1-939133-37-3},
	abstract = {Millions of software projects leverage automated workflows, like GitHub Actions, for performing common build and deploy tasks. While GitHub Actions have greatly improved the software build process for developers, they pose significant risks to the software supply chain by adding more dependencies and code complexity that may introduce security bugs.This paper presents ARGUS, the first static taint analysis system for identifying code injection vulnerabilities in GitHub Actions. We used ARGUS to perform a large-scale evaluation on 2,778,483 Workflows referencing 31,725 Actions and discovered critical code injection vulnerabilities in 4,307 Workflows and 80 Actions. We also directly compared ARGUS to two existing pattern-based GitHub Actions vulnerability scanners, demonstrating that our system exhibits a marked improvement in terms of vulnerability detection, with a discovery rate more than seven times (7x) higher than the state-of-the-art approaches.These results demonstrate that command injection vulnerabilities in the GitHub Actions ecosystem are not only pervasive but also require taint analysis to be detected.},
	booktitle = {Proceedings of the 32nd {USENIX} {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Muralee, Siddharth and Koishybayev, Igibek and Nahapetyan, Aleksandr and Tystahl, Greg and Reaves, Brad and Bianchi, Antonio and Enck, William and Kapravelos, Alexandros and Machiry, Aravind},
	year = {2023},
	note = {event-place: Anaheim, CA, USA},
}

@article{mondal_survey_2020,
	title = {A survey on clone refactoring and tracking},
	volume = {159},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2019.110429},
	doi = {10.1016/j.jss.2019.110429},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
	month = jan,
	year = {2020},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Clone refactoring, Clone tracking, Clone-types, Code clones},
}

@article{ebert_exploratory_2015,
	title = {An exploratory study on exception handling bugs in {Java} programs},
	volume = {106},
	issn = {0164-1212},
	abstract = {We study exception handling bugs from two real systems.We survey developers to understand their thoughts about exception handling bugs.Analysis of bug repositories shows small percentages of exception handling bugs.Exception handling bugs seems to be as hard to fix as other kind of bugs.We create an exception handling bug classification. Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers' perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat.Only 27\% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70\% of the organizations there are no specific tests for the exception handling code. Also, 61\% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40\% of the respondents consider the quality of exception handling code to be either good or very good and only 14\% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs.Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools.},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Ebert, Felipe and Castor, Fernando and Serebrenik, Alexander},
	month = aug,
	year = {2015},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Repository mining, Bugs, Exception handling},
	pages = {82--101},
}

@inproceedings{bhandari_cvefixes_2021,
	address = {New York, NY, USA},
	series = {{PROMISE} 2021},
	title = {{CVEfixes}: automated collection of vulnerabilities and their fixes from open-source software},
	isbn = {978-1-4503-8680-7},
	url = {https://doi.org/10.1145/3475960.3475985},
	doi = {10.1145/3475960.3475985},
	abstract = {Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
	year = {2021},
	note = {event-place: Athens, Greece},
	keywords = {dataset, Security vulnerabilities, software repository mining, source code repair, vulnerability classification, vulnerability prediction},
	pages = {30--39},
}

@book{noauthor_webmedia_2023,
	address = {New York, NY, USA},
	title = {{WebMedia} '23: {Proceedings} of the 29th {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	isbn = {979-8-4007-0908-1},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{gonzalez_large-scale_2017,
	series = {{MSR} '17},
	title = {A large-scale study on the usage of testing patterns that address maintainability attributes: patterns for ease of modification, diagnoses, and comprehension},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.8},
	doi = {10.1109/MSR.2017.8},
	abstract = {Test case maintainability is an important concern, especially in open source and distributed development environments where projects typically have high contributor turnover with varying backgrounds and experience, and where code ownership changes often. Similar to design patterns, patterns for unit testing promote maintainability quality attributes such as ease of diagnoses, modifiability, and comprehension. In this paper, we report the results of a large-scale study on the usage of four xUnit testing patterns which can be used to satisfy these maintainability attributes. This is a first-of-its-kind study which developed automated techniques to investigate these issues across 82,447 open source projects, and the findings provide more insight into testing practices in open source projects. Our results indicate that only 17\% of projects had test cases, and from the 251 testing frameworks we studied, 93 of them were being used. We found 24\% of projects with test files implemented patterns that could help with maintainability, while the remaining did not use these patterns. Multiple qualitative analyses indicate that usage of patterns was an ad-hoc decision by individual developers, rather than motivated by the characteristics of the project, and that developers sometimes used alternative techniques to address maintainability concerns.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Gonzalez, Danielle and Santos, Joanna C. S. and Popovich, Andrew and Mirakhorli, Mehdi and Nagappan, Mei},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	keywords = {mining software repositories, maintenance, open source, unit test frameworks, unit test patterns, unit testing},
	pages = {391--401},
}

@book{noauthor_chi_2023,
	address = {New York, NY, USA},
	title = {{CHI} '23: {Proceedings} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	isbn = {978-1-4503-9421-5},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{assuncao_how_2023,
	title = {How do microservices evolve? {An} empirical analysis of changes in open-source microservice repositories},
	volume = {204},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111788},
	doi = {10.1016/j.jss.2023.111788},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Assunção, Wesley K.G. and Krüger, Jacob and Mosser, Sébastien and Selaoui, Sofiane},
	month = oct,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Software evolution, Mining software repositories, Microservices, Service-oriented architecture, Software architecture},
}

@article{zhang_diverse_2023,
	title = {Diverse title generation for {Stack} {Overflow} posts with multiple-sampling-enhanced transformer},
	volume = {200},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111672},
	doi = {10.1016/j.jss.2023.111672},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Zhang, Fengji and Liu, Jin and Wan, Yao and Yu, Xiao and Liu, Xiao and Keung, Jacky},
	month = jun,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Stack Overflow, CodeT5, Maximal marginal ranking, Nucleus sampling, Title generation},
}

@book{noauthor_chi_2023-1,
	address = {New York, NY, USA},
	title = {{CHI} {EA} '23: {Extended} {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	isbn = {978-1-4503-9422-2},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{rost_mining_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Mining of {DSLs} and generator templates from reference applications},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3419492},
	doi = {10.1145/3417990.3419492},
	abstract = {Domain-Specific Languages (DSLs) found application in different domains. The development of Model-Driven Development (MDD) components is facilitated by a wealth of frameworks like EMF, Xtext, and Xtend. However, the development of the necessary IDE components still can take up to several weeks or even months until it can be used in a production environment. The first step during the development of such an MDD infrastructure is to analyse a set of reference applications to deduce the DSL used by the domain experts and the templates used in the generator. The analysis requires technical expertise and is usually performed by MDD infrastructure developers, who have to adhere to a close communication with domain experts and are exposed to high cognitive load and time-consuming tasks.The objective of this PhD project is to reduce the initial effort during the creation of new MDD infrastructure facilities for either a new domain or newly discovered platforms within a known domain. This should be made possible by the (semi-)automatic analysis of multiple codebases using Code Clone Detection (CCD) tools in a defined process flow. Code clones represent schematically redundant and generic code fragments which were found in the provided codebase. In the process, the key steps include (i) choosing appropriate reference applications (ii) distinguishing the codebase by clustering the files, (iii) reviewing the quality of the clusters, (iv) analysing the cluster by tailored CCD, and (v) transforming of the code clones, depending on the code clone type, to extract a DSL and the corresponding generator templates.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Rost, Wolf},
	year = {2020},
	note = {event-place: Virtual Event, Canada},
	keywords = {clustering and classification, code clone detection, information extraction, MDD component creation, model-driven software engineering},
}

@book{noauthor_csse_2022,
	address = {New York, NY, USA},
	title = {{CSSE} '22: {Proceedings} of the 5th {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	isbn = {978-1-4503-9778-0},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@inproceedings{canedo_breaking_2021,
	address = {New York, NY, USA},
	series = {{SBES} '21},
	title = {Breaking one barrier at a time: how women developers cope in a men-dominated industry},
	isbn = {978-1-4503-9061-3},
	url = {https://doi.org/10.1145/3474624.3474638},
	doi = {10.1145/3474624.3474638},
	abstract = {Context: Participation of women in software development teams is surrounded by several challenges — including gender bias, the difficulty of engagement, and general acceptance of the teams. Objective: We investigated the women perception in relation to interactions, contributions, gender bias, barriers and challenges that they may face in their work. Method: To achieve this goal, we conducted semi-structured interviews with 17 Brazilian women. We analyzed the data using the principles of Grounded Theory and identified three categories: Challenges, Barriers and Gender Issues. Results: Our findings reveal that tasks considered more complex are allocated to men on the team. Our respondents also commonly observe gender bias from men on the team. Conclusions: The findings indicate that most of the interviewed women observe a sexist behavior amongst the software development team members. Moreover, most of the participants stated that few women perform a leadership role in their team. We close by presenting suggestions to more inclusive work environments.},
	booktitle = {Proceedings of the {XXXV} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Canedo, Edna Dias and Mendes, Fabiana and Cerqueira, Anderson and Okimoto, Marcio and Pinto, Gustavo and Bonifacio, Rodrigo},
	year = {2021},
	note = {event-place: Joinville, Brazil},
	keywords = {barriers, challenges, gender bias, women developers},
	pages = {378--387},
}

@book{noauthor_sac_2023,
	address = {New York, NY, USA},
	title = {{SAC} '23: {Proceedings} of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	isbn = {978-1-4503-9517-5},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{tahaei_security_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Security {Notifications} in {Static} {Analysis} {Tools}: {Developers}’ {Attitudes}, {Comprehension}, and {Ability} to {Act} on {Them}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445616},
	doi = {10.1145/3411764.3445616},
	abstract = {Static analysis tools (SATs) have the potential to assist developers in finding and fixing vulnerabilities in the early stages of software development, requiring them to be able to understand and act on tools’ notifications. To understand how helpful such SAT guidance is to developers, we ran an online experiment (N=132) where participants were shown four vulnerable code samples (SQL injection, hard-coded credentials, encryption, and logging sensitive data) along with SAT guidance, and asked to indicate the appropriate fix. Participants had a positive attitude towards both SAT notifications and particularly liked the example solutions and vulnerable code. Seeing SAT notifications also led to more detailed open-ended answers and slightly improved code correction answers. Still, most SAT (SpotBugs 67\%, SonarQube 86\%) and Control (96\%) participants answered at least one code-correction question incorrectly. Prior software development experience, perceived vulnerability severity, and answer confidence all positively impacted answer accuracy.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tahaei, Mohammad and Vaniea, Kami and Beznosov, Konstantin (Kosta) and Wolters, Maria K},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {usable security, security notifications, software developers, static analysis tools},
}

@inproceedings{alsuhaibani_naming_2021,
	series = {{ICSE} '21},
	title = {On the {Naming} of {Methods}: {A} {Survey} of {Professional} {Developers}},
	isbn = {978-1-4503-9085-9},
	url = {https://doi.org/10.1109/ICSE43902.2021.00061},
	doi = {10.1109/ICSE43902.2021.00061},
	abstract = {This paper describes the results of a large (+1100 responses) survey of professional software developers concerning standards for naming source code methods. The various standards for source code method names are derived from and supported in the software engineering literature. The goal of the survey is to determine if there is a general consensus among developers that the standards are accepted and used in practice. Additionally, the paper examines factors such as years of experience and programming language knowledge in the context of survey responses. The survey results show that participants very much agree about the importance of various standards and how they apply to names and that years of experience and the programming language has almost no effect on their responses. The results imply that the given standards are both valid and to a large degree complete. The work provides a foundation for automated method name assessment during development and code reviews.},
	booktitle = {Proceedings of the 43rd {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Alsuhaibani, Reem S. and Newman, Christian D. and Decker, Michael J. and Collard, Michael L. and Maletic, Jonathan I.},
	year = {2021},
	note = {Place: Madrid, Spain},
	keywords = {coding standards, method names, naming conventions, styling},
	pages = {587--599},
}

@book{noauthor_muc_2023,
	address = {New York, NY, USA},
	title = {{MuC} '23: {Proceedings} of {Mensch} und {Computer} 2023},
	isbn = {979-8-4007-0771-1},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{trinkenreich_womens_2022,
	title = {Women’s {Participation} in {Open} {Source} {Software}: {A} {Survey} of the {Literature}},
	volume = {31},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3510460},
	doi = {10.1145/3510460},
	abstract = {Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on women’s participation in OSS. It focuses on women contributors’ representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated women’s participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5\% of projects were reported to have women as core developers, and women authored less than 5\% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Trinkenreich, Bianca and Wiese, Igor and Sarma, Anita and Gerosa, Marco and Steinmacher, Igor},
	month = aug,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {challenges, bias, female, Gender, motivation},
}

@article{c_assessing_2023,
	title = {Assessing the {Early} {Bird} {Heuristic} (for {Predicting} {Project} {Quality})},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3583565},
	doi = {10.1145/3583565},
	abstract = {Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project.To support this claim, we offer a case study with 240 projects, where we find that the information in those projects “clumps” towards the earliest parts of the project. A quality prediction model learned from just the first 150 commits works as well, or better than state-of-the-art alternatives. Using just this “early bird” data, we can build models very quickly and very early in the project life cycle. Moreover, using this early bird method, we have shown that a simple model (with just a few features) generalizes to hundreds of projects.Based on this experience, we doubt that prior work on generalizing quality models may have needlessly complicated an inherently simple process. Further, prior work that focused on later-life cycle data needs to be revisited, since their conclusions were drawn from relatively uninformative regions.Replication note: All our data and scripts are available here: https://github.com/snaraya7/early-bird.},
	number = {5},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {C., Shrikanth N. and Menzies, Tim},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data-lite, defects, early, Quality prediction},
}

@book{noauthor_assets_2023,
	address = {New York, NY, USA},
	title = {{ASSETS} '23: {Proceedings} of the 25th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	isbn = {979-8-4007-0220-4},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_sosp_2023,
	address = {New York, NY, USA},
	title = {{SOSP} '23: {Proceedings} of the 29th {Symposium} on {Operating} {Systems} {Principles}},
	isbn = {979-8-4007-0229-7},
	abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{janes_open_2023,
	title = {Open tracing tools: {Overview} and critical comparison},
	volume = {204},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2023.111793},
	doi = {10.1016/j.jss.2023.111793},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Janes, Andrea and Li, Xiaozhou and Lenarduzzi, Valentina},
	month = oct,
	year = {2023},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {ChatGPT, Multivocal literature review, Open tracing tool, Telemetry},
}

@book{noauthor_icdcn_2024,
	address = {New York, NY, USA},
	title = {{ICDCN} '24: {Proceedings} of the 25th {International} {Conference} on {Distributed} {Computing} and {Networking}},
	isbn = {979-8-4007-1673-7},
	publisher = {Association for Computing Machinery},
	year = {2024},
}

@book{noauthor_onward_2022,
	address = {New York, NY, USA},
	title = {Onward! 2022: {Proceedings} of the 2022 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	isbn = {978-1-4503-9909-8},
	abstract = {Welcome to Onward! 2022. Onward! is a premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities, and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research. Onward! 2022 is part of SPLASH 2022, taking place from Monday 5th to Saturday 10th December 2022 in Auckland, New Zealand.},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{garousi_model-based_2021,
	title = {Model-based testing in practice: {An} experience report from the web applications domain},
	volume = {180},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2021.111032},
	doi = {10.1016/j.jss.2021.111032},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Garousi, Vahid and Keleş, Alper Buğra and Balaman, Yunus and Güler, Zeynep Özdemir and Arcuri, Andrea},
	month = oct,
	year = {2021},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Web applications, Software testing, Applied research report, Experience report, Model-based testing, Test automation},
}

@book{noauthor_sbcars_2022,
	address = {New York, NY, USA},
	title = {{SBCARS} '22: {Proceedings} of the 16th {Brazilian} {Symposium} on {Software} {Components}, {Architectures}, and {Reuse}},
	isbn = {978-1-4503-9745-2},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icibe_2022,
	address = {New York, NY, USA},
	title = {{ICIBE} '22: {Proceedings} of the 8th {International} {Conference} on {Industrial} and {Business} {Engineering}},
	isbn = {978-1-4503-9758-2},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_idc_2023,
	address = {New York, NY, USA},
	title = {{IDC} '23: {Proceedings} of the 22nd {Annual} {ACM} {Interaction} {Design} and {Children} {Conference}},
	isbn = {979-8-4007-0131-3},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_icetc_2023,
	address = {New York, NY, USA},
	title = {{ICETC} '23: {Proceedings} of the 15th {International} {Conference} on {Education} {Technology} and {Computers}},
	isbn = {979-8-4007-0911-1},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{noauthor_eitce_2022,
	address = {New York, NY, USA},
	title = {{EITCE} '22: {Proceedings} of the 2022 6th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	isbn = {978-1-4503-9714-8},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_umap_2023,
	address = {New York, NY, USA},
	title = {{UMAP} '23 {Adjunct}: {Adjunct} {Proceedings} of the 31st {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	isbn = {978-1-4503-9891-6},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{bird_art_2015,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {The {Art} and {Science} of {Analyzing} {Software} {Data}},
	isbn = {0-12-411519-5},
	abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
	year = {2015},
}

@book{noauthor_icemc_2022,
	address = {New York, NY, USA},
	title = {{ICEMC} '22: {Proceedings} of the 2022 {International} {Conference} on {E}-business and {Mobile} {Commerce}},
	isbn = {978-1-4503-9716-2},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icfnds_2022,
	address = {New York, NY, USA},
	title = {{ICFNDS} '22: {Proceedings} of the 6th {International} {Conference} on {Future} {Networks} \&amp; {Distributed} {Systems}},
	isbn = {978-1-4503-9905-0},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icimmi_2022,
	address = {New York, NY, USA},
	title = {{ICIMMI} '22: {Proceedings} of the 4th {International} {Conference} on {Information} {Management} \&amp; {Machine} {Intelligence}},
	isbn = {978-1-4503-9993-7},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{batoun_empirical_2023,
	title = {An {Empirical} {Study} on {GitHub} {Pull} {Requests}’ {Reactions}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3597208},
	doi = {10.1145/3597208},
	abstract = {The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., “Thumbs-up”, “Laugh”, “Hooray”, “Heart”, “Rocket”, “Thumbs-down”, “Confused”, and “Eyes”. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests’ comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100\% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., “Thumbs-up”, “Hooray”, “Heart”, “Rocket”, and “Laugh”). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95\% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40\%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the “end-users” of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request’s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests’ reactions.},
	number = {6},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Batoun, Mohamed Amine and Yung, Ka Lai and Tian, Yuan and Sayagh, Mohammed},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {feedback, GitHub reactions, pull requests, software collaboration},
}

@book{noauthor_chi_2021,
	address = {New York, NY, USA},
	title = {{CHI} '21: {Proceedings} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	isbn = {978-1-4503-8096-6},
	publisher = {Association for Computing Machinery},
	year = {2021},
}

@inproceedings{golubev_one_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {One thousand and one stories: a large-scale survey of software refactoring},
	isbn = {978-1-4503-8562-6},
	url = {https://doi.org/10.1145/3468264.3473924},
	doi = {10.1145/3468264.3473924},
	abstract = {Despite the availability of refactoring as a feature in popular IDEs, recent studies revealed that developers are reluctant to use them, and still prefer the manual refactoring of their code. At JetBrains, our goal is to fully support refactoring features in IntelliJ-based IDEs and improve their adoption in practice. Therefore, we start by raising the following main questions. How exactly do people refactor code? What refactorings are the most popular? Why do some developers tend not to use convenient IDE refactoring tools? In this paper, we investigate the raised questions through the design and implementation of a survey targeting 1,183 users of IntelliJ-based IDEs. Our quantitative and qualitative analysis of the survey results shows that almost two-thirds of developers spend more than one hour in a single session refactoring their code; that refactoring types vary greatly in popularity; and that a lot of developers would like to know more about IDE refactoring features but lack the means to do so. These results serve us internally to support the next generation of refactoring features, as well as can help our research community to establish new directions in the refactoring usability research.},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Golubev, Yaroslav and Kurbatova, Zarina and AlOmar, Eman Abdullah and Bryksin, Timofey and Mkaouer, Mohamed Wiem},
	year = {2021},
	note = {event-place: Athens, Greece},
	keywords = {IDE Refactoring Features, Refactorings, Software Maintenance},
	pages = {1303--1313},
}

@book{noauthor_webmedia_2022,
	address = {New York, NY, USA},
	title = {{WebMedia} '22: {Proceedings} of the {Brazilian} {Symposium} on {Multimedia} and the {Web}},
	isbn = {978-1-4503-9409-3},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icse-seis_2023,
	address = {Melbourne, Australia},
	title = {{ICSE}-{SEIS} '23: {Proceedings} of the 45th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society}},
	isbn = {979-8-3503-2261-3},
	abstract = {We are delighted to introduce the Software Engineering in Society (SEIS) track program as part of the 45th IEEE/ACM International Conference on Software Engineering, to be held in Melbourne, Australia, on May 14-20, 2023. The aim of the track is to bring together researchers studying various roles that software engineering plays in society.},
	publisher = {IEEE Press},
	year = {2023},
}

@book{noauthor_gpce_2016,
	address = {New York, NY, USA},
	title = {{GPCE} 2016: {Proceedings} of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	isbn = {978-1-4503-4446-3},
	publisher = {Association for Computing Machinery},
	year = {2016},
}

@book{noauthor_chi_2021-1,
	address = {New York, NY, USA},
	title = {{CHI} {EA} '21: {Extended} {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	isbn = {978-1-4503-8095-9},
	publisher = {Association for Computing Machinery},
	year = {2021},
}

@inproceedings{nogueira-leite_cautionary_2021,
	address = {Berlin, Heidelberg},
	title = {A {Cautionary} {Tale} on {Using} {Covid}-19 {Data} for {Machine} {Learning}},
	isbn = {978-3-030-77210-9},
	url = {https://doi.org/10.1007/978-3-030-77211-6_30},
	doi = {10.1007/978-3-030-77211-6_30},
	abstract = {Introduction: Good quality and real-time epidemiological COVID-19 data are paramount to fight this pandemic through statistical/machine-learning based decision-making support mechanisms.Aims: Evaluate the resources available and used to gather COVID-19 epidemiological data by Portuguese health authorities from the onset of the pandemic until December 2020. The analysis laid on two main topics: (a) work processes at the Public Health Unit (PHU) level and (b) registry forms for epidemiological reporting and control procedures. Recommendations on requirements to overcome problems related to data integration and interoperability in order to build robust decision-making support mechanisms will also be produced.Methods: For topic (a), we revised the Portuguese Directorate-General of Health (DGS) guidelines for data treatment. For topic (b), we analysed the forms used during first and second waves, while comparing them with DGS metadata provided to researchers.Results: On topic (a), we detected the use of two complementary and non-interoperable systems. Further, the workflow does not seem to promote data quality and facilitates the occurrence of communication problems between health professionals. On topic (b), we found 27 deleted questions, 6 new questions, 1 displaced question, and 1 text modification between the 2 form versions.Discussion: Both the workflow and data gathering methods are not the best suited for the generation of good quality data. They do not effectively support Public Health Professionals (PHP) nor provide the elements for posterior data analysis. The use of data by decision-making support mechanisms demands a careful planning of the data used to depict reality, and this condition is not met by the currently used forms.},
	booktitle = {Artificial {Intelligence} in {Medicine}: 19th {International} {Conference} on {Artificial} {Intelligence} in {Medicine}, {AIME} 2021, {Virtual} {Event}, {June} 15–18, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Nogueira-Leite, Diogo and Alves, João Miguel and Marques-Cruz, Manuel and Cruz-Correia, Ricardo},
	year = {2021},
	keywords = {Data quality, Healthcare processes, Policy making, Public health surveillance, Workflow management},
	pages = {265--275},
}

@book{noauthor_siet_2023,
	address = {New York, NY, USA},
	title = {{SIET} '23: {Proceedings} of the 8th {International} {Conference} on {Sustainable} {Information} {Engineering} and {Technology}},
	isbn = {979-8-4007-0850-3},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@article{dwivedi_opinion_2023,
	title = {Opinion {Paper}: “{So} what if {ChatGPT} wrote it?” {Multidisciplinary} perspectives on opportunities, challenges and implications of generative conversational {AI} for research, practice and policy},
	volume = {71},
	issn = {0268-4012},
	url = {https://doi.org/10.1016/j.ijinfomgt.2023.102642},
	doi = {10.1016/j.ijinfomgt.2023.102642},
	number = {C},
	journal = {Int. J. Inf. Manag.},
	author = {Dwivedi, Yogesh K. and Kshetri, Nir and Hughes, Laurie and Slade, Emma Louise and Jeyaraj, Anand and Kar, Arpan Kumar and Baabdullah, Abdullah M. and Koohang, Alex and Raghavan, Vishnupriya and Ahuja, Manju and Albanna, Hanaa and Albashrawi, Mousa Ahmad and Al-Busaidi, Adil S. and Balakrishnan, Janarthanan and Barlette, Yves and Basu, Sriparna and Bose, Indranil and Brooks, Laurence and Buhalis, Dimitrios and Carter, Lemuria and Chowdhury, Soumyadeb and Crick, Tom and Cunningham, Scott W. and Davies, Gareth H. and Davison, Robert M. and Dé, Rahul and Dennehy, Denis and Duan, Yanqing and Dubey, Rameshwar and Dwivedi, Rohita and Edwards, John S. and Flavián, Carlos and Gauld, Robin and Grover, Varun and Hu, Mei-Chih and Janssen, Marijn and Jones, Paul and Junglas, Iris and Khorana, Sangeeta and Kraus, Sascha and Larsen, Kai R. and Latreille, Paul and Laumer, Sven and Malik, F. Tegwen and Mardani, Abbas and Mariani, Marcello and Mithas, Sunil and Mogaji, Emmanuel and Nord, Jeretta Horn and O’Connor, Siobhan and Okumus, Fevzi and Pagani, Margherita and Pandey, Neeraj and Papagiannidis, Savvas and Pappas, Ilias O. and Pathak, Nishith and Pries-Heje, Jan and Raman, Ramakrishnan and Rana, Nripendra P. and Rehm, Sven-Volker and Ribeiro-Navarrete, Samuel and Richter, Alexander and Rowe, Frantz and Sarker, Suprateek and Stahl, Bernd Carsten and Tiwari, Manoj Kumar and van der Aalst, Wil and Venkatesh, Viswanath and Viglia, Giampaolo and Wade, Michael and Walton, Paul and Wirtz, Jochen and Wright, Ryan},
	month = aug,
	year = {2023},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {ChatGPT, Conversational agent, Generative AI, Generative artificial intelligence, Large language models},
}

@inproceedings{biagiola_web_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Web test dependency detection},
	isbn = {978-1-4503-5572-8},
	url = {https://doi.org/10.1145/3338906.3338948},
	doi = {10.1145/3338906.3338948},
	abstract = {E2E web test suites are prone to test dependencies due to the heterogeneous multi-tiered nature of modern web apps, which makes it difficult for developers to create isolated program states for each test case. In this paper, we present the first approach for detecting and validating test dependencies present in E2E web test suites. Our approach employs string analysis to extract an approximated set of dependencies from the test code. It then filters potential false dependencies through natural language processing of test names. Finally, it validates all dependencies, and uses a novel recovery algorithm to ensure no true dependencies are missed in the final test dependency graph. Our approach is implemented in a tool called TEDD and evaluated on the test suites of six open-source web apps. Our results show that TEDD can correctly detect and validate test dependencies up to 72\% faster than the baseline with the original test ordering in which the graph contains all possible dependencies. The test dependency graphs produced by TEDD enable test execution parallelization, with a speed-up factor of up to 7×.},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Biagiola, Matteo and Stocco, Andrea and Mesbah, Ali and Ricca, Filippo and Tonella, Paolo},
	year = {2019},
	note = {event-place: Tallinn, Estonia},
	keywords = {NLP, test dependency, web testing},
	pages = {154--164},
}

@inproceedings{katura_nb-iot_2022,
	address = {New York, NY, USA},
	series = {{ICIT} '21},
	title = {{NB}-{IoT} {Based} {Waste} {Monitoring} and {Collection} {Planning} {System}},
	isbn = {978-1-4503-8497-1},
	url = {https://doi.org/10.1145/3512576.3512615},
	doi = {10.1145/3512576.3512615},
	abstract = {Increasing Municipal Solid Waste (MSW) generation has become a major challenge in Namibia particularly in the capital city of Windhoek. Lack of finances and sufficient data in waste management system is a limiting factor to improving the efficiency of MSW management system in Windhoek city. IoT technology provides cost saving solutions that enables municipal authorities and companies to implement solution using less finances. Moreover, IoT enable the municipal and responsible authorities to capture relevant data that provides more insight into the waste management system i.e. in planning an efficient waste collection process. In this paper, the author proposes a waste monitoring and collection planning system based on narrow-band Internet of Things (NB-IoT). NB-IoT communication technology provides competitive advantage in flexibility and coverage while ensuring scalability and reliable service. The proposed solution will design and implement a smart bin prototype and a web-based waste monitoring application, capable of performing real time monitoring of waste fill level and detecting foul smell from the garbage bin. Moreover, the system provides effective decision-making support in waste collection planning. This will allow the municipal City of Windhoek to collect waste materials on time in order to avoid garbage bins from overflowing and keep the city clean.},
	booktitle = {Proceedings of the 2021 9th {International} {Conference} on {Information} {Technology}: {IoT} and {Smart} {City}},
	publisher = {Association for Computing Machinery},
	author = {Katura, Martin and Wu, Liangliang and Fu, Chen and Liu, Wei},
	year = {2022},
	note = {event-place: Guangzhou, China},
	keywords = {Internet of Things, Low Power Wide Area Network, Municipal Solid Waste, Narrowband Internet of Things},
	pages = {213--219},
}

@book{noauthor_ebimcs_2022,
	address = {New York, NY, USA},
	title = {{EBIMCS} '22: {Proceedings} of the 2022 5th {International} {Conference} on {E}-{Business}, {Information} {Management} and {Computer} {Science}},
	isbn = {978-1-4503-9782-7},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icetm_2022,
	address = {New York, NY, USA},
	title = {{ICETM} '22: {Proceedings} of the 2022 5th {International} {Conference} on {Education} {Technology} {Management}},
	isbn = {978-1-4503-9801-5},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_www_2022,
	address = {New York, NY, USA},
	title = {{WWW} '22: {Companion} {Proceedings} of the {Web} {Conference} 2022},
	isbn = {978-1-4503-9130-6},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icieaeu_2023,
	address = {New York, NY, USA},
	title = {{ICIEAEU} '23: {Proceedings} of the 2023 10th {International} {Conference} on {Industrial} {Engineering} and {Applications}},
	isbn = {978-1-4503-9852-7},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@inproceedings{wu_exploring_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {Exploring the relationship between dockerfile quality and project characteristics},
	isbn = {978-1-4503-7122-3},
	url = {https://doi.org/10.1145/3377812.3382169},
	doi = {10.1145/3377812.3382169},
	abstract = {Dockerfile plays an important role in the Docker-based software development process, but many Dockerfile codes are infected with quality issues in practice. Previous empirical studies showed the existence of association between code quality and project characteristics. However, the relationship between Dockerfile quality and project characteristics has never been explored. In this paper, we seek to empirically study this relation through a large dataset of 6,334 projects. Using linear regression analysis, when controlled for various variables, we statistically identify and quantify the relationship between Dockerfile quality and project characteristics.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Yiwen},
	year = {2020},
	note = {event-place: Seoul, South Korea},
	keywords = {dockerfile, dockerfile quality, project characteristics},
	pages = {128--130},
}

@book{noauthor_sle_2016,
	address = {New York, NY, USA},
	title = {{SLE} 2016: {Proceedings} of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	isbn = {978-1-4503-4447-0},
	publisher = {Association for Computing Machinery},
	year = {2016},
}

@article{lomio_just--time_2022,
	title = {Just-in-time software vulnerability detection: {Are} we there yet?},
	volume = {188},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2022.111283},
	doi = {10.1016/j.jss.2022.111283},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Lomio, Francesco and Iannone, Emanuele and De Lucia, Andrea and Palomba, Fabio and Lenarduzzi, Valentina},
	month = jun,
	year = {2022},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Software vulnerabilities, Machine learning, Empirical SE},
}

@book{noauthor_icccm_2022,
	address = {New York, NY, USA},
	title = {{ICCCM} '22: {Proceedings} of the 10th {International} {Conference} on {Computer} and {Communications} {Management}},
	isbn = {978-1-4503-9634-9},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_icmhi_2022,
	address = {New York, NY, USA},
	title = {{ICMHI} '22: {Proceedings} of the 6th {International} {Conference} on {Medical} and {Health} {Informatics}},
	isbn = {978-1-4503-9630-1},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_goodit_2022,
	address = {New York, NY, USA},
	title = {{GoodIT} '22: {Proceedings} of the 2022 {ACM} {Conference} on {Information} {Technology} for {Social} {Good}},
	isbn = {978-1-4503-9284-6},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{lugrin_handbook_2022,
	address = {New York, NY, USA},
	edition = {1},
	title = {The {Handbook} on {Socially} {Interactive} {Agents}: 20 years of {Research} on {Embodied} {Conversational} {Agents}, {Intelligent} {Virtual} {Agents}, and {Social} {Robotics} {Volume} 2: {Interactivity}, {Platforms}, {Application}},
	volume = {48},
	isbn = {978-1-4503-9896-1},
	abstract = {The Handbook on Socially Interactive Agents provides a comprehensive overview of the research fields of Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics. Socially Interactive Agents (SIAs), whether virtually or physically embodied, are autonomous agents that are able to perceive an environment including people or other agents, reason, and decide how to interact, and express attitudes such as emotions, engagement, or empathy. They are capable of interacting with people and each other in a socially intelligent manner using multimodal communicative behaviors with the goal to support humans in various domains.Written by international experts in their respective fields, the book summarizes research in the many important research communities pertinent for SIAs, while discussing current challenges and future directions. The handbook provides easy access to modeling and studying SIAs for researchers and students and aims at further bridging the gap between the research communities involved.In two volumes, the book clearly structures the vast body of research. The first volume starts by introducing what is involved in SIAs research, in particular research methodologies and ethical implications of developing SIAs. It further examines research on appearance and behavior, focusing on multimodality. Finally, social cognition for SIAs is investigated by different theoretical models and phenomena such as theory of mind or pro-sociality. The second volume starts with perspectives on interaction, examined from different angles such as interaction in social space, group interaction, or long-term interaction. It also includes an extensive overview summarizing research and systems of human-agent platforms and of some of the major application areas of SIAs such as education, aging support, autism or games.},
	publisher = {Association for Computing Machinery},
	editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
	year = {2022},
}

@book{noauthor_mum_2022,
	address = {New York, NY, USA},
	title = {{MUM} '22: {Proceedings} of the 21st {International} {Conference} on {Mobile} and {Ubiquitous} {Multimedia}},
	isbn = {978-1-4503-9820-6},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@book{noauthor_sbcars_2023,
	address = {New York, NY, USA},
	title = {{SBCARS} '23: {Proceedings} of the 17th {Brazilian} {Symposium} on {Software} {Components}, {Architectures}, and {Reuse}},
	isbn = {979-8-4007-0952-4},
	publisher = {Association for Computing Machinery},
	year = {2023},
}

@book{whitton_seminal_2023,
	address = {New York, NY, USA},
	edition = {1},
	title = {Seminal {Graphics} {Papers}: {Pushing} the {Boundaries}, {Volume} 2},
	volume = {Volume 2},
	isbn = {979-8-4007-0897-8},
	abstract = {When we began planning how to celebrate 50 years of SIGGRAPH Conferences, there was unanimous agreement that one of the projects should be publishing a second volume of Seminal Graphics Papers. The first volume was published in 1998 as part of the celebration of the 25th SIGGRAPH conference. Seminal Graphics Papers Volume 2, perhaps more than any other activity undertaken in this milestone year, celebrates ACM SIGGRAPH's origins and continued success as a Technical and Professional Society. This collection of papers typifies the ground-breaking research that has been the conference's hallmark since 1974. A quick scan of the chapter and the paper titles shows just how far SIGGRAPH research has pushed the boundaries of our discipline and contributed to its evolution.The ACM Digital Library team has been supportive of this Seminal Graphics Papers project from the beginning. I am pleased to let you know that both Volumes 1 and 2 of Seminal Graphics Papers are freely available from the ACM Digital Library at these URLs: Volume 1: https://dl.acm.org/doi/book/10.1145/280811Volume 2: https://dl.acm.org/doi/book/10.1145/3596711},
	publisher = {Association for Computing Machinery},
	editor = {Whitton, Mary C.},
	year = {2023},
}

@article{alalfi_approach_2018,
	title = {An approach to clone detection in sequence diagrams and its application to security analysis},
	volume = {17},
	issn = {1619-1366},
	url = {https://doi.org/10.1007/s10270-016-0557-6},
	doi = {10.1007/s10270-016-0557-6},
	abstract = {Duplication in software systems is an important issue in software quality assurance. While many methods for software clone detection in source code and structural models have been described in the literature, little has been done on similarity in the dynamic behaviour of interactive systems. In this paper, we present an approach to identifying near-miss interaction clones in reverse-engineered UML sequence diagrams. Our goal is to identify patterns of interaction ("conversations") that can be used to characterize and abstract the run-time behaviour of web applications and other interactive systems. In order to leverage existing robust near-miss code clone technology, our approach is text-based, working on the level of XMI, the standard interchange serialization for UML. Clone detection in UML behavioural models, such as sequence diagrams, presents a number of challenges–first, it is not clear how to break a continuous stream of interaction between lifelines (representing the objects or actors in the system) into meaningful conversational units. Second, unlike programming languages, the XMI text representation for UML is highly non-local, using attributes to reference-related elements in the model file remotely. In this work, we use a set of contextualizing source transformations on the XMI text representation to localize related elements, exposing the hidden hierarchical structure of the model and allowing us to granularize behavioural interactions into conversational units. Then we adapt NICAD, a robust near-miss code clone detection tool, to help us identify conversational clones in reverse-engineered behavioural models. These conversational clones are then analysed to find worrisome interactions that may indicate security access violations.},
	number = {4},
	journal = {Softw. Syst. Model.},
	author = {Alalfi, Manar H. and Antony, Elizabeth P. and Cordy, James R.},
	month = oct,
	year = {2018},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Model based security analysis, Model clone detection},
	pages = {1287--1309},
}

@article{tychola_tactile_2023,
	title = {Tactile {IoT} and {5G} \&amp; beyond schemes as key enabling technologies for the future metaverse},
	volume = {84},
	issn = {1018-4864},
	url = {https://doi.org/10.1007/s11235-023-01052-y},
	doi = {10.1007/s11235-023-01052-y},
	abstract = {The Tactile Internet (TI) is a recently emerging field that has been developing and evolving to date, since its communications parallel the sense of human touch. Lately, the revolutionized concept, Metaverse, draws attention due to the evolved immersive experience of human perception of the surrounding environment. This technology supports the ultimate union between the physical and virtual world, facilitated by 5G and beyond communication networks. Users are capable of interacting with machines and devices in real-time, remotely, resembling the actions of their physical counterparts. The particular approaches are still in their infancy and expected to produce spectacular results in various sectors such as industry, healthcare, autonomous vehicles, etc. This immersion is further assisted by the Internet of Things, while expecting full wireless support by 5G networks. In this article, a systematic review studies the domains of TI, 5G and beyond networks, as well as their relations with the Metaverse, rendering the respective schemes Key Enabling Technologies for the future Metaverse. A thorough analysis is conducted on the underpinning schemes, relative architectures, structures, and operation modes. In addition, a comprehensive list is presented, focusing on related application fields and their benefits, considering the strengths and weaknesses of the involved technologies. Finally, challenges and issues arising are discussed, both from the perspective of technical requirements and the psychosomatic aspect of human experience.},
	number = {3},
	journal = {Telecommun. Syst.},
	author = {Tychola, Kyriaki A. and Voulgaridis, Konstantinos and Lagkas, Thomas},
	month = sep,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {5G and beyond, Artificial intelligence, Augmented/virtual reality, Extended reality, Immersive Internet, Metaverse, Review, Tactile internet, Tactile IoT},
	pages = {363--385},
}

@article{kuosmanen_exploring_2023,
	title = {Exploring crowdsourced self-care techniques: {A} study on {Parkinson}’s disease},
	volume = {177},
	issn = {1071-5819},
	url = {https://doi.org/10.1016/j.ijhcs.2023.103062},
	doi = {10.1016/j.ijhcs.2023.103062},
	number = {C},
	journal = {Int. J. Hum.-Comput. Stud.},
	author = {Kuosmanen, Elina and Huusko, Eetu and van Berkel, Niels and Nunes, Francisco and Vega, Julio and Goncalves, Jorge and Khamis, Mohamed and Esteves, Augusto and Ferreira, Denzil and Hosio, Simo},
	month = sep,
	year = {2023},
	note = {Place: USA
Publisher: Academic Press, Inc.},
	keywords = {Crowdsourcing, Knowledge base, Parkinson’s disease, Self-care},
}

@inproceedings{banerjee_evaluating_2019,
	address = {Berlin, Heidelberg},
	title = {Evaluating the {Choice} of {Tags} in {CQA} {Sites}},
	isbn = {978-3-030-18575-6},
	url = {https://doi.org/10.1007/978-3-030-18576-3_37},
	doi = {10.1007/978-3-030-18576-3_37},
	abstract = {Tags play a crucial role in CQA sites by facilitating organization, indexing and categorization of the entire post in a few words. The choice of tags determines the audience that is elicited upon to seek a response for any particular post. This could either lead to receiving an accurate response for the question or result in receiving no answers. The choice of tags, thus, directly determines the quality of the post as well as to a large extent the success of the CQA site itself. In this paper, we a present a novel approach to evaluate the choice of tags in any post. We perform tag network analysis to find relationship between tags. We then find the anomalous combination of tags by performing anomaly detection. We demonstrate the robustness of our approach by showing high AUC, in the range of 0.95 to 0.98, on four datasets from Stack Exchange, namely Ask Ubuntu, Server Fault, Super User and Software Engineering.},
	booktitle = {Database {Systems} for {Advanced} {Applications}: 24th {International} {Conference}, {DASFAA} 2019, {Chiang} {Mai}, {Thailand}, {April} 22–25, 2019, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	author = {Banerjee, Rohan and Rajanala, Sailaja and Singh, Manish},
	year = {2019},
	note = {event-place: Chiang Mai, Thailand},
	keywords = {Anomaly detection, CQA sites, Tag network},
	pages = {625--640},
}

@inproceedings{rodrigues_refactorings_2018,
	address = {New York, NY, USA},
	series = {{SBLP} '18},
	title = {Refactorings for replacing dynamic instructions with static ones: the case of ruby},
	isbn = {978-1-4503-6480-5},
	url = {https://doi.org/10.1145/3264637.3264645},
	doi = {10.1145/3264637.3264645},
	abstract = {Dynamic features offered by programming languages provide greater flexibility to the programmer (e.g., dynamic constructions of classes and methods) and reduction of duplicate code snippets. However, the unnecessary use of dynamic features may detract from the code in many ways, such as readability, comprehension, and maintainability of software. Therefore, this paper proposes 20 refactorings that replace dynamic instructions with static ones. In an evaluation on 28 open-source Ruby systems, we could refactor 743 of 1,651 dynamic statements (45\%).},
	booktitle = {Proceedings of the {XXII} {Brazilian} {Symposium} on {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Rodrigues, Elder and Durelli, Rafael Serapilha and de Bettio, Raphael Winckler and Montecchi, Leonardo and Terra, Ricardo},
	year = {2018},
	note = {event-place: Sao Carlos, Brazil},
	keywords = {refactoring, dynamic languages, dynamic statements, frameworks},
	pages = {59--66},
}

@article{ge_survey_2023,
	title = {A survey on computational metaphor processing techniques: from identification, interpretation, generation to application},
	volume = {56},
	issn = {0269-2821},
	url = {https://doi.org/10.1007/s10462-023-10564-7},
	doi = {10.1007/s10462-023-10564-7},
	abstract = {Metaphors are figurative expressions frequently appearing daily. Given its significance in downstream natural language processing tasks such as machine translation and sentiment analysis, computational metaphor processing has led to an upsurge in the community. The progress of Artificial Intelligence has incentivized several technological tools and frameworks in this domain. This article aims to comprehensively summarize and categorize previous computational metaphor processing approaches regarding metaphor identification, interpretation, generation, and application. Though studies on metaphor identification have made significant progress, metaphor understanding, conceptual metaphor processing, and metaphor generation still need in-depth analysis. We hope to identify future directions for prospective researchers based on comparing the strengths and weaknesses of the previous works.},
	number = {Suppl 2},
	journal = {Artif. Intell. Rev.},
	author = {Ge, Mengshi and Mao, Rui and Cambria, Erik},
	month = aug,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Conceptual metaphor processing, Metaphor generation, Metaphor identification, Metaphor interpretation, Metaphor processing application},
	pages = {1829--1895},
}

@article{pereira_greenhub_2021,
	title = {{GreenHub}: a large-scale collaborative dataset to battery consumption analysis of android devices},
	volume = {26},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-020-09925-5},
	doi = {10.1007/s10664-020-09925-5},
	number = {3},
	journal = {Empirical Softw. Engg.},
	author = {Pereira, Rui and Matalonga, Hugo and Couto, Marco and Castor, Fernando and Cabral, Bruno and Carvalho, Pedro and de Sousa, Simão Melo and Fernandes, João Paulo},
	month = may,
	year = {2021},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Android, Green software, Battery consumption analysis, Green mining},
}

@book{noauthor_iconetsi_2022,
	address = {New York, NY, USA},
	title = {{ICONETSI} '22: {Proceedings} of the 2022 {International} {Conference} on {Engineering} and {Information} {Technology} for {Sustainable} {Industry}},
	isbn = {978-1-4503-9718-6},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{khot_human-food_2019,
	title = {Human-{Food} {Interaction}},
	volume = {12},
	issn = {1551-3955},
	url = {https://doi.org/10.1561/1100000074},
	doi = {10.1561/1100000074},
	abstract = {Food is not only fundamental to our existence, its consumption, handling or even the mere sight of its also brings us immense joy. Over the years, technology has played a crucial part in supporting and enriching food-related practices, beginning from how we grow, to how we cook, eat and dispose of food. All these practices have a significant impact not only on individuals but also on the surrounding ecologies and infrastructures, often discussed under the umbrella term of Human-Food Interaction (HFI). This article aims to offer the reader an overview of the existing research in this space and to guide further its exploration. We illustrate how HFI builds on recent trends within HCI. We position this growth across four phases of HFI, namely, Growing, Cooking, Eating and Disposal. We categorize and disseminate the existing works across each of these phases to reveal a rich design space and to highlight the underexplored areas that interaction designers might find intriguing to investigate. Using the design space, we articulate a set of opportunities that emphasize particular features the technology, especially hardware, has yet to offer to drive the human-food interaction field forward. We highlight the design space for designing novel interactions with technologies by taking motivation from traditional food practices related to cooking and eating food. Finally, we introduce Human Food Practices (HFP) an emerging field of investigation that concerns itself with the formation and transformation of practices as they are enacted within the dynamics, motivations and perceptions of societal norms associated with food.},
	number = {4},
	journal = {Found. Trends Hum.-Comput. Interact.},
	author = {Khot, Rohit Ashok and Mueller, Florian},
	month = aug,
	year = {2019},
	note = {Place: Hanover, MA, USA
Publisher: Now Publishers Inc.},
	pages = {238--415},
}

@article{martnez_model-based_2017,
	title = {Model-based analysis of {Java} {EE} web security misconfigurations},
	volume = {49},
	issn = {1477-8424},
	url = {https://doi.org/10.1016/j.cl.2017.02.001},
	doi = {10.1016/j.cl.2017.02.001},
	abstract = {The Java EE framework, a popular technology of choice for the development of web applications, provides developers with the means to define access-control policies to protect application resources from unauthorized disclosures and manipulations. Unfortunately, the definition and manipulation of such security policies remains a complex and error prone task, requiring expert-level knowledge on the syntax and semantics of the Java EE access-control mechanisms. Thus, misconfigurations that may lead to unintentional security and/or availability problems can be easily introduced. In response to this problem, we present a (model-based) reverse engineering approach that automatically evaluates a set of security properties on reverse engineered Java EE security configurations, helping to detect the presence of anomalies. We evaluate the efficacy and pertinence of our approach by applying our prototype tool on a sample of real Java EE applications extracted from GitHub. HighlightsWe provide a framework to analyze Java EE access-control misconfigurations.We use model-driven engineering tools and techniques to our analysis.We evaluate the efficacy and pertinence of our approach on real applications.We provide a survey on the importance of security to Java EE developers.},
	number = {C},
	journal = {Comput. Lang. Syst. Struct.},
	author = {Martnez, Salvador and Cosentino, Valerio and Cabot, Jordi},
	month = sep,
	year = {2017},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Security, Model-driven engineering, Reverse-engineering},
	pages = {36--61},
}

@inproceedings{zaytsev_language_2017,
	series = {{MODELS} '17},
	title = {Language design with intent},
	isbn = {978-1-5386-3492-9},
	url = {https://doi.org/10.1109/MODELS.2017.16},
	doi = {10.1109/MODELS.2017.16},
	abstract = {Software languages have always been an essential component of model-driven engineering. Their importance and popularity has been on the rise thanks to language workbenches, language-oriented development and other methodologies that enable us to quickly and easily create new languages specific for each domain. Unfortunately, language design is largely a form of art and has resisted most attempts to turn it into a form of science or engineering. In this paper we borrow concepts, techniques and principles from the domain of persuasive technology, or wider yet, design with intent — which was developed as a way to influence users behaviour for social and environmental benefit. Similarly, we claim, software language designers can make conscious choices in order to influence the behaviour of language users. The paper describes a process of extracting design components from 24 books of eight categories (dragon books, parsing techniques, compiler construction, compiler design, language implementation, language documentation, programming languages, software languages), as well as from the original set of Design with Intent cards and papers on DSL design. The resulting language design card toolkit can be used by DSL designers to cover important design decisions and make them with more confidence.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 20th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {IEEE Press},
	author = {Zaytsev, Vadim},
	year = {2017},
	note = {Place: Austin, Texas},
	pages = {45--52},
}

@article{sutton_spiking_2021,
	title = {Spiking neural networks and hippocampal function: {A} web-accessible survey of simulations, modeling methods, and underlying theories},
	volume = {70},
	issn = {1389-0417},
	url = {https://doi.org/10.1016/j.cogsys.2021.07.008},
	doi = {10.1016/j.cogsys.2021.07.008},
	number = {C},
	journal = {Cogn. Syst. Res.},
	author = {Sutton, Nate M. and Ascoli, Giorgio A.},
	month = dec,
	year = {2021},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Knowledge base, Computational, Hippocampus, Modeling, Spiking neural network},
	pages = {80--92},
}

@article{bock_automatic_2023,
	title = {Automatic {Core}-{Developer} {Identification} on {GitHub}: {A} {Validation} {Study}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3593803},
	doi = {10.1145/3593803},
	abstract = {Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods.},
	number = {6},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Bock, Thomas and Alznauer, Nils and Joblin, Mitchell and Apel, Sven},
	month = sep,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {developer classification, developer networks, Open-source software projects},
}

@book{ifpo_security_2015,
	address = {USA},
	edition = {4},
	title = {Security {Supervision} and {Management}: {Theory} and {Practice} of {Asset} {Protection}},
	isbn = {978-0-12-800489-0},
	abstract = {Security Supervision and Management, Fourth Edition, fills the basic training needs for security professionals who want to move into supervisory or managerial positions. Covering everything needed from how to work with todays generation security force employees to the latest advances in the security industry, Security Supervision and Management, Fourth Edition, shows security officers how to become a more efficient and well-rounded security professional. Security Supervision and Management, Fourth Edition, is also the only text needed to prepare for the Certified in Security Supervision and Management (CSSM) designation offered by International Foundation for Protection Officers (IFPO). The IFPO also publishes The Professional Protection Officer: Practical Security Strategies and Emerging Trends, now in its 8th edition. Core text for completing the Security Supervision and Management ProgramCertified in Security Supervision and Management (CSSM) designation offered by IFPO Contributions from more than 50 experienced security professionals in a single volume Completely updated to reflect the latest procedural and technological changes in the security industry Conforms to ANSIASIS standards Table of Contents Dedication Foreword Acknowledgments Introduction Protection Officer Code of Ethics UNIT I: FOUNDATIONS 1. What is Asset Protection 2. Legal Aspects \&amp; Liability Issues 3. Crime and Deviance 4. Ethics, Integrity, and Professional Conduct 5. Risk Management 6. Accident Causation 7. Understanding the Supply Chain UNIT II: BASICS OF SUPERVISION 8. Basics of Supervision 9. Supervisory Characteristics \&amp; Expectations 10. Corporate Policy \&amp; Procedures 11. Operational Supervision 12. Supervisors Role in Safety 13. Supervisors Role in Customer Service 14. Deploying Personnel 15. Planning and Supervising Special Events UNIT III: TRAINING \&amp; DEVELOPMENT 16. Training and Development UNIT IV: HUMAN RESOURCE MANAGEMENT Part I - Human Capital 17. Recruitment, Retention and Selection of Security Personnel 18. Career Planning \&amp; Preparation 19. Orientation for Security Officers 20. Time and Stress Management 21. Employee Motivation Theory and Application 22. Supervising Across Different Cultures and Generations 23. Supervisors Role in Employee Relations 24. Personnel and Monitoring Employee Performance Part II - Security - Related Business Functions 25. Basic Statistical Analysis 26. Relationship Between Marketing \&amp; the Security Function 27. Developing and Managing a Budget 28. Scheduling 29. Total Quality Management 30. Project Management 31. Standards \&amp; Guidelines UNIT V: TECHNOLOGY IN SECURITY 32. Technology in Security 33. Access Control Systems 34. Closed CircuitDigital Video Systems 35. Fire Protection Systems 36. Other Technologies (K-9, X-Ray, Radiation Detection, etc.) 37. Identification and Documents UNIT VI: EMERGENCY MANAGEMENT 38. Incident Command System 39. Incident Management (Critical Incident Management in the Post 911 Era) UNIT VII: INVESTIGATIONS 40. Managing Investigations 41. Conducting Interviews (Rapport-Based vs Adversarial) 42. Conducting a Search (Conveyances, Buildings, etc) 43. Crime Scene and Evidence Preservation 44. The Importance of Report Writing to the Security Operation 45. Detention \&amp; Apprehension Procedures UNIT VIII: INFORMATION ASSURANCE 46. Industrial Espionage - A Primer 47. The Relationship Between Security and Corporate IT 48. Cyber Security UNIT IX: CURRENT ISSUES IN SECURITY 49. Workplace Violence - Active Shooter 50. International Scene 51. Privacy},
	publisher = {Butterworth-Heinemann},
	author = {{IFPO} and Davies, Sandi J},
	year = {2015},
}

@book{noauthor_splash_2022,
	address = {New York, NY, USA},
	title = {{SPLASH} {Companion} 2022: {Companion} {Proceedings} of the 2022 {ACM} {SIGPLAN} {International} {Conference} on {Systems}, {Programming}, {Languages}, and {Applications}: {Software} for {Humanity}},
	isbn = {978-1-4503-9901-2},
	abstract = {Welcome to the SPLASH 2022! After two years of virtual only (SPLASH 2020), closed borders USA only (SPLASH 2021), we finally feel the reopening and going back to the pre-Covid in person vibe of the 37th OOPSLA/SPLASH. I am especially proud of having SPLASH outside of the USA/Canada region for the 3rd time in its history and the first time it is held in the Asia Pacific. We invited the Asian Symposium on Programming Languages and Systems (APLAS) to co-locate with us for the 3rd year in a row to celebrate this occasion.},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@inproceedings{spadini_mock_2017,
	series = {{MSR} '17},
	title = {To mock or not to mock? an empirical study on mocking practices},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.61},
	doi = {10.1109/MSR.2017.61},
	abstract = {When writing automated unit tests, developers often deal with software artifacts that have several dependencies. In these cases, one has the possibility of either instantiating the dependencies or using mock objects to simulate the dependencies' expected behavior. Even though recent quantitative studies showed that mock objects are widely used in OSS projects, scientific knowledge is still lacking on how and why practitioners use mocks. Such a knowledge is fundamental to guide further research on this widespread practice and inform the design of tools and processes to improve it.The objective of this paper is to increase our understanding of which test dependencies developers (do not) mock and why, as well as what challenges developers face with this practice. To this aim, we create MockExtractor, a tool to mine the usage of mock objects in testing code and employ it to collect data from three OSS projects and one industrial system. Sampling from this data, we manually analyze how more than 2,000 test dependencies are treated. Subsequently, we discuss our findings with developers from these systems, identifying practices, rationales, and challenges. These results are supported by a structured survey with more than 100 professionals. The study reveals that the usage of mocks is highly dependent on the responsibility and the architectural concern of the class. Developers report to frequently mock dependencies that make testing difficult and prefer to not mock classes that encapsulate domain concepts/rules of the system. Among the key challenges, developers report that maintaining the behavior of the mock compatible with the behavior of original class is hard and that mocking increases the coupling between the test and the production code.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Spadini, Davide and Aniche, Maurício and Bruntink, Magiel and Bacchelli, Alberto},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	pages = {402--412},
}

@inproceedings{mesbah_automated_2012,
	series = {{ICSE} '12},
	title = {Automated analysis of {CSS} rules to support style maintenance},
	isbn = {978-1-4673-1067-3},
	abstract = {CSS is a widely used language for describing the presentation semantics of HTML elements on the web. The language has a number of characteristics, such as inheritance and cascading order, which makes maintaining CSS code a challenging task for web developers. As a result, it is common for unused rules to be accumulated over time. Despite these challenges, CSS analysis has not received much attention from the research community. We propose an automated technique to support styling code maintenance, which (1) analyzes the runtime relationship between the CSS rules and DOM elements of a given web application (2) detects unmatched and ineffective selectors, overridden declaration properties, and undefined class values. Our technique, implemented in an open source tool called CILLA, has a high precision and recall rate. The results of our case study, conducted on fifteen open source and industrial web-based systems, show an average of 60\% unused CSS selectors in deployed applications, which points to the ubiquity of the problem.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Mesbah, Ali and Mirshokraie, Shabnam},
	year = {2012},
	note = {Place: Zurich, Switzerland},
	pages = {408--418},
}

@article{zhong_burner_2022,
	title = {Burner: {Recipe} {Automatic} {Generation} for {HPC} {Container} {Based} on {Domain} {Knowledge} {Graph}},
	volume = {2022},
	issn = {1530-8669},
	url = {https://doi.org/10.1155/2022/4592428},
	doi = {10.1155/2022/4592428},
	abstract = {As one of the emerging cloud computing technologies, containers are widely used in academia and industry. The cloud computing built by the container in the high performance computing (HPC) center can provide high-quality services to users at the edge. Singularity Definition File and Dockerfile (we refer to such files as recipes) have attracted wide attention due to their encapsulation of the application running environment in a container. However, creating a recipe requires extensive domain knowledge, which is error-prone and time-consuming. Accordingly, more than 34\% of Dockerfiles in Github cannot successfully build container images. The crucial points about recipe creation include selecting the entities (base images and packages) and determining their relationships (correct installation order for transitive dependencies). Since the relationships between entities can be expressed accurately and efficiently by the knowledge graph, we introduce knowledge graph to generate high-quality recipes automatically. This paper proposes an automatic recipe generation system named Burner, enabling users with no professional computer background to generate the recipes. We first develop a toolset including a recipe parser and an entity-relationship miner. Our two-phase recipe parsing method can perform abstract syntax tree (AST) parsing more deeply on the recipe file to achieve entity extraction; the parsing success rate (PSR) of the two-phase parsing method is 10.1\% higher than the one-phase parsing. Then, we build a knowledge base containing 2,832 entities and 62,614 entity relationships, meeting the needs of typical HPC applications. In the test of image build, the singularity image build success rate reaches 80\%. Compared with the ItemCF recommendation method, our recommendation method TB-TFIDF achieves a performance improvement by up to 50.86\%.},
	journal = {Wirel. Commun. Mob. Comput.},
	author = {Zhong, Shuaihao and Wang, Duoqiang and Li, Wei and Lu, Feng and Jin, Hai and Wang, Yingjie},
	month = jan,
	year = {2022},
	note = {Place: GBR
Publisher: John Wiley and Sons Ltd.},
}

@inproceedings{shrikanth_early_2021,
	series = {{ICSE} '21},
	title = {Early {Life} {Cycle} {Software} {Defect} {Prediction}: {Why}? {How}?},
	isbn = {978-1-4503-9085-9},
	url = {https://doi.org/10.1109/ICSE43902.2021.00050},
	doi = {10.1109/ICSE43902.2021.00050},
	abstract = {Many researchers assume that, for software analytics, "more data is better." We write to show that, at least for learning defect predictors, this may not be true.To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models.We hope these results inspire other researchers to adopt a "simplicity-first" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for "short cuts" that can simplify the analysis.},
	booktitle = {Proceedings of the 43rd {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Shrikanth, N. C. and Majumder, Suvodeep and Menzies, Tim},
	year = {2021},
	note = {Place: Madrid, Spain},
	keywords = {early, analytics, defect prediction, sampling},
	pages = {448--459},
}

@book{noauthor_cscw_2017,
	address = {New York, NY, USA},
	title = {{CSCW} '17: {Proceedings} of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	isbn = {978-1-4503-4335-0},
	abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI \&amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
	publisher = {Association for Computing Machinery},
	year = {2017},
}

@book{noauthor_ic3ina_2022,
	address = {New York, NY, USA},
	title = {{IC3INA} '22: {Proceedings} of the 2022 {International} {Conference} on {Computer}, {Control}, {Informatics} and {Its} {Applications}},
	isbn = {978-1-4503-9790-2},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{van_ditmarsch_be_2023,
	title = {To be announced},
	volume = {292},
	issn = {0890-5401},
	url = {https://doi.org/10.1016/j.ic.2023.105026},
	doi = {10.1016/j.ic.2023.105026},
	number = {C},
	journal = {Inf. Comput.},
	author = {van Ditmarsch, Hans},
	month = jun,
	year = {2023},
	note = {Place: USA
Publisher: Academic Press, Inc.},
}

@book{moffett_bridging_2014,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {Bridging {UX} and {Web} {Development}: {Better} {Results} through {Team} {Integration}},
	isbn = {0-12-420245-4},
	abstract = {The divide between UX and Web development can be stifling. Bridging UX and Web Development prepares you to break down those walls by teaching you how to integrate with your team's developers. You examine the process from their perspective, discovering tools and coding principles that will help you bridge the gap between design and implementation. With these tried and true approaches, you'll be able to capitalize on a more productive work environment. Whether you're a novice UX professional finding your place in the software industry and looking to nail down your technical skills, or a seasoned UI designer looking for practical information on how to integrate your team with development, this is the must-have resource for your UX library. Establish a collaboration lifecycle, mapping design activities to counterparts in the software development process Learn about software tools that will improve productivity and collaboration Work through step-by-step exercises that teach font-end coding principles to improve your prototyping and implementation activities Discover practical, usable HTML and CSS examples Uncover tips for working with various developer personas},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Moffett, Jack},
	year = {2014},
}

@inproceedings{martinez_model-based_2016,
	address = {New York, NY, USA},
	series = {{MiSE} '16},
	title = {Model-based analysis of {Java} {EE} web security configurations},
	isbn = {978-1-4503-4164-6},
	url = {https://doi.org/10.1145/2896982.2896986},
	doi = {10.1145/2896982.2896986},
	abstract = {The widespread use of Java EE web applications as a means to provide distributed services to remote clients imposes strong security requirements, so that the resources managed by these applications remain protected from unauthorized disclosures and manipulations. For this purpose, the Java EE framework provides developers with mechanisms to define access-control policies. Unfortunately, the variety and complexity of the provided security configuration mechanisms cause the definition and manipulation of a security policy to be complex and error prone. As security requirements are not static, and thus, implemented policies must be changed and reviewed often, discovering and representing the policy at an appropriate abstraction level to enable their understanding and reenginering appears as a critical requirement. To tackle this problem, this paper presents a (model-based) approach aimed to help security experts to visualize, (automatically) analyse and manipulate web security policies.},
	booktitle = {Proceedings of the 8th {International} {Workshop} on {Modeling} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Martínez, Salvador and Cosentino, Valerio and Cabot, Jordi},
	year = {2016},
	note = {event-place: Austin, Texas},
	keywords = {security, access-control, reverse-engineering},
	pages = {55--61},
}

@book{noauthor_europlop_2022,
	address = {New York, NY, USA},
	title = {{EuroPLop} '22: {Proceedings} of the 27th {European} {Conference} on {Pattern} {Languages} of {Programs}},
	isbn = {978-1-4503-9594-6},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@inproceedings{friedman_stalltalk_2013,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '13},
	title = {{StallTalk}: graffiti, toilets, and anonymous location based micro blogging},
	isbn = {978-1-4503-1952-2},
	url = {https://doi.org/10.1145/2468356.2468738},
	doi = {10.1145/2468356.2468738},
	abstract = {The ways in which we leave graffiti have not changed much in thousands of years. Humans have felt the need to anonymously leave messages to one another for centuries. In this paper, we introduce StallTalk (www.stalltalk.info), an anonymous location-based micro blogging website that uses QR codes posted in bathroom stalls. StallTalk allows users to leave digital graffiti on bathroom walls without actually causing permanent damage. Users scan the QR codes, which are unique to each stall, and write short messages to each other. We deployed StallTalk in over 500 locations and have had almost 9,000 unique visitors to our website.},
	booktitle = {{CHI} '13 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Friedman, Jonathan and Horn, Michael S.},
	year = {2013},
	note = {event-place: Paris, France},
	keywords = {bathrooms, graffiti, location-based micro blog, qr codes, toilets},
	pages = {2179--2188},
}

@inproceedings{yoo_secure_2021,
	address = {New York, NY, USA},
	series = {{SPIN} '21},
	title = {Secure {Keyed} {Hashing} on {Programmable} {Switches}},
	isbn = {978-1-4503-8637-1},
	url = {https://doi.org/10.1145/3472873.3472881},
	doi = {10.1145/3472873.3472881},
	abstract = {Cyclic Redundancy Check (CRC) is a computationally inexpensive function readily available in many high-speed networking devices, and thus it is used extensively as a hash function in many data-plane applications. However, CRC is not a true cryptographic hash function, and it leaves applications vulnerable to attack. While cryptographically secure hash functions exist, there is no fast and efficient implementation for such functions on high-speed programmable switches. In this paper, we introduce an implementation of a secure keyed hash function optimized for commodity programmable switches and capable of running entirely within the data plane. We implement HalfSipHash on the Barefoot Tofino switch by using dependency management schemes to conserve pipeline stages and slicing semantics for concise circular bit shift operations. We show that our efficient implementation performs 67 million, 90 million, 150 million, and 304 million hashes per second for 32-byte, 24-byte, 16-byte, and 8-byte input strings, respectively.},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} 2021 {Workshop} on {Secure} {Programmable} {Network} {INfrastructure}},
	publisher = {Association for Computing Machinery},
	author = {Yoo, Sophia and Chen, Xiaoqi},
	year = {2021},
	note = {event-place: Virtual Event, USA},
	keywords = {CRC32, Data Plane, Hash Function, P4, SipHash},
	pages = {16--22},
}

@book{noauthor_asonam_2022,
	address = {Istanbul, Turkey},
	title = {{ASONAM} '22: {Proceedings} of the 2022 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	isbn = {978-1-6654-5661-6},
	abstract = {We were delighted to welcome each participant at ASONAM 2022 and thank you for having contributed virtually or in person in Istanbul. ASONAM 2022 was the fourteenth annual conference in the successful ASONAM conferences series and also the first hybrid version of the conference. Previous ASONAM conferences were held in Athens (2009), Odense (2010), Kaohsiung (2011), Istanbul (2012), Niagara Falls (2013), Beijing (2014), Paris (2015), San Francisco (2016), Sydney (2017), Barcelona (2018), Vancouver (2019), Virtual (2020), Virtual (2021). The pre-pandemic locations of the conferences have enabled the participants to enjoy local sights and to engage in person-to-person interactions, making new contacts and form new scientific collaborations. These possibilities were only available in a limited form during the virtual conferences. As the covid pandemic seems to be moving towards an endemic form it was decided to have the conference in the hybrid form, as a move towards normal endemic in-person conferences.For more than a century, social networks have been studied in a variety of disciplines including sociology, anthropology, psychology, and economics. The Internet, the social Web, and other large-scale, sociotechnological infrastructures have triggered a growing interest and resulted in significant methodological advancements in social network analysis and mining. Method development in graph theory, statistics, data mining, machine learning, and AI have inspired new research problems and, in turn, opens up further possibilities for application. These spiraling trends have led to a rising prominence of social network analysis and mining methods and tools in academia, politics, security, and business.},
	publisher = {IEEE Press},
	year = {2022},
}

@article{acreoaie_vmtl_2018,
	title = {{VMTL}: a language for end-user model transformation},
	volume = {17},
	issn = {1619-1366},
	url = {https://doi.org/10.1007/s10270-016-0546-9},
	doi = {10.1007/s10270-016-0546-9},
	abstract = {Model transformation is a key enabling technology of Model-Driven Engineering (MDE). Existing model transformation languages are shaped by and for MDE practitioners–a user group with needs and capabilities which are not necessarily characteristic of modelers in general. Consequently, these languages are largely ill-equipped for adoption by end-user modelers in areas such as requirements engineering, business process management, or enterprise architecture. We aim to introduce a model transformation language addressing the skills and requirements of end-user modelers. With this contribution, we hope to broaden the application scope of model transformation and MDE technology in general. We discuss the profile of end-user modelers and propose a set of design guidelines for model transformation languages addressing them. We then introduce Visual Model Transformation Language (VMTL) following these guidelines. VMTL draws on our previous work on the usability-oriented Visual Model Query Language. We implement VMTL using the Henshin model transformation engine, and empirically investigate its learnability via two user experiments and a think-aloud protocol analysis. Our experiments, although conducted on computer science students exhibiting only some of the characteristics of end-user modelers, show that VMTL compares favorably in terms of learnability with two state-of the-art model transformation languages: Epsilon and Henshin. Our think-aloud protocol analysis confirms many of the design decisions adopted for VMTL, while also indicating possible improvements.},
	number = {4},
	journal = {Softw. Syst. Model.},
	author = {Acre?Oaie, Vlad and Störrle, Harald and Strüber, Daniel},
	month = oct,
	year = {2018},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {End-user modelers, Epsilon, Experiment, Henshin, Learnability, Think-aloud protocol, Transparent model transformation, VMTL},
	pages = {1139--1167},
}

@book{noauthor_ciis_2022,
	address = {New York, NY, USA},
	title = {{CIIS} '22: {Proceedings} of the 2022 5th {International} {Conference} on {Computational} {Intelligence} and {Intelligent} {Systems}},
	isbn = {978-1-4503-9761-2},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{porter_discoverframework_2021,
	title = {The {DiscoverFramework} freeware toolkit for multivariate spatio-temporal environmental data visualization and evaluation},
	volume = {143},
	issn = {1364-8152},
	url = {https://doi.org/10.1016/j.envsoft.2021.105104},
	doi = {10.1016/j.envsoft.2021.105104},
	number = {C},
	journal = {Environ. Model. Softw.},
	author = {Porter, Misty E. and Hill, Mary C. and Harris, Ted and Brookfield, Andrea and Li, Xingong},
	month = sep,
	year = {2021},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Data visualization, Harmful algal blooms (HABs), Knowledge discovery, Water resources, Web application},
}

@inproceedings{fortuna_towards_2012,
	address = {New York, NY, USA},
	series = {{WOT} '12},
	title = {Towards building a global oracle: a physical mashup using artificial intelligence technology},
	isbn = {978-1-4503-1603-3},
	url = {https://doi.org/10.1145/2379756.2379763},
	doi = {10.1145/2379756.2379763},
	abstract = {In this paper, we describe Videk - a physical mashup which uses artificial intelligence technology. We make an analogy between human senses and sensors; and between human brain and artificial intelligence technology respectively. This analogy leads to the concept of Global Oracle. We introduce a mashup system which automatically collects data from sensors. The data is processed and stored by SenseStream while the meta-data is fed into ResearchCyc. SenseStream indexes aggregates, performs clustering and learns rules which then it exports as RuleML. ResearchCyc performs logical inference on the meta-data and transliterates logical sentences. The GUI mashes up sensor data with SenseStream output, ResearchCyc output and other external data sources: GoogleMaps, Geonames, Wikipedia and Panoramio.},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on the {Web} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Fortuna, Carolina and Vucnik, Matevz and Fortuna, Blaz and Kenda, Klemen and Moraru, Alexandra and Mladenic, Dunja},
	year = {2012},
	note = {event-place: Newcastle, United Kingdom},
	keywords = {machine learning, artificial intelligence, data mining, mashup, sensors, things, web services},
}

@article{hara_improving_2015,
	title = {Improving {Public} {Transit} {Accessibility} for {Blind} {Riders} by {Crowdsourcing} {Bus} {Stop} {Landmark} {Locations} with {Google} {Street} {View}: {An} {Extended} {Analysis}},
	volume = {6},
	issn = {1936-7228},
	url = {https://doi.org/10.1145/2717513},
	doi = {10.1145/2717513},
	abstract = {Low-vision and blind bus riders often rely on known physical landmarks to help locate and verify bus stop locations (e.g., by searching for an expected shelter, bench, or newspaper bin). However, there are currently few, if any, methods to determine this information a priori via computational tools or services. In this article, we introduce and evaluate a new scalable method for collecting bus stop location and landmark descriptions by combining online crowdsourcing and Google Street View (GSV). We conduct and report on three studies: (i) a formative interview study of 18 people with visual impairments to inform the design of our crowdsourcing tool, (ii) a comparative study examining differences between physical bus stop audit data and audits conducted virtually with GSV, and (iii) an online study of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of crowdsourcing bus stop audits using our custom tool with GSV. Our findings reemphasize the importance of landmarks in nonvisual navigation, demonstrate that GSV is a viable bus stop audit dataset, and show that minimally trained crowd workers can find and identify bus stop landmarks with 82.5\% accuracy across 150 bus stop locations (87.3\% with simple quality control).},
	number = {2},
	journal = {ACM Trans. Access. Comput.},
	author = {Hara, Kotaro and Azenkot, Shiri and Campbell, Megan and Bennett, Cynthia L. and Le, Vicki and Pannella, Sean and Moore, Robert and Minckler, Kelly and Ng, Rochelle H. and Froehlich, Jon E.},
	month = mar,
	year = {2015},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {accessible bus stops, bus stop auditing, Crowdsourcing accessibility, Google Street View, low-vision and blind users, Mechanical Turk, remote data collection},
}

@book{longo_edmund_2015,
	title = {Edmund {Berkeley} and the {Social} {Responsibility} of {Computer} {Professionals}},
	volume = {6},
	isbn = {978-1-970001-39-6},
	abstract = {Edmund C. Berkeley (1909–1988) was a mathematician, insurance actuary, inventor, publisher, and a founder of the Association for Computing Machinery (ACM). His book “Giant Brains or Machines That Think” (1949) was the first explanation of computers for a general readership. His journal "Computers and Automation" (1951–1973) was the first journal for computer professionals. In the 1950s, Berkeley developed mail-order kits for small, personal computers such as Simple Simon and the Brainiac. In an era when computer development was on a scale barely affordable by universities or government agencies, Berkeley took a different approach and sold simple computer kits to average Americans. He believed that digital computers, using mechanized reasoning based on symbolic logic, could help people make more rational decisions. The result of this improved reasoning would be better social conditions and fewer large-scale wars. Although Berkeley's populist notions of computer development in the public interest did not prevail, the events of his life exemplify the human side of ongoing debates concerning the social responsibility of computer professionals. This biography of Edmund Berkeley, based on primary sources gathered over 15 years of archival research, provides a lens to understand social and political decisions surrounding early computer development, and the consequences of these decisions in our 21st century lives.},
	publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
	author = {Longo, Bernadette},
	year = {2015},
}

@book{vacca_computer_2013,
	address = {San Francisco, CA, USA},
	edition = {2nd},
	title = {Computer and {Information} {Security} {Handbook}, {Second} {Edition}},
	isbn = {0-12-394397-3},
	abstract = {Thesecond editionof this comprehensive handbook of computer and information securityprovides the most complete view of computer security and privacy available. It offers in-depth coverage of security theory, technology, and practice as they relate to established technologies as well as recent advances. It explores practical solutions to many security issues. Individual chapters are authored by leading experts in the field and address the immediate and long-term challenges in the authors' respective areas of expertise. The book is organized into10 parts comprised of70 contributed chapters by leading experts in the areas of networking and systems security, information management, cyber warfare and security, encryption technology, privacy, data storage, physical security, and a host of advanced security topics. New to this edition are chapters on intrusion detection, securing the cloud, securing web apps, ethical hacking, cyber forensics, physical security, disaster recovery, cyber attack deterrence, and more. Chapters by leaders in the field on theory and practice of computer and information security technology, allowing the reader to develop a new level of technical expertise Comprehensive and up-to-date coverage of security issues allows the reader to remain current and fully informed from multiple viewpoints Presents methods of analysis and problem-solving techniques, enhancing the reader's grasp of the material and ability to implement practical solutions},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Vacca, John R. and Vacca, John R.},
	year = {2013},
}

@article{ortiz_de_gortari_auditory_2014,
	title = {Auditory {Experiences} in {Game} {Transfer} {Phenomena}: {An} {Empirical} {Self}-{Report} {Study}},
	volume = {4},
	issn = {2155-7136},
	url = {https://doi.org/10.4018/ijcbpl.2014010105},
	doi = {10.4018/ijcbpl.2014010105},
	abstract = {This study investigated gamers' auditory experiences as after effects of playing. This was done by classifying, quantifying, and analysing 192 experiences from 155 gamers collected from online videogame forums. The gamers' experiences were classified as: (i) involuntary auditory imagery (e.g., hearing the music, sounds or voices from the game), (ii) inner speech (e.g., completing phrases in the mind), (iii) auditory misperceptions (e.g., confusing real life sounds with videogame sounds), and (iv) multisensorial auditory experiences (e.g., hearing music while involuntary moving the fingers). Gamers heard auditory cues from the game in their heads, in their ears, but also coming from external sources. Occasionally, the vividness of the sound evoked thoughts and emotions that resulted in behaviours and copying strategies. The psychosocial implications of the gamers' auditory experiences are discussed. This study contributes to the understanding of the effects of auditory features in videogames, and to the phenomenology of non-volitional auditory experiences.},
	number = {1},
	journal = {Int. J. Cyber Behav. Psychol. Learn.},
	author = {Ortiz de Gortari, Angelica B. and Griffiths, Mark D.},
	month = jan,
	year = {2014},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Auditory Verbal Hallucinations, Effects of Priming, Game Transfer Phenomena, Implicity Memory, Involuntary Auditory Imagery, Nueral Adaptations, Self-Monitoring Failures, Videogame Effects, Videogames' Auditory Cues},
	pages = {59--75},
}

@book{rosenzweig_successful_2015,
	address = {San Francisco, CA, USA},
	title = {Successful {User} {Experience}: {Strategies} and {Roadmaps}},
	isbn = {978-0-12-801061-7},
	abstract = {Successful User Experience: Strategy and Roadmaps provides you with a hands-on guide for pulling all of the User Experience (UX) pieces together to create a strategy that includes tactics, tools, and methodologies. Leveraging material honed in user experience courses and over 25 years in the field, the author explains the value of strategic models to refine goals against available data and resources. You will learn how to think about UX from a high level, design the UX while setting goals for a product or project, and how to turn that into concrete actionable steps. After reading this book, you'll understand: How to bring high-level planning into concrete actionable steps How Design Thinking relates to creating a good UX How to set UX Goals for a product or project How to decide which tool or methodology to use at what point in product lifecycle This book takes UX acceptance as a point of departure, and builds on it with actionable steps and case studies to develop a complete strategy, from the big picture of product design, development and commercialization, to how UX can help create stronger products. This is a must-have book for your complete UX library. Uses strategic models that focus product design and development Teaches how to decipher what tool or methodology is right for a given moment, project, or a specific team Presents tactics on how to understand how to connect the dots between tools, data, and design Provides actionable steps and case studies that help users develop a complete strategy, from the big picture of product design, development, and commercialization, to how UX can help create stronger products Case studies in each chapter to aid learning Table of Contents Foreword Acknowledgements Introduction What is UX Design Thinking UX Thinking Strategic Models Beyond Mobile: Device Agnostic UX Inspection Methods Usability Testing Iterating on the Design Moving Past the Lab Global UX and Online Studies Surveys Web Analytics/Social Media Service Design Getting Buy in Successful Stories Failure as Success Big Picture: Checklists and Roadmaps Glossary},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Rosenzweig, Elizabeth},
	year = {2015},
}

@article{cofta_foundations_2013,
	title = {The {Foundations} of a {Trustworthy} {Web}},
	volume = {3},
	issn = {1555-077X},
	url = {https://doi.org/10.1561/1800000020},
	doi = {10.1561/1800000020},
	abstract = {It is trivial to say that the Web has changed the lives of the majority of us. There is no activity that has not been touched by the Web (directly or indirectly) and — in this process — somehow altered by it. The Web (and other large ICT mega-systems) permeates our lives regardless of our awareness of it and whether we like it or not. We increasingly depend on it, and thus we would like to be able to trust it. Further, we would like it to be trustworthy — and these two constructs do not always go hand in hand. There is a separate monograph [33] that discusses trust and trustworthiness. This one provides a foundation for this discussion — a model.In order to systematically discuss the Web we have to have a model of it. However, a technical model does not suffice, as the Web is not only a technical but also a social phenomenon, embodied in technology. As the ultimate goal is to discuss trustworthiness of the Web, it would be beneficial to model the Web from the social perspective and to position the Web as a component of social practices — the component that interferes with them. This requires a model of society as well as an analysis of how the Web affects this model. This monograph does just that: it takes a particular model of society, extends it in a way that is relevant to the Web and defines the Web by systematically observing potential (and actual) changes.This monograph goes one step further by asking the question what the Web look should like to be more relevant to its social function — if there is one. Again, the objective of this discussion is to facilitate a trustworthy web, as it is much easier to agree what a trustworthy web should look like if we know what the Web is for. The answer to this question leads to some practical design guidelines discussed here, whether they are technical, managerial or related to the governance of the Web. It is not a blueprint for a 'better' Web, but it is a discussion of the Web that may be more relevant to society — and hopefully more trustworthy in satisfying that society's needs.},
	number = {3–4},
	journal = {Found. Trends Web Sci.},
	author = {Cofta, Piotr},
	month = may,
	year = {2013},
	note = {Place: Hanover, MA, USA
Publisher: Now Publishers Inc.},
	keywords = {Gs General Science, Rb General Computer Science, Rd Databases/Information Sciences},
	pages = {137--385},
}

@book{conrad_cissp_2012,
	edition = {2},
	title = {{CISSP} {Study} {Guide}},
	isbn = {978-1-59749-961-3},
	abstract = {The CISSP certification is the most prestigious, globally-recognized, vendor neutral exam for information security professionals. The newest edition of this acclaimed study guide is aligned to cover all of the material included in the newest version of the exams Common Body of Knowledge. The ten domains are covered completely and as concisely as possible with an eye to acing the exam. Each of the ten domains has its own chapter that includes specially designed pedagogy to aid the test-taker in passing the exam, including: Clearly stated exam objectives; Unique termsDefinitions; Exam Warnings; Learning by Example; Hands-On Exercises; Chapter ending questions. Furthermore, special features include: Two practice exams; Tiered chapter ending questions that allow for a gradual learning curve; and a self-test appendix Provides the most complete and effective study guide to prepare you for passing the CISSP exam-contains only what you need to pass the test, with no fluff! Eric Conrad has prepared hundreds of professionals for passing the CISSP exam through SANS, a popular and well-known organization for information security professionals. Covers all of the new information in the Common Body of Knowledge updated in January 2012, and also provides two practice exams, tiered end-of-chapter questions for a gradual learning curve, and a complete self-test appendix. Table of Contents Introduction How to Take the CISSP Exam Domain 1: Information Security Governance and Risk Management Domain 2: Access Control Domain 3: Cryptography Domain 4: Physical (Environmental) Security Domain 5: Security Architecture and Design Domain 6: Business Continuity and Disaster Recovery Planning Domain 7: Telecommunications and Network Security Domain 8: Application Development Security Domain 9: Operations Security Domain 10: Legal, Regulations, Investigations, and Compliance},
	publisher = {Syngress Publishing},
	author = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	year = {2012},
}

@article{kumar_inducing_2018,
	title = {Inducing {Personalities} and {Values} from {Language} {Use} in {Social} {Network} {Communities}},
	volume = {20},
	issn = {1387-3326},
	url = {https://doi.org/10.1007/s10796-017-9793-8},
	doi = {10.1007/s10796-017-9793-8},
	abstract = {A community in social networks is generally assumed to be composed of a group of individuals with similar characteristics. Although there has been a plethora of work on understanding network topologies (edge density, clustering coefficient, etc.) within an online community, the psycho-sociological compositions of social network communities have hardly been studied. The present paper aims to analyse the communities as composition of induced psycholinguistic and sociolinguistic variables (Personalities, Values and Ethics) across individuals in social media networks. The motivation behind this analysis is to understand the behavioural characteristics at individual as well as societal level in social networks. To this end, three studies were carried out on six different datasets: three Twitter corpora, two Facebook corpora, and an Essay corpus, annotated with Values and Ethics of the users. First, experiments on creating automatic models to determine the Personality and Values of individuals by analysing their language usage and social media behaviour. Second, experiments on understanding the characteristics or blend of characteristics of individuals within an online community. Finally, generation of a map of values and ethics for India, a multi-lingual and multi-cultural country. Striking similarities to general intuitive perception could be observed, i.e., the results obtained in the study resemble our general perception about the cities/towns of India.},
	number = {6},
	journal = {Information Systems Frontiers},
	author = {Kumar, Upendra and Reganti, Aishwarya N. and Maheshwari, Tushar and Chakroborty, Tanmoy and Gambäck, Björn and Das, Amitava},
	month = dec,
	year = {2018},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Community, Ethics, Personality, Social network, Values},
	pages = {1219--1240},
}

@inproceedings{beller_oops_2017,
	series = {{MSR} '17},
	title = {Oops, my tests broke the build: an explorative analysis of {Travis} {CI} with {GitHub}},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.62},
	doi = {10.1109/MSR.2017.62},
	abstract = {Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10\% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
	year = {2017},
	note = {Place: Buenos Aires, Argentina},
	pages = {356--367},
}

@inproceedings{osman_ambiguous_2018,
	address = {New York, NY, USA},
	series = {{RET} '18},
	title = {Ambiguous software requirement specification detection: an automated approach},
	isbn = {978-1-4503-5749-4},
	url = {https://doi.org/10.1145/3195538.3195545},
	doi = {10.1145/3195538.3195545},
	abstract = {Software requirement specification (SRS) document is the most crucial document in software development process. All subsequent steps in software development are influenced by this document. However, issues in requirement, such as ambiguity or incomplete specification may lead to misinterpretation of requirements which consequently, influence the testing activities and higher the risk of time and cost overrun of the project. Finding defects in the initial development phase is crucial since the defect that found late is more expensive than if it was found early. This study describes an automated approach for detecting ambiguous software requirement specification. To this end, we propose the combination of text mining and machine learning. Since the dataset is derived from Malaysian industrial SRS documents, this study only focuses on the Malaysian context. We used text mining for feature extraction and for preparing the training set. Based on this training set, the method 'learns' to detect the ambiguous requirement specification.In this paper, we study a set of nine (9) classification algorithms from the machine learning community and evaluate which algorithm performs best to detect the ambiguous software requirement specification. Based on the experiment's result, we develop a working prototype which later is used for our initial validation of our approach. The initial validation shows that the result produced by the classification model is reasonably acceptable. Even though this study is an experimental benchmark, we optimist that this approach may contributes to enhance the quality of SRS.},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Requirements} {Engineering} and {Testing}},
	publisher = {Association for Computing Machinery},
	author = {Osman, Mohd Hafeez and Zaharin, Mohd Firdaus},
	year = {2018},
	note = {event-place: Gothenburg, Sweden},
	keywords = {machine learning, software engineering, requirement engineering, text mining},
	pages = {33--40},
}

@article{elhabbash_self-awareness_2019,
	title = {Self-awareness in {Software} {Engineering}: {A} {Systematic} {Literature} {Review}},
	volume = {14},
	issn = {1556-4665},
	url = {https://doi.org/10.1145/3347269},
	doi = {10.1145/3347269},
	abstract = {Background: Self-awareness has been recently receiving attention in computing systems for enriching autonomous software systems operating in dynamic environments.Objective: We aim to investigate the adoption of computational self-awareness concepts in autonomic software systems and motivate future research directions on self-awareness and related problems.Method: We conducted a systemic literature review to compile the studies related to the adoption of self-awareness in software engineering and explore how self-awareness is engineered and incorporated in software systems. From 865 studies, 74 studies have been selected as primary studies. We have analysed the studies from multiple perspectives, such as motivation, inspiration, and engineering approaches, among others.Results: Results have shown that self-awareness has been used to enable self-adaptation in systems that exhibit uncertain and dynamic behaviour. Though there have been recent attempts to define and engineer self-awareness in software engineering, there is no consensus on the definition of self-awareness. Also, the distinction between self-aware and self-adaptive systems has not been systematically treated.Conclusions: Our survey reveals that self-awareness for software systems is still a formative field and that there is growing attention to incorporate self-awareness for better reasoning about the adaptation decision in autonomic systems. Many pending issues and open problems outline possible research directions.},
	number = {2},
	journal = {ACM Trans. Auton. Adapt. Syst.},
	author = {Elhabbash, Abdessalam and Salama, Maria and Bahsoon, Rami and Tino, Peter},
	month = oct,
	year = {2019},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {systematic literature review, survey, Adaptation processes, research challenges, self-adaptive software, self-aware software, self-properties, software architecture},
}

@book{noauthor_sosp_2013,
	address = {New York, NY, USA},
	title = {{SOSP} '13: {Proceedings} of the {Twenty}-{Fourth} {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	isbn = {978-1-4503-2388-8},
	abstract = {Welcome to the Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP 2013), held at the Nemacolin Woodlands Resort, Farmington, Pennsylvania, USA. This year's program includes 30 papers, and touches on a wide range of computer systems topics, from kernels to big data, from responsiveness to correctness, and from devices to data centers. The program committee made every effort to identify and include some of the most creative and thought-provoking ideas in computer systems today. Each accepted paper was shepherded by a program committee member to make sure the papers are as readable and complete as possible. We hope you will enjoy the program as much as we did in selecting it.},
	publisher = {Association for Computing Machinery},
	year = {2013},
}

@article{rai_survey_2020,
	title = {A {Survey} on {Computational} {Metaphor} {Processing}},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3373265},
	doi = {10.1145/3373265},
	abstract = {In the last decade, the problem of computational metaphor processing has garnered immense attention from the domains of computational linguistics and cognition. A wide panorama of approaches, ranging from a hand-coded rule system to deep learning techniques, have been proposed to automate different aspects of metaphor processing. In this article, we systematically examine the major theoretical views on metaphor and present their classification. We discuss the existing literature to provide a concise yet representative picture of computational metaphor processing. We conclude the article with possible research directions.},
	number = {2},
	journal = {ACM Comput. Surv.},
	author = {Rai, Sunny and Chakraverty, Shampa},
	month = mar,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {conceptual metaphor, creative text, figurative language, linguistic metaphor, Metaphor, metaphor detection, metaphor interpretation, metaphor processing},
}

@book{wu_cloud_2015,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {Cloud {Data} {Centers} and {Cost} {Modeling}: {A} {Complete} {Guide} {To} {Planning}, {Designing} and {Building} a {Cloud} {Data} {Center}},
	isbn = {0-12-801413-X},
	abstract = {Cloud Data Centers and Cost Modeling establishes a framework for strategic decision-makers to facilitate the development of cloud data centers. Just as building a house requires a clear understanding of the blueprints, architecture, and costs of the project; building a cloud-based data center requires similar knowledge. The authors take a theoretical and practical approach, starting with the key questions to help uncover needs and clarify project scope. They then demonstrate probability tools to test and support decisions, and provide processes that resolve key issues. After laying a foundation of cloud concepts and definitions, the book addresses data center creation, infrastructure development, cost modeling, and simulations in decision-making, each part building on the previous. In this way the authors bridge technology, management, and infrastructure as a service, in one complete guide to data centers that facilitates educated decision making. Explains how to balance cloud computing functionality with data center efficiency Covers key requirements for power management, cooling, server planning, virtualization, and storage management Describes advanced methods for modeling cloud computing cost including Real Option Theory and Monte Carlo Simulations Blends theoretical and practical discussions with insights for developers, consultants, and analysts considering data center development},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Wu, Caesar and Buyya, Rajkumar},
	year = {2015},
}

@inproceedings{noauthor_front_2021,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-030-78220-7},
	booktitle = {Design, {User} {Experience}, and {Usability}: {UX} {Research} and {Design}: 10th {International} {Conference}, {DUXU} 2021, {Held} as {Part} of the 23rd {HCI} {International} {Conference}, {HCII} 2021, {Virtual} {Event}, {July} 24–29, 2021, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	year = {2021},
	pages = {i--xxxii},
}

@book{noauthor_www_2019,
	address = {New York, NY, USA},
	title = {{WWW} '19: {The} {World} {Wide} {Web} {Conference}},
	isbn = {978-1-4503-6674-8},
	abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
	publisher = {Association for Computing Machinery},
	year = {2019},
}

@inproceedings{elphinstone_l3_2013,
	address = {New York, NY, USA},
	series = {{SOSP} '13},
	title = {From {L3} to {seL4} what have we learnt in 20 years of {L4} microkernels?},
	isbn = {978-1-4503-2388-8},
	url = {https://doi.org/10.1145/2517349.2522720},
	doi = {10.1145/2517349.2522720},
	abstract = {The L4 microkernel has undergone 20 years of use and evolution. It has an active user and developer community, and there are commercial versions which are deployed on a large scale and in safety-critical systems. In this paper we examine the lessons learnt in those 20 years about microkernel design and implementation. We revisit the L4 design papers, and examine the evolution of design and implementation from the original L4 to the latest generation of L4 kernels, especially seL4, which has pushed the L4 model furthest and was the first OS kernel to undergo a complete formal verification of its implementation as well as a sound analysis of worst-case execution times. We demonstrate that while much has changed, the fundamental principles of minimality and high IPC performance remain the main drivers of design and implementation decisions.},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {Association for Computing Machinery},
	author = {Elphinstone, Kevin and Heiser, Gernot},
	year = {2013},
	note = {event-place: Farminton, Pennsylvania},
	pages = {133--150},
}

@inproceedings{rapousis_qowater_2015,
	address = {New York, NY, USA},
	series = {{CySWater}'15},
	title = {{QoWater}: {A} crowd-sourcing approach for assessing the water quality},
	isbn = {978-1-4503-3485-3},
	url = {https://doi.org/10.1145/2738935.2738946},
	doi = {10.1145/2738935.2738946},
	abstract = {This paper presents QoWater, a novel user-centric crowdsourcing system that enables mobile users to evaluate the quality of drinking, recreational, and irrigation water. The QoWater follows a client-to-server architecture. Via QoWater clients, users upload their feedback about the water quality, together with their position and a timestamp in the spatio-temporal database of QoWater. At the same time, a wireless sensor network monitors the water distribution network (WDN) and uploads the measurements to the QoWater database. We have developed a proof-of-concept implementation of QoWater and performed a preliminary analysis. The paper also highlights the research directions and challenges for developing a robust and effective mechanism.},
	booktitle = {Proceedings of the 1st {ACM} {International} {Workshop} on {Cyber}-{Physical} {Systems} for {Smart} {Water} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Rapousis, Nikolaos and Katsarakis, Michalis and Papadopouli, Maria},
	year = {2015},
	note = {event-place: Seattle, WA, USA},
}

@article{waseem_design_2021,
	title = {Design, monitoring, and testing of microservices systems: {The} practitioners’ perspective},
	volume = {182},
	issn = {0164-1212},
	url = {https://doi.org/10.1016/j.jss.2021.111061},
	doi = {10.1016/j.jss.2021.111061},
	number = {C},
	journal = {J. Syst. Softw.},
	author = {Waseem, Muhammad and Liang, Peng and Shahin, Mojtaba and Di Salle, Amleto and Márquez, Gastón},
	month = dec,
	year = {2021},
	note = {Place: USA
Publisher: Elsevier Science Inc.},
	keywords = {Design, Testing, Industrial survey, Microservices architecture, Monitoring},
}

@inproceedings{liu_interior_2022,
	address = {New York, NY, USA},
	series = {{AIAM2021}},
	title = {Interior {Design} of {Smart} {Home} {Based} on {Intelligent} {3D} {Virtual} {Technology}},
	isbn = {978-1-4503-8504-6},
	url = {https://doi.org/10.1145/3495018.3495315},
	doi = {10.1145/3495018.3495315},
	abstract = {With the development of society and the continuous advancement of technology, people's research on smart home products is gradually deepening. Compared with the past, smart homes are no longer out of reach, but actually penetrate into our lives. However, while continuously exploring the technology of smart home products, designers have not conducted in-depth research on the interactive mode of the control interface of smart home products. Based on this, the purpose of this article is to study the smart home interior design based on intelligent three-dimensional virtual technology. This article first analyzes the development and current situation of smart homes, and on this basis, combines three-dimensional virtual reality technology to research and analyze smart home systems (S H S). This article systematically elaborates the design of functional modules of the S H S based on intelligent 3D virtual reality technology and the method of converting 2D plane to 3D space coordinate. And use comparative analysis method, interview method and other research methods to carry out experimental research on the theme of this article. The research shows that compared with the traditional S H S, the S H S based on 3D virtual technology studied in this paper has higher feasibility.},
	booktitle = {2021 3rd {International} {Conference} on {Artificial} {Intelligence} and {Advanced} {Manufacture}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Wei},
	year = {2022},
	note = {event-place: Manchester, United Kingdom},
	pages = {975--979},
}

@book{ginsberg_essentials_2012,
	address = {San Francisco, CA, USA},
	title = {Essentials of {Artificial} {Intelligence}},
	isbn = {978-0-323-13968-7},
	abstract = {Since its publication, Essentials of Artificial Intelligence has beenadopted at numerous universities and colleges offering introductory AIcourses at the graduate and undergraduate levels. Based on the author'scourse at Stanford University, the book is an integrated, cohesiveintroduction to the field. The author has a fresh, entertaining writingstyle that combines clear presentations with humor and AI anecdotes. At thesame time, as an active AI researcher, he presents the materialauthoritatively and with insight that reflects a contemporary, first handunderstanding of the field. Pedagogically designed, this book offers arange of exercises and examples. Table of Contents 1 Introduction: What is AI 2 Overview 3 Blind Search 4 Heuristic Search 5 Adversary Search 6 Introduction to Knowledge Representation 7 Predicate Logic 8 First-Order Logic 9 Putting Logic to Work: Control of Reasoning 10 Assumption-Based Truth Maintenance 11 Nonmonotonic Reasoning 12 Probability 13 Putting Knowledge to Work: Frames and Semantic Nets 14 Planning 15 Learning 16 Vision 17 Nature Language 18 Expert Systems 19 Concluding Remarks},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Ginsberg, Matt},
	year = {2012},
}

@book{noauthor_www_2019-1,
	address = {New York, NY, USA},
	title = {{WWW} '19: {Companion} {Proceedings} of {The} 2019 {World} {Wide} {Web} {Conference}},
	isbn = {978-1-4503-6675-5},
	abstract = {It is our great pleasure to welcome you to \&lt;I\&gt;The Web Conference 2019\&lt;/I\&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
	publisher = {Association for Computing Machinery},
	year = {2019},
}

@article{li_hybrid_2022,
	title = {Hybrid {IoT} and {Data} {Fusion} {Model} for e-{Commerce} {Big} {Data} {Analysis}},
	volume = {2022},
	issn = {1530-8669},
	url = {https://doi.org/10.1155/2022/2292321},
	doi = {10.1155/2022/2292321},
	abstract = {This article is aimed at studying the e-commerce big data recommendation model based on data fusion and the Internet of Things. This article chooses an embedded system for the construction of an e-commerce platform and uses data fusion technology to collect, transmit, and filter useful information from various information sources. Then, the collected information and data are analyzed and integrated, and visualization algorithms are used to better present data analysis, and association rules and structural similarity methods for electronic comparison are uses. This article uses the B/S architecture to design the overall framework of the data access layer, business logic layer, and user presentation layer; collects, organizes, stores, and presents the acquired consumer information; and finally analyzes the e-commerce background, platform performance, and supply and demand analysis. The experimental results show that the average clustering coefficient of the platform (0.7559) is smaller than the average clustering coefficient value of 8 items in the store (0.811) and smaller than the average network diameter and average path length of the online store (3.86, 7.7). Store products are better than store products, and the diameter and length of the product should be larger (2.71, 5.75). The recall rate of the e-commerce big data platform and model matching review method designed in this paper is 5\% higher than that of the word matching model method and has a better expected effect in terms of user supply and demand.},
	journal = {Wirel. Commun. Mob. Comput.},
	author = {Li, Bing and Lei, Qi and Elhoseny, Mohamed},
	month = jan,
	year = {2022},
	note = {Place: GBR
Publisher: John Wiley and Sons Ltd.},
}

@inproceedings{tsai_exploring_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Exploring and {Promoting} {Diagnostic} {Transparency} and {Explainability} in {Online} {Symptom} {Checkers}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445101},
	doi = {10.1145/3411764.3445101},
	abstract = {Online symptom checkers (OSC) are widely used intelligent systems in health contexts such as primary care, remote healthcare, and epidemic control. OSCs use algorithms such as machine learning to facilitate self-diagnosis and triage based on symptoms input by healthcare consumers. However, intelligent systems’ lack of transparency and comprehensibility could lead to unintended consequences such as misleading users, especially in high-stakes areas such as healthcare. In this paper, we attempt to enhance diagnostic transparency by augmenting OSCs with explanations. We first conducted an interview study (N=25) to specify user needs for explanations from users of existing OSCs. Then, we designed a COVID-19 OSC that was enhanced with three types of explanations. Our lab-controlled user study (N=20) found that explanations can significantly improve user experience in multiple aspects. We discuss how explanations are interwoven into conversation flow and present implications for future OSC designs.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tsai, Chun-Hua and You, Yue and Gui, Xinning and Kou, Yubo and Carroll, John M.},
	year = {2021},
	note = {event-place: Yokohama, Japan},
	keywords = {COVID-19, Explanation, Health, Symptom Checker, Transparency},
}

@inproceedings{noauthor_front_2020,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-030-59634-7},
	booktitle = {Cloud {Computing} – {CLOUD} 2020: 13th {International} {Conference}, {Held} as {Part} of the {Services} {Conference} {Federation}, {SCF} 2020, {Honolulu}, {HI}, {USA}, {September} 18-20, 2020, {Proceedings}},
	publisher = {Springer-Verlag},
	year = {2020},
	note = {event-place: Honolulu, HI, USA},
	pages = {i--xiv},
}

@article{fatima_how_2019,
	title = {How persuasive is a phishing email? {A}\&nbsp;phishing game for phishing awareness},
	volume = {27},
	issn = {0926-227X},
	url = {https://doi.org/10.3233/JCS-181253},
	doi = {10.3233/JCS-181253},
	number = {6},
	journal = {J. Comput. Secur.},
	author = {Fatima, Rubia and Yasin, Affan and Liu, Lin and Wang, Jianmin},
	month = jan,
	year = {2019},
	note = {Place: NLD
Publisher: IOS Press},
	keywords = {collaborative learning, empirical evaluation, human and social aspects, information assurance, Security and privacy, serious game, social engineering},
	pages = {581--612},
}

@article{kiewhuo_osadhi_2023,
	title = {{OSADHI} – {An} online structural and analytics based database for herbs of {India}},
	volume = {102},
	issn = {1476-9271},
	url = {https://doi.org/10.1016/j.compbiolchem.2022.107799},
	doi = {10.1016/j.compbiolchem.2022.107799},
	number = {C},
	journal = {Comput. Biol. Chem.},
	author = {Kiewhuo, Kikrusenuo and Gogoi, Dipshikha and Mahanta, Hridoy Jyoti and Rawal, Ravindra K. and Das, Debabrata and S, Vaikundamani and Jamir, Esther and Sastry, G. Narahari},
	month = feb,
	year = {2023},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {ADMET, Cheminformatics, Medicinal plants, Phytochemicals, Traditional Knowledge},
}

@inproceedings{noauthor_front_2022,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-031-17617-3},
	booktitle = {{HCI} {International} 2022 - {Late} {Breaking} {Papers}. {Multimodality} in {Advanced} {Interaction} {Environments}: 24th {International} {Conference} on {Human}-{Computer} {Interaction}, {HCII} 2022, {Virtual} {Event}, {June} 26 – {July} 1, 2022, {Proceedings}},
	publisher = {Springer-Verlag},
	year = {2022},
	pages = {i--xx},
}

@article{lazaro_cyber-physical_2015,
	title = {Cyber-{Physical} {Platform} {Development} for {Multivariable} {Artificial} {Pancreas} {Systems}},
	volume = {6},
	issn = {1947-9158},
	url = {https://doi.org/10.4018/IJHCR.2015070101},
	doi = {10.4018/IJHCR.2015070101},
	abstract = {This paper describes a distributed sensor platform for a new breed of artificial pancreas devices. In recent work, a multi-variable adaptive algorithm has been proposed which incorporates physical activity of the patients for accurate prediction and control of glucose levels. In order to facilitate this algorithm, the authors integrate a smartphone and multiple sensors including activity trackers and a glucose monitor into a distributed system. The proposed sensor platform provides real-time data access for the artificial pancreas control algorithm hosted on a remote device.},
	number = {3},
	journal = {Int. J. Handheld Comput. Res.},
	author = {Lazaro, Caterina and Oruklu, Erdal and Cinar, Ali},
	month = jul,
	year = {2015},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Activity Trackers, Artificial Pancreas, Bluetooth LE, Remote Database, Sensors Fusion},
	pages = {1--16},
}

@book{erder_continuous_2015,
	address = {San Francisco, CA, USA},
	title = {Continuous {Architecture}: {Sustainable} {Architecture} in an {Agile} and {Cloud}-{Centric} {World}},
	isbn = {978-0-12-803285-5},
	abstract = {Continuous Architecture provides a broad architectural perspective for continuous delivery, and describes a new architectural approach that supports and enables it. As the pace of innovation and software releases increases, IT departments are tasked to deliver value quickly and inexpensively to their business partners. With a focus on getting software into end-users hands faster, the ultimate goal of daily software updates is in sight to allow teams to ensure that they can release every change to the system simply and efficiently. This book presents an architectural approach to support modern application delivery methods and provide a broader architectural perspective, taking architectural concerns into account when deploying agile or continuous delivery approaches. The authors explain how to solve the challenges of implementing continuous delivery at the project and enterprise level, and the impact on IT processes including application testing, software deployment and software architecture. Covering the application of enterprise and software architecture concepts to the Agile and Continuous Delivery models Explains how to create an architecture that can evolve with applications Incorporates techniques including refactoring, architectural analysis, testing, and feedback-driven development Provides insight into incorporating modern software development when structuring teams and organizations Table of Contents A Brief Introduction to Continuous Architecture The Principles of Continuous Architecture (CA) Getting Started With the Continuous Architecture Process Evolving the Architecture Continuous Architecture and Continuous Delivery Validating the Architecture Continuous Architecture in Practice: A Case Study How Does Continuous Architecture Impact the Role of the Architect Continuous Architecture in the Enterprise What about Enterprise Services Conclusion},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Erder, Murat and Pureur, Pierre},
	year = {2015},
}

@article{basciftci_expert_2018,
	title = {An expert system design to diagnose cancer by using a new method reduced rule base},
	volume = {157},
	issn = {0169-2607},
	url = {https://doi.org/10.1016/j.cmpb.2018.01.020},
	doi = {10.1016/j.cmpb.2018.01.020},
	number = {C},
	journal = {Comput. Methods Prog. Biomed.},
	author = {Başçiftçi, Fatih and Avuçlu, Emre},
	month = apr,
	year = {2018},
	note = {Place: USA
Publisher: Elsevier North-Holland, Inc.},
	keywords = {Cancer symptoms and types, Expert system, Minimization method, Mobile programming},
	pages = {113--120},
}

@book{messier_collaboration_2014,
	edition = {1st},
	title = {Collaboration with {Cloud} {Computing}: {Security}, {Social} {Media}, and {Unified} {Communications}},
	isbn = {0-12-417040-4},
	abstract = {Collaboration with Cloud Computing discusses the risks associated with implementing these technologies across the enterprise and provides you with expert guidance on how to manage risk through policy changes and technical solutions. Drawing upon years of practical experience and using numerous examples and case studies, author Ric Messier discusses: The evolving nature of information security The risks, rewards, and security considerations when implementing SaaS, cloud computing and VoIP Social media and security risks in the enterprise The risks and rewards of allowing remote connectivity and accessibility to the enterprise network Discusses the risks associated with technologies such as social media, voice over IP (VoIP) and cloud computing and provides guidance on how to manage that risk through policy changes and technical solutions Presents a detailed look at the risks and rewards associated with cloud computing and storage as well as software as a service (SaaS) and includes pertinent case studies Explores the risks associated with the use of social media to the enterprise network Covers the bring-your-own-device(BYOD) trend, including policy considerations and technical requirements},
	publisher = {Syngress Publishing},
	author = {Messier, Ric},
	year = {2014},
}

@book{timm_school_2014,
	address = {USA},
	title = {School {Security}: {How} to {Build} and {Strengthen} a {School} {Safety} {Program}},
	isbn = {978-0-12-407873-4},
	abstract = {It seems that every day there's a new story about a security lapse, emergency lock-down, or violent act taking place at a school somewhere in the United States. Today it's simply inexcusable not to have adequate security measures in place regardless of how safe you think your community may be. In School Security, author Paul Timm, a nationally acclaimed school security expert, explains how to make your institution a safer place to learn with easy-to-follow steps. Throughout the book, Timm emphasizes a proactive rather than reactive approach to school security. Readers are introduced to basic loss prevention and safety concepts, including how to communicate safety information to students and staff, how to raise security awareness, and how to prepare for emergencies. The book discusses how to positively influence student behavior, lead staff training programs, and write sound security policies. An entire chapter is dedicated to describing what school security resources are available for follow-up reading and further training. School Security isn't just a book for security professionals: it helps people without formal security training namely, educators and school administrators effectively address school risk. Serves as a comprehensive guide for building an effective security program at little or no cost. Covers fundamental crime prevention concepts, making it suitable for both school security professionals and educators with no formal security training. Addresses the risks commonly facing school administrators today, from access control to social media. Takes a holistic approach to school security rather than focusing on a particular threat or event. Table of Contents Chapter 1: What Is School Security Chapter 2: How Safe Is Your School Chapter 3: Developing a Plan Chapter 4: Securing Your Environment Chapter 5: Influencing Behavior Chapter 6: Preparing Your People Chapter 7: Managing Emergencies Chapter 8: Tackling Social Media Risks Chapter 9: School Security Resources and Conclusion},
	publisher = {Butterworth-Heinemann},
	author = {Timm, Paul},
	year = {2014},
}

@inproceedings{noauthor_front_2022-1,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-031-05310-8},
	booktitle = {Human-{Computer} {Interaction}. {Theoretical} {Approaches} and {Design} {Methods}: {Thematic} {Area}, {HCI} 2022, {Held} as {Part} of the 24th {HCI} {International} {Conference}, {HCII} 2022, {Virtual} {Event}, {June} 26–{July} 1, 2022, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	year = {2022},
	pages = {i--xxxiv},
}

@inproceedings{kim_sensr_2013,
	address = {New York, NY, USA},
	series = {{CSCW} '13},
	title = {Sensr: evaluating a flexible framework for authoring mobile data-collection tools for citizen science},
	isbn = {978-1-4503-1331-5},
	url = {https://doi.org/10.1145/2441776.2441940},
	doi = {10.1145/2441776.2441940},
	abstract = {Across HCI and social computing platforms, mobile applications that support citizen science, empowering non-experts to explore, collect, and share data have emerged. While many of these efforts have been successful, it remains difficult to create citizen science applications without extensive programming expertise. To address this concern, we present Sensr, an authoring environment that enables people without programming skills to build mobile data collection and management tools for citizen science. We demonstrate how Sensr allows people without technical skills to create mobile applications. Findings from our case study demonstrate that our system successfully overcomes technical constraints and provides a simple way to create mobile data collection tools.},
	booktitle = {Proceedings of the 2013 {Conference} on {Computer} {Supported} {Cooperative} {Work}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Sunyoung and Mankoff, Jennifer and Paulos, Eric},
	year = {2013},
	note = {event-place: San Antonio, Texas, USA},
	keywords = {sustainability, citizen science, mobile applications},
	pages = {1453--1462},
}

@book{noauthor_siggraph_2018,
	address = {New York, NY, USA},
	title = {{SIGGRAPH} '18: {ACM} {SIGGRAPH} 2018 {Courses}},
	isbn = {978-1-4503-5809-5},
	abstract = {Learn New Concepts and SkillsSIGGRAPH courses are learning sessions in which experts from all areas of computer graphics technology and interactive techniques share their knowledge. Course presenters distill key concepts and ideas into self-contained lessons. Courses may lie anywhere on a continuum from conceptual and theoretical to practical and applied.Courses are presented in both long (3.25 hour) or short (1.5 hour) sessions and may include elements of interactive demonstration, performance, or other imaginative approaches to teaching.},
	publisher = {Association for Computing Machinery},
	year = {2018},
}

@article{huang_designing_2020,
	title = {Designing for cultural learning and reflection using {IoT} serious game approach},
	volume = {25},
	issn = {1617-4909},
	url = {https://doi.org/10.1007/s00779-020-01482-4},
	doi = {10.1007/s00779-020-01482-4},
	abstract = {Previous studies have highlighted the difficulty that designers face in designing interactive systems that will help visitors learn and reflect upon cultural issues in support of museums’ new roles of shaping cultural identify and community building. In this paper, we report a study to explore the potential of Internet of things (IoT) serious games to support cultural learning and reflection by incorporating five IoT-mediated interactive exhibits—an online trading game, an interactive grinder and Goldsmith simulators, a lantern design and a virtual house decoration station. We present design goals to meet the aim and evaluate our approach through a museum study. The results show significant learning gains from pre-test to post-test, and our interview analyses indicate some evidence of participants engaging in all levels of reflection. From our findings, we discuss the design goals and recommend design guidelines for future IoT serious games to support reflective learning in museums.},
	number = {3},
	journal = {Personal Ubiquitous Comput.},
	author = {Huang, Hai and Ng, Kher Hui},
	month = nov,
	year = {2020},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Digital culture, Internet of things, Museum, Reflection, Serious games},
	pages = {509--524},
}

@inproceedings{wei_food_2014,
	address = {New York, NY, USA},
	series = {{CHI} '14},
	title = {Food messaging: using edible medium for social messaging},
	isbn = {978-1-4503-2473-1},
	url = {https://doi.org/10.1145/2556288.2557026},
	doi = {10.1145/2556288.2557026},
	abstract = {Food is more than just a means of survival; it is also a form of communication. In this paper, we investigate the potential of food as a social message carrier (a.k.a., food messaging). To investigate how people accept, use, and perceive food messaging, we conducted exploratory interviews, a field study, and follow-up interviews over four weeks in a large information technology (IT) company. We collected 904 messages sent by 343 users. Our results suggest strong acceptance of food messaging as an alternative message channel. Further analysis implies that food messaging embodies characteristics of both text messaging and gifting. It is preferred in close relationships for its evocation of positive emotions. As the first field study on edible social messaging, our empirical findings provide valuable insights into the uniqueness of food as a message carrier and its capabilities to promote greater social bonding.},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Jun and Ma, Xiaojuan and Zhao, Shengdong},
	year = {2014},
	note = {event-place: Toronto, Ontario, Canada},
	keywords = {field study, affective communication, edible social messaging, food hci, food messaging, food printer},
	pages = {2873--2882},
}

@inproceedings{martins_web_2012,
	address = {Berlin, Heidelberg},
	title = {A {Web} {Portal} for the {Certification} of {Open} {Source} {Software}},
	isbn = {978-3-642-54337-1},
	url = {https://doi.org/10.1007/978-3-642-54338-8_20},
	doi = {10.1007/978-3-642-54338-8_20},
	abstract = {This paper presents a web portal for the certification of open source software. The portal aims at helping programmers in the internet age, when there are too many open source reusable libraries and tools available. Our portal offers programmers a web-based and easy setting to analyze and certify open source software, which is a crucial step to help programmers choosing among many available alternatives, and to get some guarantees before using one piece of software.The paper presents our first prototype of such web portal. It also describes in detail a domain specific language that allows programmers to describe with a high degree of abstraction specific open source software certifications. The design and implementation of this language is the core of the web portal.},
	booktitle = {Revised {Selected} {Papers} of the {SEFM} 2012 {Satellite} {Events} on {Information} {Technology} and {Open} {Source}: {Applications} for {Education}, {Innovation}, and {Sustainability} - {Volume} 7991},
	publisher = {Springer-Verlag},
	author = {Martins, Pedro and Fernandes, João P. and Saraiva, João},
	year = {2012},
	keywords = {Open source software, Programming languages, Software analysis, Software certification},
	pages = {244--260},
}

@inproceedings{noauthor_front_2022-2,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-031-06423-4},
	booktitle = {Human {Interface} and the {Management} of {Information}: {Visual} and {Information} {Design}: {Thematic} {Area}, {HIMI} 2022, {Held} as {Part} of the 24th {HCI} {International} {Conference}, {HCII} 2022, {Virtual} {Event}, {June} 26 – {July} 1, 2022, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	year = {2022},
	pages = {i--xxv},
}

@inproceedings{noauthor_front_2022-3,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-031-06508-8},
	booktitle = {Human {Interface} and the {Management} of {Information}: {Applications} in {Complex} {Technological} {Environments}: {Thematic} {Area}, {HIMI} 2022, {Held} as {Part} of the 24th {HCI} {International} {Conference}, {HCII} 2022, {Virtual} {Event}, {June} 26 – {July} 1, 2022, {Proceedings}, {Part} {II}},
	publisher = {Springer-Verlag},
	year = {2022},
	pages = {i--xxvi},
}

@article{mathur_sapdb_2021,
	title = {{SAPdb}: {A} database of short peptides and the corresponding nanostructures formed by self-assembly},
	volume = {133},
	issn = {0010-4825},
	url = {https://doi.org/10.1016/j.compbiomed.2021.104391},
	doi = {10.1016/j.compbiomed.2021.104391},
	number = {C},
	journal = {Comput. Biol. Med.},
	author = {Mathur, Deepika and Kaur, Harpreet and Dhall, Anjali and Sharma, Neelam and Raghava, Gajendra P.S.},
	month = jun,
	year = {2021},
	note = {Place: USA
Publisher: Pergamon Press, Inc.},
	keywords = {Database, Dipeptide, Nanostructure, Self-assembly, Tripeptide},
}

@inproceedings{noauthor_front_2022-4,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-031-06037-3},
	booktitle = {Cross-{Cultural} {Design}. {Interaction} {Design} {Across} {Cultures}: 14th {International} {Conference}, {CCD} 2022, {Held} as {Part} of the 24th {HCI} {International} {Conference}, {HCII} 2022, {Virtual} {Event}, {June} 26 – {July} 1, 2022, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	year = {2022},
	pages = {i--xxii},
}

@book{leibtag_digital_2013,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {The {Digital} {Crown}: {Winning} at {Content} on the {Web}},
	isbn = {0-12-407674-2},
	abstract = {In 1997, Bill Gates famously said "Content is king." Since then, the digital marketing world has been scrambling to fulfil this promise, as we finally shift our focus to what consumers really want from our brands: a conversation.The Digital Crown walks you through the essentials of crafting great content: the fundamentals of branding, messaging, business goal alignment, and creating portable, mobile content that is future-ready. Systems create freedom, and within this book you'll learn the seven critical rules to align your internal and external content processes, including putting your audience first, involving stakeholders early and often, and creating multidisciplinary content teams. Complete with cases studies and experience drawn directly from global content projects, you are invited to observe the inner workings of successful content engagements. You'll learn how to juggle the demands of IT, design, and content teams, while acquiring all the practical tools you need to devise a roadmap for connecting and engaging with your customers. This is your next step on the journey to creating and managing winning content to engage your audience and keep them coming back for more.Discover easy-to-follow, simple breakdowns of the major ideas behind engaging with your customer Learn both the theoretical and practical applications of content and communication on-line Maximize on the case studies and real-world examples, enabling you to find the best fit for your own business},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Leibtag, Ahava},
	year = {2013},
}

@book{price_practical_2012,
	address = {USA},
	edition = {2},
	title = {Practical {Aviation} {Security}: {Predicting} and {Preventing} {Future} {Threats}},
	isbn = {978-0-12-391485-9},
	abstract = {The second edition of Practical Aviation Security is a complete guide to the aviation security system, from crucial historical events to the policies, policymakers, and major terrorist and criminal acts that have shaped the procedures in use today. The tip-of-the-spear technologies that are shaping the future are also addressed. This text equips readers in airport security or other aviation management roles with the knowledge to implement the effective security programs, to meet international guidelines, and to responsibly protect facilities or organizations of any size. Using case studies and practical security measures now in use at airports worldwide, readers learn the effective methods and the fundamental principles involved in designing and implementing a security system. The aviation security system is comprehensive and requires continual focus and attention to stay a step ahead of the next attack. Practical Aviation Security, Second Edition helps prepare practitioners to enter the industry, and helps seasoned professionals prepare for new threats and prevent new tragedies. Covers commercial airport security, general aviation and cargo operations, threats, and threat detection and response systems, as well as international security issues Lays out the security fundamentals that can ensure the future of global travel and commerce Applies real-world aviation experience to the task of anticipating and deflecting threats Table of Contents Foreword by Jeanne Olivier, General Manager, Aviation Security \&amp; Technology, Port Authority of NY \&amp; NJ Introduction by Rafi Ron, Director of Security, Ben-Gurion Intl Airport, 1997-2001 Preface Acknowledgments Chapter 1 Overview of the Aviation Industry and Security in the Post 9-11 World Chapter 2 Crime and Terrorism in Aviation: A Retrospective Chapter 3 Policies and Procedures: The Development of Aviation Security Practices Chapter 4 The Role of Government in Aviation Security Chapter 5 Commercial Aviation Airport Security Chapter 6 Introduction to Screening Chapter 7 Passenger and Baggage Screening Chapter 8 Commercial Aviation Aircraft Operator Security Chapter 9 General Aviation Security Chapter 10 Air Cargo Chapter 11 The Threat Matrix Chapter 12 Security Operations},
	publisher = {Butterworth-Heinemann},
	author = {Price, Jeffrey and Forrest, Jeffrey},
	year = {2012},
}

@inproceedings{noauthor_front_2021-1,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-030-90962-8},
	booktitle = {{HCI} {International} 2021 - {Late} {Breaking} {Papers}: {Multimodality}, {EXtended} {Reality}, and {Artificial} {Intelligence}: 23rd {HCI} {International} {Conference}, {HCII} 2021, {Virtual} {Event}, {July} 24–29, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	year = {2021},
	pages = {i--xxi},
}

@book{balasubramanian_conformal_2014,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {Conformal {Prediction} for {Reliable} {Machine} {Learning}: {Theory}, {Adaptations} and {Applications}},
	isbn = {0-12-398537-4},
	abstract = {The conformal predictions framework is a recent development in machine learning that can associate a reliable measure of confidence with a prediction in any real-world pattern recognition application, including risk-sensitive applications such as medical diagnosis, face recognition, and financial risk prediction. Conformal Predictions for Reliable Machine Learning: Theory, Adaptations and Applications captures the basic theory of the framework, demonstrates how to apply it to real-world problems, and presents several adaptations, including active learning, change detection, and anomaly detection. As practitioners and researchers around the world apply and adapt the framework, this edited volume brings together these bodies of work, providing a springboard for further research as well as a handbook for application in real-world problems. Understand the theoretical foundations of this important framework that can provide a reliable measure of confidence with predictions in machine learning Be able to apply this framework to real-world problems in different machine learning settings, including classification, regression, and clustering Learn effective ways of adapting the framework to newer problem settings, such as active learning, model selection, or change detection},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Balasubramanian, Vineeth and Ho, Shen-Shyang and Vovk, Vladimir},
	year = {2014},
}

@article{porto_virtual_2021,
	title = {Virtual screening of peptides with high affinity for {SARS}-{CoV}-2 main protease},
	volume = {133},
	issn = {0010-4825},
	url = {https://doi.org/10.1016/j.compbiomed.2021.104363},
	doi = {10.1016/j.compbiomed.2021.104363},
	number = {C},
	journal = {Comput. Biol. Med.},
	author = {Porto, William Farias},
	month = jun,
	year = {2021},
	note = {Place: USA
Publisher: Pergamon Press, Inc.},
	keywords = {COVID-19, Genetic algorithm, Molecular docking, RESTful API},
}

@inproceedings{noauthor_front_2022-5,
	address = {Berlin, Heidelberg},
	title = {Front {Matter}},
	isbn = {978-3-031-05411-2},
	booktitle = {Human-{Computer} {Interaction}. {User} {Experience} and {Behavior}: {Thematic} {Area}, {HCI} 2022, {Held} as {Part} of the 24th {HCI} {International} {Conference}, {HCII} 2022, {Virtual} {Event}, {June} 26 – {July} 1, 2022, {Proceedings}, {Part} {III}},
	publisher = {Springer-Verlag},
	year = {2022},
	pages = {i--xxii},
}

@inproceedings{tomtsis_iot_2016,
	address = {New York, NY, USA},
	series = {{SEEDA}-{CECNSM} '16},
	title = {{IoT} {Architecture} for {Monitoring} {Wine} {Fermentation} {Process} of {Debina} {Variety} {Semi}-{Sparkling} {Wine}},
	isbn = {978-1-4503-4810-2},
	url = {https://doi.org/10.1145/2984393.2984398},
	doi = {10.1145/2984393.2984398},
	abstract = {This paper proposes a new system architecture and HTTP communication mechanism called Smart Barrel System (Wine-SBS) for the process of monitoring Debina varietal sparkling wine fermenting conditions, produced at the area of Zitsa Epirus, Greece. The system includes microcontroller equipment with sensors that monitor wine attributes and storage conditions, called CBS-sensor transceivers, which are distributed among the debina fermentation vessels. The transmission of measurements, which occur periodically, are sent to a central cloud system application service. The CBS-sensor data are collected by a CBS-sensor collector and then follows an HTTP/2 request of multiplexed HTTP flows to a remote application server.},
	booktitle = {Proceedings of the {SouthEast} {European} {Design} {Automation}, {Computer} {Engineering}, {Computer} {Networks} and {Social} {Media} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Tomtsis, Dimitrios and Kontogiannis, Sotirios and Kokkonis, George and Zinas, Nicholas},
	year = {2016},
	note = {event-place: Kastoria, Greece},
	keywords = {Precision enology, wine fermentation monitoring system, wireless sensor network},
	pages = {42--47},
}

@article{ramli_integrated_2021,
	title = {Integrated {Smart} {Home} {Model}: {An} {IoT} {Learning}-{Inspired} {Platform}},
	volume = {17},
	issn = {1548-1093},
	url = {https://doi.org/10.4018/IJWLTT.20220501.oa1},
	doi = {10.4018/IJWLTT.20220501.oa1},
	abstract = {Today, in the realm of Industry 4.0, vastly diverse Internet of Things (IoT) technology are integrated everywhere, not to mention included in academic programs in schools and universities. Domain ratio of the final year projects in Universiti Teknologi Mara exposes a staggering hype in IoT as compared to other domains despite not having IoT included in any of the courses. Meanwhile, to fulfill the needs of the student in exploring this technology, an integrated IoT learning platform is developed. It integrates an IoT smart home model and a web-based interface as a learning platform to inspire hands-on learning for the students. The raspberry pi, motion sensor, analog gas sensor, atmospheric sensor, ultrasonic proximity sensor, and rain detector sensor are integrated together in a Lego-built smart home model where its connectivity and readings are displayed in a simple web interface to enable and inspire learning. A manual to set up the entire model is also prepared as a guide for students to set up and further explore the functionalities and operabilities of “things”.},
	number = {3},
	journal = {Int. J. Web-Based Learn. Teach. Technol.},
	author = {Ramli, Nurshahrily Idura and Rawi, Mohd Izani Mohamed and Rebuan, Fatin Nur Nabila},
	month = nov,
	year = {2021},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Internet of Things, Analog Gas Sensor, Atmospheric Sensor, Motion Sensor, Rain Detector Sensor, Raspberry Pi, Sensors, Ultrasonic Proximity Sensor, Web-Based System},
	pages = {1--14},
}

@book{noauthor_siggraph_2014,
	address = {New York, NY, USA},
	title = {{SIGGRAPH} '14: {ACM} {SIGGRAPH} 2014 {Studio}},
	isbn = {978-1-4503-2977-4},
	publisher = {Association for Computing Machinery},
	year = {2014},
}

@inproceedings{kaiying_legitimacy_2016,
	address = {New York, NY, USA},
	series = {{PDC} '16},
	title = {Legitimacy, boundary objects \&amp; participation in transnational {DIY} biology},
	isbn = {978-1-4503-4046-5},
	url = {https://doi.org/10.1145/2940299.2940307},
	doi = {10.1145/2940299.2940307},
	abstract = {Prior research has stipulated that DIY making appeals to many of the concerns central to participatory design: democratization of technology production, individual empowerment and inclusivity. In this paper, we take this stipulation as the starting point of our inquiry, exploring how it happened that making came to be seen as enabler of participatory values and practices. We draw from ethnographic research that followed a transnational collaboration between DIY biologists, scientists, makers, and artists from Indonesia, Europe and India. The paper focuses on the production of three artifacts, tracing their enactment as boundary objects and experimentation in DIY biology. The artifacts did not only help legitimize DIYbio, but also positioned Indonesia itself as a legitimate participant in international networks of knowledge production. The paper contributes to prior research that has challenged stable frames like West/the rest. It draws out a positionality for PD that opens up making by recognizing its multiplicity crucial to the making of alternative and never stable futures.},
	booktitle = {Proceedings of the 14th {Participatory} {Design} {Conference}: {Full} {Papers} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Kaiying, Cindy Lin and Lindtner, Silvia},
	year = {2016},
	note = {event-place: Aarhus, Denmark},
	keywords = {DIY, DIYbio, do it yourself, Indonesia, innovation, knowledge production, maker, making, PD, STS, transnational},
	pages = {171--180},
}

@article{sudjianto_linear_2023,
	title = {Linear iterative feature embedding: an ensemble framework for an interpretable model},
	volume = {35},
	issn = {0941-0643},
	url = {https://doi.org/10.1007/s00521-023-08204-w},
	doi = {10.1007/s00521-023-08204-w},
	abstract = {A new ensemble framework for an interpretable model called linear iterative feature embedding (LIFE) has been developed to achieve high prediction accuracy, easy interpretation, and efficient computation simultaneously. The LIFE algorithm is able to fit a wide single-hidden-layer neural network (NN) accurately with three steps: defining the subsets of a dataset by the linear projections of neural nodes, creating the features from multiple narrow single-hidden-layer NNs trained on the different subsets of the data, combining the features with a linear model. The theoretical rationale behind LIFE is also provided by the connection to the loss ambiguity decomposition of stack ensemble methods. Both simulation and empirical experiments confirm that LIFE consistently outperforms directly trained single-hidden-layer NNs and also outperforms many other benchmark models, including multilayers feed forward neural network (FFNN), Xgboost, and random forest (RF) in many experiments. As a wide single-hidden-layer NN, LIFE is intrinsically interpretable. Meanwhile, both variable importance and global main and interaction effects can be easily created and visualized. In addition, the parallel nature of the base learner building makes LIFE computationally efficient by leveraging parallel computing.},
	number = {13},
	journal = {Neural Comput. Appl.},
	author = {Sudjianto, Agus and Qiu, Jinwen and Li, Miaoqi and Chen, Jie},
	month = mar,
	year = {2023},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Ensemble method, Interaction detection, Linear iterative feature embedding, Loss decomposition, Variable importance},
	pages = {9657--9685},
}

@article{gui_tree_2013,
	title = {The {Tree} of {Knowledge} {Project}: {Organic} {Designs} as {Virtual} {Learning} {Spaces}},
	volume = {4},
	issn = {1947-8518},
	url = {https://doi.org/10.4018/jvple.2013040105},
	doi = {10.4018/jvple.2013040105},
	abstract = {The virtual Department of English at the Hong Kong Polytechnic University, also known as the Tree of Knowledge, is a project premised upon using ecology and organic forms to promote language learning in Second Life SL. Inspired by Salmon's 2010 Tree of Learning concept this study examines how an interactive ecological environment-in this case, a tree-might offer numerous learning possibilities via every segment of the structure. Third-party billboard and sculpt modeling techniques, SL building tools and mega prim applications which are more effective for organic shapes were used to develop a three dimensional textured trunk, two-faced layered leaves and size-locked branches, crown, and roots. Preliminary student survey responses to the various elements of the virtual department architecture included an appreciation for creativity, innovation, and attractiveness in the design; challenges included a sense of dizziness when maneuvering around, difficulty in controlling the avatar, slow computer system responses, and lack of instruction in how to navigate through the structure.},
	number = {2},
	journal = {Int. J. Virtual Pers. Learn. Environ.},
	author = {Gui, Dean A. F. and AuYeung, Gigi},
	month = apr,
	year = {2013},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Ecology, English, Language Learning, Second Life SL, Texturing, Tree Of Knowledge},
	pages = {85--106},
}

@inproceedings{hara_improving_2013,
	address = {New York, NY, USA},
	series = {{ASSETS} '13},
	title = {Improving public transit accessibility for blind riders by crowdsourcing bus stop landmark locations with {Google} street view},
	isbn = {978-1-4503-2405-2},
	url = {https://doi.org/10.1145/2513383.2513448},
	doi = {10.1145/2513383.2513448},
	abstract = {Low-vision and blind bus riders often rely on known physical landmarks to help locate and verify bus stop locations (e.g., by searching for a shelter, bench, newspaper bin). However, there are currently few, if any, methods to determine this information a priori via computational tools or services. In this paper, we introduce and evaluate a new scalable method for collecting bus stop location and landmark descriptions by combining online crowdsourcing and Google Street View (GSV). We conduct and report on three studies in particular: (i) a formative interview study of 18 people with visual impairments to inform the design of our crowdsourcing tool; (ii) a comparative study examining differences between physical bus stop audit data and audits conducted virtually with GSV; and (iii) an online study of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of crowdsourcing bus stop audits using our custom tool with GSV. Our findings reemphasize the importance of landmarks in non-visual navigation, demonstrate that GSV is a viable bus stop audit dataset, and show that minimally trained crowd workers can find and identify bus stop landmarks with 82.5\% accuracy across 150 bus stop locations (87.3\% with simple quality control).},
	booktitle = {Proceedings of the 15th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {Association for Computing Machinery},
	author = {Hara, Kotaro and Azenkot, Shiri and Campbell, Megan and Bennett, Cynthia L. and Le, Vicki and Pannella, Sean and Moore, Robert and Minckler, Kelly and Ng, Rochelle H. and Froehlich, Jon E.},
	year = {2013},
	note = {event-place: Bellevue, Washington},
	keywords = {accessible bus stops, low-vision and blind users, crowdsourcing accessibility, google street view, mechanical turk},
}

@article{meneghetti_dimensionality_2023,
	title = {A dimensionality reduction approach for convolutional neural networks},
	volume = {53},
	issn = {0924-669X},
	url = {https://doi.org/10.1007/s10489-023-04730-1},
	doi = {10.1007/s10489-023-04730-1},
	abstract = {The focus of this work is on the application of classical Model Order Reduction techniques, such as Active Subspaces and Proper Orthogonal Decomposition, to Deep Neural Networks. We propose a generic methodology to reduce the number of layers in a pre-trained network by combining the aforementioned techniques for dimensionality reduction with input-output mappings, such as Polynomial Chaos Expansion and Feedforward Neural Networks. The motivation behind compressing the architecture of an existing Convolutional Neural Network arises from its usage in embedded systems with specific storage constraints. The conducted numerical tests demonstrate that the resulting reduced networks can achieve a level of accuracy comparable to the original Convolutional Neural Network being examined, while also saving memory allocation. Our primary emphasis lies in the field of image recognition, where we tested our methodology using VGG-16 and ResNet-110 architectures against three different datasets: CIFAR-10, CIFAR-100, and a custom dataset.},
	number = {19},
	journal = {Applied Intelligence},
	author = {Meneghetti, Laura and Demo, Nicola and Rozza, Gianluigi},
	month = jul,
	year = {2023},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Active subspaces, Deep neural networks, Neural network reduction, Proper orthogonal decomposition},
	pages = {22818--22833},
}

@article{m_time-series_2021,
	title = {Time-{Series} {Forecasting} and {Analysis} of {COVID}-19 {Outbreak} in {Highly} {Populated} {Countries}: {A} {Data}-{Driven} {Approach}},
	volume = {13},
	issn = {1947-315X},
	url = {https://doi.org/10.4018/IJEHMC.20220701.oa3},
	doi = {10.4018/IJEHMC.20220701.oa3},
	abstract = {A novel corona virus, COVID-19 is spreading across different countries in an alarming proportion and it has become a major threat to the existence of human community. With more than eight lakh death count within a very short span of seven months, this deadly virus has affected more than 24 million people across 213 countries and territories around the world. Time-series analysis, modeling and forecasting is an important research area that explores the hidden insights from larger set of time-bound data for arriving better decisions. In this work, data analysis on COVID-19 dataset is performed by comparing the top six populated countries in the world. The data used for the evaluation is taken for a time period from 22nd January 2020 to 23rd August 2020.A novel time-series forecasting approach based on Auto-regressive integrated moving average (ARIMA) model is also proposed. The results will help the researchers from medical and scientific community to gauge the trend of the disease spread and improvise containment strategies accordingly.},
	number = {2},
	journal = {Int. J. E-Health Med. Commun.},
	author = {M, Arunkumar P. and Ramasamy, Lakshmana Kumar and M, Amala Jayanthi},
	month = jul,
	year = {2021},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {COVID-19, ARIMA, Data Analysis, Disease Spread, Time-Series Forecasting},
	pages = {1--17},
}

@inproceedings{putjorn_designing_2017,
	address = {New York, NY, USA},
	series = {{MobileHCI} '17},
	title = {Designing a ubiquitous sensor-based platform to facilitate learning for young children in {Thailand}},
	isbn = {978-1-4503-5075-4},
	url = {https://doi.org/10.1145/3098279.3098525},
	doi = {10.1145/3098279.3098525},
	abstract = {Education plays an important role in helping developing nations reduce poverty and improving quality of life. Ubiquitous and mobile technologies could greatly enhance education in such regions by providing augmented access to learning. This paper presents a three-year iterative study where a ubiquitous sensor based learning platform was designed, developed and tested to support science learning among primary school students in underprivileged Northern Thailand. The platform is built upon the school's existing mobile devices and was expanded to include sensor-based technology. Throughout the iterative design process, observations, interviews and group discussions were carried out with stakeholders. This lead to key reflections and design concepts such as the value of injecting anthropomorphic qualities into the learning device and providing personally and culturally relevant learning experiences through technology. Overall, the results outlined in this paper help contribute to knowledge regarding the design, development and implementation of ubiquitous sensor-based technology to support learning.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services}},
	publisher = {Association for Computing Machinery},
	author = {Putjorn, Pruet and Siriaraya, Panote and Ang, Chee Siang and Deravi, Farzin},
	year = {2017},
	note = {event-place: Vienna, Austria},
	keywords = {mobile devices, mobile learning, primary school students, ubiquitous learning environment, wireless sensing},
}

@article{jongeling_negative_2017,
	title = {On negative results when using sentiment analysis tools for software engineering research},
	volume = {22},
	issn = {1382-3256},
	url = {https://doi.org/10.1007/s10664-016-9493-x},
	doi = {10.1007/s10664-016-9493-x},
	abstract = {Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to diverging conclusions. Finally, we perform two replications of previously published studies and observe that the results of those studies cannot be confirmed when a different sentiment analysis tool is used.},
	number = {5},
	journal = {Empirical Softw. Engg.},
	author = {Jongeling, Robbert and Sarkar, Proshanta and Datta, Subhajit and Serebrenik, Alexander},
	month = oct,
	year = {2017},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Negative results, Replication study, Sentiment analysis tools},
	pages = {2543--2584},
}

@inproceedings{abeysinghe_lsu_2019,
	address = {New York, NY, USA},
	series = {{PEARC} '19},
	title = {{LSU} {Computational} {System} {Biology} {Gateway} for {Education}},
	isbn = {978-1-4503-7227-5},
	url = {https://doi.org/10.1145/3332186.3333259},
	doi = {10.1145/3332186.3333259},
	abstract = {Science gateways are a mechanism for delivering scientific software as a service, especially when the software requires high performance computing (HPC) resources to run effectively. The existence of a science gateway eliminates the user's need to learn to work with HPC systems and to manage software installations and updates. With well-designed user interfaces, users can more quickly become effective users of scientific applications and can manage information needed for replicating, modifying, and sharing results. All of these efficiency gains enable users to focus more on their research. In addition, science gateways are being identified as an effective educational tool, a tool to be used in classroom environments as a method to get students quickly into research on domain specific questions. In the absence of a science gateway, students are likely to need a considerable time to learn to work with HPC systems, and any time spent on such will reduce their time on the actual science. This poster presents how the Louisiana State University (LSU) gateway for the Computational System Biology Group (CSBG) - (www.brylinski.org) was updated and improved to be a classroom teaching tool. This work makes extensive use of Apache Airavata's group management capabilities.},
	booktitle = {Practice and {Experience} in {Advanced} {Research} {Computing} 2019: {Rise} of the {Machines} ({Learning})},
	publisher = {Association for Computing Machinery},
	author = {Abeysinghe, Eroma and Brylinski, Michal and Christie, Marcus and Marru, Suresh and Pierce, Marlon},
	year = {2019},
	note = {event-place: Chicago, IL, USA},
	keywords = {Bioinformatics, Apache Airavata, Education, Science Gateway},
}

@article{fantozzi_tape_2017,
	title = {Tape music archives: from preservation to access},
	volume = {18},
	issn = {1432-5012},
	url = {https://doi.org/10.1007/s00799-017-0208-8},
	doi = {10.1007/s00799-017-0208-8},
	abstract = {This article presents a methodology for the active preservation of, and the access to, magnetic tapes of audio archives. The methodology has been defined and implemented by a multidisciplinary team involving engineers as well as musicians, composers and archivists. The strong point of the methodology is the philological awareness that influenced the development of digital tools, which consider the critical questions in the historian and musicologist's approach: the secondary information and the history of transmission of an audio document.},
	number = {3},
	journal = {Int. J. Digit. Libr.},
	author = {Fantozzi, Carlo and Bressan, Federica and Pretto, Niccolò and Canazza, Sergio},
	month = sep,
	year = {2017},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {A/D transfer, Audio collections, Cultural memory, Metadata, Tape music},
	pages = {233--249},
}

@article{wojciechowski_assisstive_2017,
	title = {Assisstive technology application for enhancing social and language skills of young children with autism},
	volume = {76},
	issn = {1380-7501},
	url = {https://doi.org/10.1007/s11042-016-3995-9},
	doi = {10.1007/s11042-016-3995-9},
	abstract = {In this paper we present an assistive system designed for supporting young children affected by autism in their process of learning pronunciation and meaning of new words. The system is built-up of a mobile application and objects identifiers which in our case were Estimote Beacon sensors. The system requires active participation of a parent who selects words to learn, records pronunciation of object names, selects illustrations, and activates and turns off the application. The entire process is designed to extend parents' care and to support autistic children with an instant repetition of pronounced object's names when those items are met during playing or moving around the house. An experimental part of our project consists of a report where we compare collected results of two autistic children using our application installed on a smart watch and on a smart phone. In both reported cases autistic children made a visible progress in speed of learning new words when compared to an equivalent period of time without assistive application support.},
	number = {4},
	journal = {Multimedia Tools Appl.},
	author = {Wojciechowski, Adam and Al-Musawi, Raed},
	month = feb,
	year = {2017},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Assisstive technology, Autism, Experimental validation, Mobile system, Multimedia},
	pages = {5419--5439},
}

@article{kelley_obtaining_2017,
	title = {Obtaining and {Managing} {Answer} {Quality} for {Online} {Data}-{Intensive} {Services}},
	volume = {2},
	issn = {2376-3639},
	url = {https://doi.org/10.1145/3055280},
	doi = {10.1145/3055280},
	abstract = {Online data-intensive (OLDI) services use anytime algorithms to compute over large amounts of data and respond quickly. Interactive response times are a priority, so OLDI services parallelize query execution across distributed software components and return best effort answers based on the data so far processed. Omitted data from slow components could lead to better answers, but tracing online how much better the answers could be is difficult. We propose Ubora, a design approach to measure the effect of slow-running components on the quality of answers. Ubora randomly samples online queries and executes them a second time. The first online execution omits data from slow components and provides interactive answers. The second execution uses mature results from intermediate components completed after the online execution finishes. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of services, including Hadoop/Yarn, Apache Lucene, the EasyRec Recommendation Engine, and the OpenEphyra question-answering system. Ubora computes answer quality with more mature executions per second than competing approaches that do not use memoization. With Ubora, we show that answer quality is effective at guiding online admission control. While achieving the same answer quality on high-priority queries, our adaptive controller had 55\% higher peak throughput on low-priority queries than a competing controller guided by the rate of timeouts.},
	number = {2},
	journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
	author = {Kelley, Jaimie and Stewart, Christopher and Morris, Nathaniel and Tiwari, Devesh and He, Yuxiong and Elnikety, Sameh},
	month = apr,
	year = {2017},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Answer quality, big data, services},
}

@inproceedings{lehoczky_high-level_2018,
	address = {New York, NY, USA},
	series = {{CoNGA} '18},
	title = {High-level .{NET} software implementations of unum type {I} and posit with simultaneous {FPGA} implementation using {Hastlayer}},
	isbn = {978-1-4503-6414-0},
	url = {https://doi.org/10.1145/3190339.3190343},
	doi = {10.1145/3190339.3190343},
	abstract = {The unum arithmetic framework has been proposed by Gustafson, D. J. to address the short-comings of the IEEE 754 Standard's floating-point. In this paper, we present our software and hardware implementations of Type I and posit unums. The software implementation is built on the .NET platform as an open source library written in the C\# programming language. We automatically create hardware implementations using our .NET to FPGA converter tool called Hastlayer. The amount of hardware resources needed for addition operations are quantified, and the performance of software and prototype hardware for posits are compared. We show that posits are significantly more hardware friendly than Type I unums. Furthermore, our posit FPGA implementation is about 2.04 times more efficient per clock cycle than its software implementation.},
	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic}},
	publisher = {Association for Computing Machinery},
	author = {Lehóczky, Zoltán and Retzler, András and Tóth, Richárd and Szabó, Álmos and Farkas, Benedek and Somogyi, Krisztián},
	year = {2018},
	note = {event-place: Singapore, Singapore},
	keywords = {floating-point airthmetic, FPGA, high-level synthesis, NET, posit, unum},
}

@article{getchell_network_2016,
	title = {A network analysis of official {Twitter} accounts during the {West} {Virginia} water crisis},
	volume = {54},
	issn = {0747-5632},
	url = {https://doi.org/10.1016/j.chb.2015.06.044},
	doi = {10.1016/j.chb.2015.06.044},
	abstract = {Online networks using Web 2.0 technologies have proven useful for communication among all parties involved in managing crises. These networks rapidly disseminate information allowing for coordination among organizations responding to the needs of those whose safety and wellbeing are threatened by the crisis and its aftermath. This study provides a network analysis of official Twitter accounts activated during the Charleston, West Virginia, water contamination crisis in 2014. The city's water supply was rendered unfit for drinking or bathing after 7500 gallons of a toxic chemical leaked into the Elk River. The network created by the 41 Twitter accounts associated with the West Virginia water contamination lacked density, contained several isolates, exchanged information quickly (geodesic distance diameter), and contained both national and local accounts. The lack of density indicates limited exchange of information, particularly between national and federal accounts. The rapid dissemination of the information that was shared and the fact that some accounts did bridge the local and national gap, however, show the positive potential for such networks in responding to crises. We analyzed a Twitter network of 41 accounts related to the W.V. water crisis.We examined the network in the context of rapid dissemination of info in a crisis.The networks density is lower than ideal from a crisis communication perspective.There are 2 factions: national level organizations and W.V. organizations.},
	number = {C},
	journal = {Comput. Hum. Behav.},
	author = {Getchell, Morgan C. and Sellnow, Timothy L.},
	month = jan,
	year = {2016},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Crisis communication, Network analysis, Risk communication, Social media},
	pages = {597--606},
}

@article{aragon_virtual_2014,
	title = {{VIRTUAL} {OFFICE}: {A} {Web} {Platform} for a {Collaborative} {Networked} {Organization}},
	volume = {6},
	issn = {1938-0194},
	url = {https://doi.org/10.4018/IJWP.2014100101},
	doi = {10.4018/IJWP.2014100101},
	abstract = {The integration of functions in business networks requires a high level of integration of the information processes. Based on file transfers, these networks respond to the requirements of collaborative processes. There is an effective need for a strategy of integration among the members of the network. In the ICT era, the collaborative company needs to reach and maintain agility in the dynamics of their collaborative processes. Within the frame of a collaborative network, the development of a web platform permits the growth of an area that integrates collaborative processes, in which several companies participate, each supplying their own data. The Fruit-and-Vegetable Collaborative Network studied in this paper is formed by producers, processers, packaging companies, marketers, transporters, and distributors. It has been developed via a web platform Virtual Office, which allows the network to carry out processes in a collaborative way, and helps the network in its process of confidence-building and in the interactions among its members.},
	number = {4},
	journal = {Int. J. Web Portals},
	author = {Aragon, Maria Victoria de la Fuente and Ros-McDonnell, Lorenzo},
	month = oct,
	year = {2014},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Collaborative Network, Exchange of Information and Knowledge, Information Flows, Information Processes, Interoperability, Web Platform},
	pages = {1--17},
}

@article{bhadra_pipeline_2017,
	title = {Pipeline for inferring protein function from dynamics using coarse-grained molecular mechanics forcefield},
	volume = {83},
	issn = {0010-4825},
	url = {https://doi.org/10.1016/j.compbiomed.2017.02.009},
	doi = {10.1016/j.compbiomed.2017.02.009},
	abstract = {Dynamics is integral to the function of proteins, yet the use of molecular dynamics (MD) simulation as a technique remains under-explored for molecular function inference. This is more important in the context of genomics projects where novel proteins are determined with limited evolutionary information. Recently we developed a method to match the query protein's flexible segments to infer function using a novel approach combining analysis of residue fluctuation-graphs and auto-correlation vectors derived from coarse-grained (CG) MD trajectory. The method was validated on a diverse dataset with sequence identity between proteins as low as 3\%, with high function-recall rates. Here we share its implementation as a publicly accessible web service, named DynFunc (Dynamics Match for Function) to query protein function from 1 s long CG dynamics trajectory information of protein subunits. Users are provided with the custom-developed coarse-grained molecular mechanics (CGMM) forcefield to generate the MD trajectories for their protein of interest. On upload of trajectory information, the DynFunc web server identifies specific flexible regions of the protein linked to putative molecular function. Our unique application does not use evolutionary information to infer molecular function from MD information and can, therefore, work for all proteins, including moonlighting and the novel ones, whenever structural information is available. Our pipeline is expected to be of utility to all structural biologists working with novel proteins and interested in moonlighting functions. Display Omitted Method to infer function of any proteins, including divergent and moonlighting ones.Inference of protein function without recourse to evolutionary information.Webserver to provide forcefield potential function/procedure to execute simulation.Webserver to query dynamics information archived in database to infer function.Novel approach via autocorrelation vectors to match segments with similar dynamics.},
	number = {C},
	journal = {Comput. Biol. Med.},
	author = {Bhadra, Pratiti and Pal, Debnath},
	month = apr,
	year = {2017},
	note = {Place: USA
Publisher: Pergamon Press, Inc.},
	keywords = {Database, Dynamics, Forcefield, Molecular function, Simulation},
	pages = {134--142},
}

@article{papadopoulos_modeling_2016,
	title = {Modeling {Place}: {Usage} of {Mobile} {Data} {Services} and {Applications} within {Different} {Places}},
	volume = {8},
	issn = {1941-627X},
	url = {https://doi.org/10.4018/IJESMA.2016040101},
	doi = {10.4018/IJESMA.2016040101},
	abstract = {Recent research has shown how new mobile networked environments have added new complexity to the notion of "place", now considered as an important concept in the context of technology adoption and the usage of mobile technologies, applications and services. The aim of this paper is to propose a new framework to gain insight into the usage behaviour of mobile data services and applications. A model of place was generated, adopting the humanistic geographical perspective as represented by the four "layers of place" introduced by Tuan's theory, which can be served as a sensitizing device in order to interpret and analyse the collected data. The model has been applied on an observational field study which tried to explore, understand and highlight the role of place in the decision of the people to use mobile data services. The model assisted researchers to show that the end users' decision to start using mobile data services has been influenced and triggered by their experience of place. This highlights the value of the proposed new framework of understanding users' interaction with their surrounding environment when adopting new technologies and using mobile data services.},
	number = {2},
	journal = {Int. J. E-Services Mob. Appl.},
	author = {Papadopoulos, Homer},
	month = apr,
	year = {2016},
	note = {Place: USA
Publisher: IGI Global},
	keywords = {Mobile Applications, Mobile Data Services, Model of Place, Place, Usage},
	pages = {1--20},
}

@book{mclamb_keeping_2015,
	address = {USA},
	title = {Keeping {Religious} {Institutions} {Secure}},
	isbn = {978-0-12-801488-2},
	abstract = {Keeping Religious Institutions Secure explores the unique vulnerabilities that churches, synagogues, and mosques face in regards to security, making them attractive to criminals who see them as easy targets. The text illustrates why all places of worship should think about security and the types of breaches that can drive people away. The book focuses on the most frequent security concerns experienced by houses of worship, including embezzlement, vandalism, assault, hate crime, and in rare cases, an active shooter-and how to help prevent them from occurring. Beginning with an overview of the basic security concepts and principles that can enhance the security of any religious facility, it then delves deeply into the particular security concerns of houses of worship, including the use of volunteers, protecting religious leaders, ensuring safety for children and teens, interacting with local law enforcement, handling the media, and much more. Covers security best practices that are adaptable to any type of religious institution. Addresses the key security measures-physical, electronic, environmental, and procedural-for protecting people and facilities. Includes guidance on identifying threats and vulnerabilities and instituting countermeasures for deterring crime and violence. Table of Contents Chapter 1: Introduction Chapter 2: Examining Typical Crimes Chapter 3: Understanding Basic Security Principles Chapter 4: Evaluating Risk Chapter 5: Utilizing the Total Environment Chapter 6: Choosing Physical Security Measures Chapter 7: Developing Policies and Procedures Chapter 8: Identifying and Handling At-Risk People Chapter 9: Reacting to an Event in Progress Chapter 10: Responding to an Active Shooter Chapter 11: Recovering from an Incident Chapter 12: Handling the Media Chapter 13: Recognizing Intangible Assets and Liability Concerns Chapter 14: Increasing Security Awareness Chapter 15: Implementing and Training Chapter 16: Keeping the Principal Safe Chapter 17: Assessing the Need for Less-Lethal Weapons and Firearms Chapter 18: Establishing Security Partnerships Chapter 19: Protecting Children and Youth Chapter 20: Conclusion and Summary},
	publisher = {Butterworth-Heinemann},
	author = {McLamb, Jennie-Leigh},
	year = {2015},
}

@article{veredas_web-based_2014,
	title = {A web-based e-learning application for wound diagnosis and treatment},
	volume = {116},
	issn = {0169-2607},
	url = {https://doi.org/10.1016/j.cmpb.2014.06.005},
	doi = {10.1016/j.cmpb.2014.06.005},
	abstract = {Pressure ulcers (PrU) are considered as one of the most challenging problems that Nursing professionals have to deal with in their daily practice. Nowadays, the education on PrUs is mainly based on traditional lecturing, seminars and face-to-face instruction, sometimes with the support of photographs of wounds being used as teaching material. This traditional educational methodology suffers from some important limitations, which could affect the efficacy of the learning process. This current study has been designed to introduce information and communication technologies (ICT) in the education on PrU for undergraduate students, with the main objective of evaluating the advantages an disadvantages of using ICT, by comparing the learning results obtained from using an e-learning tool with those from a traditional teaching methodology. In order to meet this major objective, a web-based learning system named ePULab has been designed and developed as an adaptive e-learning tool for the autonomous acquisition of knowledge on PrU evaluation. This innovative system has been validated by means of a randomized controlled trial that compares its learning efficacy with that from a control group receiving a traditional face-to-face instruction. Students using ePULab gave significantly better (p \&lt; 0.01) learning acquisition scores (from pre-test mean 8.27 (SD 1.39) to post-test mean 15.83 (SD 2.52)) than those following traditional lecture-style classes (from pre-test mean 8.23 (SD 1.23) to post-test mean 11.6 (SD 2.52)). In this article, the ePULab software is described in detail and the results from that experimental educational validation study are also presented and analyzed.},
	number = {3},
	journal = {Comput. Methods Prog. Biomed.},
	author = {Veredas, Francisco J. and Ruiz-Bandera, Esperanza and Villa-Estrada, Francisca and Rufino-González, Juan F. and Morente, Laura},
	month = oct,
	year = {2014},
	note = {Place: USA
Publisher: Elsevier North-Holland, Inc.},
	keywords = {Adaptive tutorial systems, Computational intelligence, E-learning, Image processing, Nursing informatics, Pressure ulcer},
	pages = {236--248},
}

@article{falomir_automation_2012,
	title = {Automation of {Food} {Questionnaires} in {Medical} {Studies}},
	volume = {42},
	issn = {0010-4825},
	url = {https://doi.org/10.1016/j.compbiomed.2012.07.008},
	doi = {10.1016/j.compbiomed.2012.07.008},
	abstract = {Applications for automating the most commonly used dietary surveys in nutritional research, Food Frequency Questionnaires (FFQs) and 24h Dietary Recalls (24HDRs), are reviewed in this paper. A comprehensive search of electronic databases was carried out and findings were classified by a group of experts in nutrition and computer science into: (i) Computerized Questionnaires and Web-based Questionnaires; (ii) FFQs and 24HDRs and combinations of both; and (iii) interviewer-administered or self-administered questionnaires. A discussion on the classification made and the works reported is included. Finally, works that apply innovative technologies are outlined and the future trends for automating questionnaires in nutrition are identified. Graphical abstractDisplay Omitted Highlights Automation of FFQs and 24HDRs saves costs and speeds nutritional studies. Two kinds of automation succeeded in last years: Web-based and computerized. Web-based FFQs and 24HDRs provide more advantages than those computer-based. New trends involve automating FFQs and 24HDRs using PDAs and mobile phones. Studies that use and test these new technological trends are now emerging.},
	number = {10},
	journal = {Comput. Biol. Med.},
	author = {Falomir, Zoe and Arregui, María and Madueño, Francisco and Corella, Dolores and Coltell, Oscar},
	month = oct,
	year = {2012},
	note = {Place: USA
Publisher: Pergamon Press, Inc.},
	keywords = {24h dietary recall (24HDR), Engineering technologies, Food composition database, Food frequency questionnaire (FFQ), World wide web (WWW)},
	pages = {964--974},
}

@article{craige_conceptual_2016,
	title = {Conceptual design of a biofeedstock supply chain model for eastern redcedar},
	volume = {121},
	issn = {0168-1699},
	url = {https://doi.org/10.1016/j.compag.2015.11.019},
	doi = {10.1016/j.compag.2015.11.019},
	abstract = {Creation of an online, techno-economic supply chain model.Use of web-based GIS location-allocation, mapping, and service area generation.Stochastic estimate of transport and harvest costs using Monte Carlo analysis.Modular interface and database design for incorporation of additional biofeedstocks. Modeling a biomass supply chain in its entirety, i.e. location selection, harvest, transport, processing, and refining costs, is essential to determining the economic feasibility of a production strategy. Eastern redcedar (Juniperus virginiana) has been proposed as a viable biofeedstock in Oklahoma because multiple products may be manufactured from the tree including biofuel, cedar oil, pharmaceuticals, mulch, and lumber products. To facilitate development of eastern redcedar commerce a comprehensive, modular, web-based supply chain model was developed as a computational tool to evaluate biofeedstock markets. Geospatial programming is used to perform location allocation, develop service areas, and biomass yield maps. User input data was employed to approximate costs at each node in the supply chain, while Monte Carlo and one-way sensitivity analysis were used to quantify uncertainty. The model is focused on fully utilizing redcedar by enabling users to evaluate the economic feasibility of producing multiple end products simultaneously. The dynamic and modular framework of the model provides a strong foundation for expanding the model to other biomass feedstocks.},
	number = {C},
	journal = {Comput. Electron. Agric.},
	author = {Craige, C.C. and Buser, M.D. and Frazier, R.S. and Hiziroglu, S.S. and Holcomb, R.B. and Huhnke, R.L.},
	month = feb,
	year = {2016},
	note = {Place: NLD
Publisher: Elsevier Science Publishers B. V.},
	keywords = {Modeling, Biofuels, Bioproducts, Economics, GIS, Supply chain},
	pages = {12--24},
}

@article{sojo_ivichem_2012,
	title = {{IVIChem}: {An} integrative web environment for computational chemistry},
	volume = {12},
	issn = {1472-7978},
	abstract = {IVIChem is an integrative web environment for computational chemistry that can be accessed globally from any typical computer via the World Wide Web. It assists users all along the research pipeline by facilitating the introduction of molecular geometries, specification of options for a calculation, assembling input files, submission to the calculation queue, monitoring the status of calculations, and graphical analysis of results. IVIChem's graphical user interfaces considerably improve the usability of computational chemistry software in different areas of chemical modeling, and they enhance the performance of researchers by automating the analysis of results and the addition of further calculation packages.},
	number = {4–6},
	journal = {J. Comp. Methods in Sci. and Eng.},
	author = {Sojo, Víctor and Peraza, Alexander and Ruette, Fernando and Sánchez, Morella and Acosta, A. Eleonora},
	month = oct,
	year = {2012},
	note = {Place: NLD
Publisher: IOS Press},
	keywords = {Agilus, Cativic, Computational Chemistry, Gui, Ivichem, Mopac, Usability, User Interfaces},
	pages = {397--406},
}

@article{esmaeilzadeh_looking_2012,
	title = {Looking back and looking forward: power, performance, and upheaval},
	volume = {55},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2209249.2209272},
	doi = {10.1145/2209249.2209272},
	abstract = {The past 10 years have delivered two significant revolutions. (1) Microprocessor design has been transformed by the limits of chip power, wire latency, and Dennard scaling—leading to multicore processors and heterogeneity. (2) Managed languages and an entirely new software landscape emerged—revolutionizing how software is deployed, is sold, and interacts with hardware. Researchers most often examine these changes in isolation. Architects mostly grapple with microarchitecture design through the narrow software context of native sequential SPEC CPU benchmarks, while language researchers mostly consider microarchitecture in terms of performance alone. This work explores the clash of these two revolutions over the past decade by measuring power, performance, energy, and scaling, and considers what the results may mean for the future. Our diverse findings include the following: (a) native sequential workloads do not approximate managed workloads or even native parallel workloads; (b) diverse application power profiles suggest that future applications and system software will need to participate in power optimization and management; and (c) software and hardware researchers need access to real measurements to optimize for power and energy.},
	number = {7},
	journal = {Commun. ACM},
	author = {Esmaeilzadeh, Hadi and Cao, Ting and Yang, Xi and Blackburn, Stephen M. and McKinley, Kathryn S.},
	month = jul,
	year = {2012},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {105--114},
}

@article{udayakumar_receptor-based_2013,
	title = {Receptor-based pharmacophore tool for design and development of next-generation drugs},
	volume = {9},
	issn = {1744-5485},
	url = {https://doi.org/10.1504/IJBRA.2013.056076},
	doi = {10.1504/IJBRA.2013.056076},
	abstract = {Drug discovery is an intricate process in which new drugs are designed or discovered. A pharmacophore is an essential ensemble of steric and electronic features for drug discovery, which is necessary to ensure optimal interactions with a specific target structure and to trigger its biological response. Here we present our innovative approach: the Receptor-Based Pharmacophoric Tool RBPT, a transparent user friendly GUI sphere that generates pharmacophores using the structure of the target protein at the specified binding site. The pharmacophore generated for the specified binding site of a target can be used in virtual screening and for further studies.},
	number = {5},
	journal = {Int. J. Bioinformatics Res. Appl.},
	author = {Udayakumar, M. and Kumar, Pupala Suresh and Hemavathi, K. and Shanmugapriya, P. and Seenivasagam, R.},
	month = aug,
	year = {2013},
	note = {Place: Geneva 15, CHE
Publisher: Inderscience Publishers},
	pages = {487--516},
}

@inproceedings{lu_deep_2013,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'13},
	title = {A deep architecture for matching short texts},
	abstract = {Many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents, etc.). The matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema, although proven successful on a range of matching tasks, is insufficient for capturing the rich structure in the matching process of more complicated objects. In this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More specifically, we apply this model to matching tasks in natural language, e.g., finding sensible responses for a tweet, or relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Lu, Zhengdong and Li, Hang},
	year = {2013},
	note = {event-place: Lake Tahoe, Nevada},
	pages = {1367--1375},
}

@book{noauthor_siet_2022,
	address = {New York, NY, USA},
	title = {{SIET} '22: {Proceedings} of the 7th {International} {Conference} on {Sustainable} {Information} {Engineering} and {Technology}},
	isbn = {978-1-4503-9711-7},
	publisher = {Association for Computing Machinery},
	year = {2022},
}

@article{pintore_efficient_2021,
	title = {Efficient computation of bifurcation diagrams with a deflated approach to reduced basis spectral element method},
	volume = {47},
	issn = {1019-7168},
	url = {https://doi.org/10.1007/s10444-020-09827-6},
	doi = {10.1007/s10444-020-09827-6},
	abstract = {The majority of the most common physical phenomena can be described using partial differential equations (PDEs). However, they are very often characterized by strong nonlinearities. Such features lead to the coexistence of multiple solutions studied by the bifurcation theory. Unfortunately, in practical scenarios, one has to exploit numerical methods to compute the solutions of systems of PDEs, even if the classical techniques are usually able to compute only a single solution for any value of a parameter when more branches exist. In this work, we implemented an elaborated deflated continuation method that relies on the spectral element method (SEM) and on the reduced basis (RB) one to efficiently compute bifurcation diagrams with more parameters and more bifurcation points. The deflated continuation method can be obtained combining the classical continuation method and the deflation one: the former is used to entirely track each known branch of the diagram, while the latter is exploited to discover the new ones. Finally, when more than one parameter is considered, the efficiency of the computation is ensured by the fact that the diagrams can be computed during the online phase while, during the offline one, one only has to compute one-dimensional diagrams. In this work, after a more detailed description of the method, we will show the results that can be obtained using it to compute a bifurcation diagram associated with a problem governed by the Navier-Stokes equations.},
	number = {1},
	journal = {Adv. Comput. Math.},
	author = {Pintore, Moreno and Pichi, Federico and Hess, Martin and Rozza, Gianluigi and Canuto, Claudio},
	month = feb,
	year = {2021},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {35B60, 35Q35, 37M20, 76E30, 76M22, Bifurcation diagram, Deflated continuation method, Reduced basis method, Reduced order model, Spectral element method, Steady bifurcations},
}

@article{rio_php_2023-1,
	title = {{PHP} code smells in web apps: {Evolution}, survival and anomalies},
	volume = {200},
	issn = {0164-1212},
	shorttitle = {{PHP} code smells in web apps},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223000390},
	doi = {10.1016/j.jss.2023.111644},
	abstract = {Context:
Code smells are symptoms of poor design, leading to future problems, such as reduced maintainability. Therefore, it becomes necessary to understand their evolution and how long they stay in code. This paper presents a longitudinal study on the evolution and survival of code smells (CS) for web apps built with PHP, the most widely used server-side programming language in web development and seldom studied.
Objectives:
We aimed to discover how CS evolve and what is their survival/lifespan in typical PHP web apps. Does CS survival depend on their scope or app life period? Are there sudden variations (anomalies) in the density of CS through the evolution of web apps?
Method:
We analyzed the evolution of 18 CS in 12 PHP web applications and compared it with changes in app and team size. We characterized the distribution of CS and used survival analysis techniques to study CS’ lifespan. We specialized the survival studies into localized (specific location) and scattered CS (spanning multiple classes/methods) categories. We further split the observations for each web app into two consecutive time frames. As for the CS evolution anomalies, we standardized their detection criteria.
Results:
The CS density trend along the evolution of PHP web apps is mostly stable, with variations, and correlates with the developer’s numbers. We identified the smells that survived the most. CS live an average of about 37\% of the life of the applications, almost 4 years on average in our study; around 61\% of CS introduced are removed. Most applications have different survival times for localized and scattered CS, and localized CS have a shorter life. The CS survival time is shorter and more CS are introduced and removed in the first half of the life of the applications. We found anomalies in the evolution of 5 apps and show how a graphical representation of sudden variations found in the evolution of CS unveils the story of a development project.
Conclusion:
CS stay a long time in code. The removal rate is low and did not change substantially in recent years. An effort should be made to avoid this bad behavior and change the CS density trend to decrease.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Rio, Américo and Brito e Abreu, Fernando},
	month = jun,
	year = {2023},
	keywords = {Code smells, PHP, Software evolution, Survival, Web apps},
	pages = {111644},
	file = {Texto completo:files/1233/Rio e Brito e Abreu - 2023 - PHP code smells in web apps Evolution, survival a.pdf:application/pdf},
}

@article{mumtaz_systematic_2021,
	title = {A systematic mapping study on architectural smells detection},
	volume = {173},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220302752},
	doi = {10.1016/j.jss.2020.110885},
	abstract = {The recognition of the need for high-quality software architecture is evident from the increasing trend in investigating architectural smells. Detection of architectural smells is paramount because they can seep through to design and implementation stages if left unidentified. Many architectural smells detection techniques and tools are proposed in the literature. The diversity in the detection techniques and tools suggests the need for their collective analysis to identify interesting aspects for practice and open research areas. To fulfill this, in this paper, we unify the knowledge about the detection of architectural smells through a systematic mapping study. We report on the existing detection techniques and tools for architectural smells to identify their limitations. We find there has been limited investigation of some architectural smells (e.g., micro-service smells); many architectural smells are not detected by tools yet; and there are limited empirical validations of techniques and tools. Based on our findings, we suggest several open research problems, including the need to (1) investigate undetected architectural smells (e.g., Java package smells), (2) improve the coverage of architectural smell detection across architecture styles (e.g., service-oriented and cloud), and (3) perform empirical validations of techniques and tools in industry across different languages and project domains.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Mumtaz, Haris and Singh, Paramvir and Blincoe, Kelly},
	month = mar,
	year = {2021},
	keywords = {Antipatterns, Systematic mapping study, Architectural debt, Architectural smells, Smell detection techniques},
	pages = {110885},
}

@article{singjai_practitioners_2021,
	title = {On the practitioners’ understanding of coupling smells — {A} grey literature based {Grounded}-{Theory} study},
	volume = {134},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584921000264},
	doi = {10.1016/j.infsof.2021.106539},
	abstract = {Context:
Code and design smells, such as the coupling smells examined in this article, are widely studied. Existing empirical studies reveal gaps between the scientific theory and practice, not yet explained by the scientific literature. Only basic coupling smell detection approaches and metrics seem to have been transferred to practice so far.
Objective:
This article aims to study the current practitioner’s understanding of coupling smells.
Method:
Based on grey literature sources containing practitioner views on coupling smells, we performed a Grounded Theory (GT) study. We used UML-based modeling to precisely encode our findings and performed a rigorous analysis of our codes and models.
Results:
Our results are defining factors of coupling smells, as well as smell impacts, trade-offs, relationships to other smells, relationships to practices and patterns, and fix options as perceived by practitioners. We further identified gaps in the understanding of coupling smells between science and practice, and derived opportunities and challenges for future scientific work.
Conclusions:
Five lessons are presented as opportunities and challenges for future research. Our results can help scientists to get a better understanding of practitioner concerns, and practitioners to get an overview of the current perception of other practitioners on coupling smells.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Singjai, Apitchaka and Simhandl, Georg and Zdun, Uwe},
	month = jun,
	year = {2021},
	keywords = {Code smells, Code quality, Coupling smells, Design smells, Grey literature, Grounded theory, Software design quality},
	pages = {106539},
	file = {Versão submetida:files/1234/Singjai et al. - 2021 - On the practitioners’ understanding of coupling sm.pdf:application/pdf},
}

@article{garousi_smells_2018,
	title = {Smells in software test code: {A} survey of knowledge in industry and academia},
	volume = {138},
	issn = {0164-1212},
	shorttitle = {Smells in software test code},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121217303060},
	doi = {10.1016/j.jss.2017.12.013},
	abstract = {As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal’ literature mapping (classification) on both the scientific literature and also practitioners’ grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Garousi, Vahid and Küçük, Barış},
	month = apr,
	year = {2018},
	keywords = {Survey, Software testing, Automated testing, Test automation, Multivocal literature mapping, Systematic mapping, Test anti-patterns, Test scripts, Test smells},
	pages = {52--81},
}

@article{singh_systematic_2018,
	title = {A systematic literature review: {Refactoring} for disclosing code smells in object oriented software},
	volume = {9},
	issn = {2090-4479},
	shorttitle = {A systematic literature review},
	url = {https://www.sciencedirect.com/science/article/pii/S2090447917300412},
	doi = {10.1016/j.asej.2017.03.002},
	abstract = {Context
Reusing a design pattern is not always in the favor of developers. Thus, the code starts smelling. The presence of “Code Smells” leads to more difficulties for the developers. This racket of code smells is sometimes called Anti-Patterns.
Objective
The paper aimed at a systematic literature review of refactoring with respect to code smells. However the review of refactoring is done in general and the identification of code smells and anti-patterns is performed in depth.
Method
A systematic literature survey has been performed on 238 research items that includes articles from leading Conferences, Workshops and premier journals, theses of researchers and book chapters.
Results
Several data sets and tools for performing refactoring have been revealed under the specified research questions.
Conclusion
The work done in the paper is an addition to prior systematic literature surveys. With the study of paper the attentiveness of readers about code smells and anti-patterns will be enhanced.},
	number = {4},
	urldate = {2024-08-06},
	journal = {Ain Shams Engineering Journal},
	author = {Singh, Satwinder and Kaur, Sharanpreet},
	month = dec,
	year = {2018},
	keywords = {Code smells, Refactoring, Anti-patterns},
	pages = {2129--2151},
}

@article{yamashita_what_2013-1,
	title = {To what extent can maintenance problems be predicted by code smell detection? – {An} empirical study},
	volume = {55},
	issn = {0950-5849},
	shorttitle = {To what extent can maintenance problems be predicted by code smell detection?},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584913001614},
	doi = {10.1016/j.infsof.2013.08.002},
	abstract = {Context
Code smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution.
Objective
This study is aimed at a detailed investigation to which extent problems in maintenance projects can be predicted by the detection of currently known code smells.
Method
A multiple case study was conducted, in which the problems faced by six developers working on four different Java systems were registered on a daily basis, for a period up to four weeks. Where applicable, the files associated to the problems were registered. Code smells were detected in the pre-maintenance version of the systems, using the tools Borland Together and InCode. In-depth examination of quantitative and qualitative data was conducted to determine if the observed problems could be explained by the detected smells.
Results
From the total set of problems, roughly 30\% percent were related to files containing code smells. In addition, interaction effects were observed amongst code smells, and between code smells and other code characteristics, and these effects led to severe problems during maintenance. Code smell interactions were observed between collocated smells (i.e., in the same file), and between coupled smells (i.e., spread over multiple files that were coupled).
Conclusions
The role of code smells on the overall system maintainability is relatively minor, thus complementary approaches are needed to achieve more comprehensive assessments of maintainability. Moreover, to improve the explanatory power of code smells, interaction effects amongst collocated smells and coupled smells should be taken into account during analysis.},
	number = {12},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Yamashita, Aiko and Moonen, Leon},
	month = dec,
	year = {2013},
	keywords = {Code smells, Empirical study, Maintainability},
	pages = {2223--2242},
}

@article{chen_understanding_2018-1,
	title = {Understanding metric-based detectable smells in {Python} software: {A} comparative study},
	volume = {94},
	issn = {0950-5849},
	shorttitle = {Understanding metric-based detectable smells in {Python} software},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584916301690},
	doi = {10.1016/j.infsof.2017.09.011},
	abstract = {Context
Code smells are supposed to cause potential comprehension and maintenance problems in software development. Although code smells are studied in many languages, e.g. Java and C\#, there is a lack of technique or tool support addressing code smells in Python.
Objective
Due to the great differences between Python and static languages, the goal of this study is to define and detect code smells in Python programs and to explore the effects of Python smells on software maintainability.
Method
In this paper, we introduced ten code smells and established a metric-based detection method with three different filtering strategies to specify metric thresholds (Experience-Based Strategy, Statistics-Based Strategy, and Tuning Machine Strategy). Then, we performed a comparative study to investigate how three detection strategies perform in detecting Python smells and how these smells affect software maintainability with different detection strategies. This study utilized a corpus of 106 Python projects with most stars on GitHub.
Results
The results showed that: (1) the metric-based detection approach performs well in detecting Python smells and Tuning Machine Strategy achieves the best accuracy; (2) the three detection strategies discover some different smell occurrences, and Long Parameter List and Long Method are more prevalent than other smells; (3) several kinds of code smells are more significantly related to changes or faults in Python modules.
Conclusion
These findings reveal the key features of Python smells and also provide a guideline for the choice of detection strategy in detecting and analyzing Python smells.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Chen, Zhifei and Chen, Lin and Ma, Wanwangying and Zhou, Xiaoyu and Zhou, Yuming and Xu, Baowen},
	month = feb,
	year = {2018},
	keywords = {Code smell, Detection strategy, Python, Software maintainability},
	pages = {14--29},
}

@article{skipina_automatic_2024,
	title = {Automatic detection of {Feature} {Envy} and {Data} {Class} code smells using machine learning},
	volume = {243},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423033572},
	doi = {10.1016/j.eswa.2023.122855},
	abstract = {Code smells in software indicate poor design and implementation choices. Detecting and removing them is critical for sustainable software development. Machine learning (ML) can automate code smell detection. Most ML solutions train models from scratch on code smell datasets, using handcrafted source code metrics as features. Pretrained language models, like BERT, fueled a paradigm shift in natural language processing: from handcrafted features to automatically inferred features and from training models from scratch to using pretrained models. Code embeddings offer the potential to bring a similar paradigm shift to code analysis. Nevertheless, the potential of using pretrained neural code embeddings for code smell detection has yet to be fully explored. To this end, we evaluated ML models trained using different code representations: code metrics and state-of-the-art neural code embeddings (CodeT5 and CuBERT). We experimented with CodeT5 variants (base and small) and explored multiple ways of embedding code snippets (by combining line-level embeddings or passing the entire code snippet as input). We tested our approaches on the tasks of detecting Data Class and Feature Envy on the MLCQ dataset. Considering the results of this study and our previous research, performance-wise, there is no clear winner between using code metrics or code embeddings for different code smell types and programming languages. However, given that, in contrast to code metrics, code embeddings can automatically adapt to new programming constructs and are expected to scale better with dataset size, these models are likely to become the future state-of-the-art feature generation technique for code smell detection.},
	urldate = {2024-08-06},
	journal = {Expert Systems with Applications},
	author = {Škipina, Milica and Slivka, Jelena and Luburić, Nikola and Kovačević, Aleksandar},
	month = jun,
	year = {2024},
	keywords = {Software engineering, Machine learning, Code metrics, Code smell detection, Neural source code embeddings},
	pages = {122855},
	file = {Versão submetida:files/1235/Škipina et al. - 2024 - Automatic detection of Feature Envy and Data Class.pdf:application/pdf},
}

@article{tahir_large_2020,
	title = {A large scale study on how developers discuss code smells and anti-pattern in {Stack} {Exchange} sites},
	volume = {125},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584920300926},
	doi = {10.1016/j.infsof.2020.106333},
	abstract = {Context: In this paper, we investigate how developers discuss code smells and anti-patterns across three technical Stack Exchange sites. Understanding developers perceptions of these issues is important to inform and align future research efforts and direct tools vendors to design tailored tools that best suit developers. Method: we mined three Stack Exchange sites and used quantitative and qualitative methods to analyse more than 4000 posts that discuss code smells and anti-patterns.Results: results showed that developers often asked their peers to smell their code, thus utilising those sites as an informal, crowd-based code smell/anti-pattern detector. The majority of questions (556) asked were focused on smells like Duplicated Code, Spaghetti Code, God and Data Classes. In terms of languages, most of discussions centred around popular languages such as C\# (772 posts), JavaScript (720) and Java (699), however greater support is available for Java compared to other languages (especially modern languages such as Swift and Kotlin). We also found that developers often discuss the downsides of implementing specific design patterns and ‘flag’ them as potential anti-patterns to be avoided. Some well-defined smells and anti-patterns are discussed as potentially being acceptable practice in certain scenarios. In general, developers actively seek to consider trade-offs to decide whether to use a design pattern, an anti-pattern or not.Conclusion: our results suggest that there is a need for: 1) more context and domain sensitive evaluations of code smells and anti-patterns, 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry, and 3) a unified, constantly updated, catalog of smells and anti-patterns. We conjecture that the crowd-based detection approach considers contextual factors and thus tend to be more trusted by developers than automated detection tools.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Tahir, Amjed and Dietrich, Jens and Counsell, Steve and Licorish, Sherlock and Yamashita, Aiko},
	month = sep,
	year = {2020},
	keywords = {Code smells, Mining software repositories, Anti-patterns, Stack exchange},
	pages = {106333},
}

@article{yamashita_code_2013,
	title = {Code smells as system-level indicators of maintainability: {An} empirical study},
	volume = {86},
	issn = {0164-1212},
	shorttitle = {Code smells as system-level indicators of maintainability},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121213001258},
	doi = {10.1016/j.jss.2013.05.007},
	abstract = {Context
Code smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations.
Aim
The research in this paper investigates the potential of code smells to reflect system-level indicators of maintainability.
Method
We evaluated four medium-sized Java systems using code smells and compared the results against previous evaluations on the same systems based on expert judgment and the Chidamber and Kemerer suite of metrics. The systems were maintained over a period of up to 4 weeks. During maintenance, effort (person-hours) and number of defects were measured to validate the different evaluation approaches.
Results
Most code smells are strongly influenced by size; consequently code smells are not good indicators for comparing the maintainability of systems differing greatly in size. Also, from the comparison of the different evaluation approaches, expert judgment was found as the most accurate and flexible since it considered effects due to the system's size and complexity and could adapt to different maintenance scenarios.
Conclusion
Code smell approaches show promise as indicators of the need for maintenance in a way that other purely metric-based approaches lack.},
	number = {10},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Yamashita, Aiko and Counsell, Steve},
	month = oct,
	year = {2013},
	keywords = {Code smells, Empirical study, Maintainability, System evaluation},
	pages = {2639--2653},
	file = {Texto completo:files/1236/Yamashita e Counsell - 2013 - Code smells as system-level indicators of maintain.pdf:application/pdf},
}

@article{rubert_effects_2022-1,
	title = {On the effects of continuous delivery on code quality: {A} case study in industry},
	volume = {81},
	issn = {0920-5489},
	shorttitle = {On the effects of continuous delivery on code quality},
	url = {https://www.sciencedirect.com/science/article/pii/S0920548921000830},
	doi = {10.1016/j.csi.2021.103588},
	abstract = {Continuous delivery has been adopted by organizations to make software available to their users at any time. The transition from traditional software delivery methodologies to continuous delivery can impact on the results generated by organizations, e.g., the quality of source code and products. Although widely adopted, little is known about its effects. To account for this, this article reports a case study on the effects of continuous delivery on the quality of source code and products produced. Our case study was carried out for 12 months within a software development company in Brazil. Our findings indicate that the adoption of continuous delivery practices improved the quality of delivered products, mainly considering the number of defects reported by customers, the number of demands delivered per month, and user satisfaction. However, the adoption of continuous delivery did not favor the quality of source code, including the number of bugs, security vulnerabilities, code smells, duplicated code, and code complexity. Researchers and practitioners may benefit from our findings typically when delivering software products, designing and seeking to improve deployment pipeline practices. Finally, our study draws up some implications and shows the potential of adopting continuous delivery for developing enterprise applications that are constantly evolving.},
	urldate = {2024-08-06},
	journal = {Computer Standards \& Interfaces},
	author = {Rubert, Maluane and Farias, Kleinner},
	month = apr,
	year = {2022},
	keywords = {Software engineering, Continuous delivery, Enterprise resource planning, Software development},
	pages = {103588},
}

@article{kumara_s_2021,
	title = {The do’s and don’ts of infrastructure code: {A} systematic gray literature review},
	volume = {137},
	issn = {0950-5849},
	shorttitle = {The do’s and don’ts of infrastructure code},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584921000720},
	doi = {10.1016/j.infsof.2021.106593},
	abstract = {Context:
Infrastructure-as-code (IaC) is the DevOps tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual hardware configuration or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the relative scarcity of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Kumara, Indika and Garriga, Martín and Romeu, Angel Urbano and Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian Andrew and van den Heuvel, Willem-Jan},
	month = sep,
	year = {2021},
	keywords = {DevOps, Gray literature review, Infrastructure-as-code},
	pages = {106593},
	file = {Texto completo:files/1237/Kumara et al. - 2021 - The do’s and don’ts of infrastructure code A syst.pdf:application/pdf},
}

@article{mariani_systematic_2017-1,
	title = {A systematic review on search-based refactoring},
	volume = {83},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584916303779},
	doi = {10.1016/j.infsof.2016.11.009},
	abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest. Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities. Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation. Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler’s Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics. Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Mariani, Thainá and Vergilio, Silvia Regina},
	month = mar,
	year = {2017},
	keywords = {Refactoring, Evolutionary algorithms, Search-based software engineering},
	pages = {14--34},
}

@article{admiraal_deriving_2024,
	title = {Deriving modernity signatures of codebases with static analysis},
	volume = {211},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121224000165},
	doi = {10.1016/j.jss.2024.111973},
	abstract = {This paper addresses the problem of determining the modernity of software systems by analysing the use of new language features and their adoption over time. We propose the concept of modernity signatures to estimate the age of a codebase, naturally adjusted for maintenance practices, such that the modernity of a regularly updated system would be above that of a more recently created one which neglects current features and best practices. This can provide insights into coding practices, codebase health and the evolution of software languages. We present case studies on PHP and Python code, demonstrating the effectiveness of modernity signatures in determining the age of a codebase without executing the code or performing extensive human inspection. The paper describes the technical implementation details of generating the modernity signature for both of these languages, including the use of existing tools like the PHP parser and Vermin. The findings suggest that modernity signatures can aid developers in many ways from choosing whether to use a system or how to approach its maintenance, to assessing usefulness of a language feature, thus providing a valuable tool for source code analysis and manipulation.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Admiraal, Chris and van den Brink, Wouter and Gerhold, Marcus and Zaytsev, Vadim and Zubcu, Cristian},
	month = may,
	year = {2024},
	keywords = {Software evolution, Coupled evolution, Programming language adoption},
	pages = {111973},
}

@article{amanatidis_relation_2017,
	title = {The relation between technical debt and corrective maintenance in {PHP} web applications},
	volume = {90},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584916302026},
	doi = {10.1016/j.infsof.2017.05.004},
	abstract = {Context
Technical Debt Management (TDM) refers to activities that are performed to prevent the accumulation of Technical Debt (TD) in software. The state-of-research on TDM lacks empirical evidence on the relationship between the amount of TD in a software module and the interest that it accumulates. Considering the fact that in the last years, a large portion of software applications are deployed in the web, we focus this study on PHP applications.
Objective
Although the relation between debt amount and interest is well-defined in traditional economics (i.e., interest is proportional to the amount of debt), this relation has not yet been explored in the context of TD. To this end, the aim of this study is to investigate the relation between the amount of TD and the interest that has to be paid during corrective maintenance.
Method
To explore this relation, we performed a case study on 10 open source PHP projects. The obtained data have been analyzed to assess the relation between the amount of TD and two aspects of interest: (a) corrective maintenance (i.e., bug fixing) frequency, which translates to interest probability and (b) corrective maintenance effort which is related to interest amount.
Results
Both interest probability and interest amount are positively related with the amount of TD accumulated in a specific module. Moreover, the amount of TD is able to discriminate modules that are in need of heavy corrective maintenance.
Conclusions
The results of the study confirm the cornerstone of TD research, which suggests that modules with a higher level of incurred TD, are costlier in maintenance activities. In particular, such modules prove to be more defect-prone and consequently require more (corrective) maintenance effort.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Amanatidis, Theodoros and Chatzigeorgiou, Alexander and Ampatzoglou, Apostolos},
	month = oct,
	year = {2017},
	keywords = {PHP, Corrective maintenance, Technical debt, Case study, Empirical evidence, Interest},
	pages = {70--74},
	file = {Texto completo:files/1238/Amanatidis et al. - 2017 - The relation between technical debt and corrective.pdf:application/pdf},
}

@article{zakeri-nasrabadi_systematic_2023-1,
	title = {A systematic literature review on source code similarity measurement and clone detection: {Techniques}, applications, and challenges},
	volume = {204},
	issn = {0164-1212},
	shorttitle = {A systematic literature review on source code similarity measurement and clone detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223001917},
	doi = {10.1016/j.jss.2023.111796},
	abstract = {Measuring and evaluating source code similarity is a fundamental software engineering activity that embraces a broad range of applications, including but not limited to code recommendation, duplicate code, plagiarism, malware, and smell detection. This paper proposes a systematic literature review and meta-analysis on code similarity measurement and evaluation techniques to shed light on the existing approaches and their characteristics in different applications. We initially found over 10,000 articles by querying four digital libraries and ended up with 136 primary studies in the field. The studies were classified according to their methodology, programming languages, datasets, tools, and applications. A deep investigation reveals 80 software tools, working with eight different techniques on five application domains. Nearly 49\% of the tools work on Java programs and 37\% support C and C++, while there is no support for many programming languages. A noteworthy point was the existence of 12 datasets related to source code similarity measurement and duplicate codes, of which only eight datasets were publicly accessible. The lack of reliable datasets, empirical evaluations, hybrid methods, and focuses on multi-paradigm languages are the main challenges in the field. Emerging applications of code similarity measurement concentrate on the development phase in addition to the maintenance.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Zakeri-Nasrabadi, Morteza and Parsa, Saeed and Ramezani, Mohammad and Roy, Chanchal and Ekhtiarzadeh, Masoud},
	month = oct,
	year = {2023},
	keywords = {Code clone, Code recommendation, Plagiarism detection, Source code similarity, Systematic literature review},
	pages = {111796},
	file = {Versão submetida:files/1239/Zakeri-Nasrabadi et al. - 2023 - A systematic literature review on source code simi.pdf:application/pdf},
}

@article{jaafar_analyzing_2017-1,
	title = {Analyzing software evolution and quality by extracting {Asynchrony} change patterns},
	volume = {131},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121217300948},
	doi = {10.1016/j.jss.2017.05.047},
	abstract = {Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones. Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Jaafar, Fehmi and Lozano, Angela and Guéhéneuc, Yann-Gaël and Mens, Kim},
	month = sep,
	year = {2017},
	keywords = {Anti-patterns, Change patterns, Clones, Fault-proneness, Software quality},
	pages = {311--322},
}

@article{sharma_survey_2024,
	title = {A survey on machine learning techniques applied to source code},
	volume = {209},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
	doi = {10.1016/j.jss.2023.111934},
	abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Sharma, Tushar and Kechagia, Maria and Georgiou, Stefanos and Tiwari, Rohit and Vats, Indira and Moazen, Hadi and Sarro, Federica},
	month = mar,
	year = {2024},
	keywords = {Source code analysis, Tools, Deep learning, Datasets, Machine learning for software engineering},
	pages = {111934},
	file = {Texto completo:files/1240/Sharma et al. - 2024 - A survey on machine learning techniques applied to.pdf:application/pdf},
}

@article{de_stefano_impacts_2022-1,
	title = {Impacts of software community patterns on process and product: {An} empirical study},
	volume = {214},
	issn = {0167-6423},
	shorttitle = {Impacts of software community patterns on process and product},
	url = {https://www.sciencedirect.com/science/article/pii/S0167642321001246},
	doi = {10.1016/j.scico.2021.102731},
	abstract = {Software engineering projects are now more than ever a community effort. In the recent past, researchers have shown that their success not only depends on source code quality, but also on other aspects like the balance of power distance, culture, and global engineering practices, and more. In such a scenario, understanding the characteristics of the community around a project and foresee possible problems may be the key to develop successful systems. In this paper, we focus on this research problem and propose an exploratory study on the relation between community patterns, i.e.,recurrent mixes of organizational or social structure types, and aspects related to the quality of software products and processes by mining open-source software repositories hosted on GitHub. We first exploit association rule mining to discover frequent relations between community pattern and community smells, i.e.,sub-optimal patterns across the organizational structure of a software development community that may be precursors of some form of social debt. Further on, we use statistical analyses to understand their impact on software maintainability and on the community engagement, in terms of contributions and issues. Our findings show that different organizational patterns are connected to different forms of socio-technical problems; further on, specific combinations are set in equally specific contextual conditions. Findings support two possible conclusions: (1) practitioners should put in place specific preventive actions aimed at avoiding the emergence of community smells and (2) such actions should be drawn according to the contextual conditions of the organization and the project.},
	urldate = {2024-08-06},
	journal = {Science of Computer Programming},
	author = {De Stefano, Manuel and Iannone, Emanuele and Pecorelli, Fabiano and Tamburri, Damian Andrew},
	month = feb,
	year = {2022},
	keywords = {Empirical studies, Community patterns, Community smells},
	pages = {102731},
}

@article{alomar_preserving_2021-1,
	title = {On preserving the behavior in software refactoring: {A} systematic mapping study},
	volume = {140},
	issn = {0950-5849},
	shorttitle = {On preserving the behavior in software refactoring},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584921001348},
	doi = {10.1016/j.infsof.2021.106675},
	abstract = {Context:
Refactoring is the art of modifying the design of a system without altering its behavior. The idea is to reorganize variables, classes and methods to facilitate their future adaptations and comprehension. As the concept of behavior preservation is fundamental for refactoring, several studies, using formal verification, language transformation and dynamic analysis, have been proposed to monitor the execution of refactoring operations and their impact on the program semantics. However, there is no existing study that examines the available behavior preservation strategies for each refactoring operation.
Objective:
This paper identifies behavior preservation approaches in the research literature.
Method:
We conduct, in this paper, a systematic mapping study, to capture all existing behavior preservation approaches that we classify based on several criteria including their methodology, applicability, and their degree of automation.
Results:
The results indicate that several behavior preservation approaches have been proposed in the literature. The approaches vary between using formalisms and techniques, developing automatic refactoring safety tools, and performing a manual analysis of the source code.
Conclusion:
Our taxonomy reveals that there exist some types of refactoring operations whose behavior preservation is under-researched. Our classification also indicates that several possible strategies can be combined to better detect any violation of the program semantics.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali},
	month = dec,
	year = {2021},
	keywords = {Refactoring, Systematic mapping study, Behavior preservation},
	pages = {106675},
	file = {Versão submetida:files/1241/AlOmar et al. - 2021 - On preserving the behavior in software refactoring.pdf:application/pdf},
}

@article{lenarduzzi_sonarqube_2020,
	title = {Some {SonarQube} issues have a significant but small effect on faults and changes. {A} large-scale empirical study},
	volume = {170},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220301734},
	doi = {10.1016/j.jss.2020.110750},
	abstract = {Context:
Companies frequently invest effort to remove technical issues believed to impact software qualities, such as removing anti-patterns or coding styles violations.
Objective:
We aim to analyze the diffuseness of SonarQube issues in software systems and to assess their impact on code changes and fault-proneness, considering also their different types and severities.
Methods:
We conducted a case study among 33 Java projects from the Apache Software Foundation repository.
Results:
We analyzed 726 commits containing 27K faults and 12M changes in Java files. The projects violated 173 SonarQube rules generating more than 95K SonarQube issues in more than 200K classes. Classes not affected by SonarQube issues are less change-prone than affected ones, but the difference between the groups is small. Non-affected classes are slightly more change-prone than classes affected by SonarQube issues of type Code Smell or Security Vulnerability. As for fault-proneness, there is no difference between non-affected and affected classes. Moreover, we found incongruities in the type and severity assigned by SonarQube.
Conclusion:
Our result can be useful for practitioners to understand which SonarQube issues should be refactored and for researchers to bridge the missing gaps. Moreover, results can also support companies and tool vendors in identifying SonarQube issues as accurately as possible.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Lenarduzzi, Valentina and Saarimäki, Nyyti and Taibi, Davide},
	month = dec,
	year = {2020},
	keywords = {Empirical study, Fault-proneness, SonarQube, Change-proneness},
	pages = {110750},
	file = {Versão submetida:files/1242/Lenarduzzi et al. - 2020 - Some SonarQube issues have a significant but small.pdf:application/pdf},
}

@incollection{doty_306_2020,
	address = {Oxford},
	title = {3.06 - {Disorders} of {Taste} and {Smell}},
	isbn = {978-0-12-805409-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128093245237681},
	abstract = {Synopsis
All environmental chemicals responsible for life enter the body through the nose or mouth. The senses of taste and smell monitor such substances, not only warning of such dangers as spoiled food, leaking natural gas, polluted air, and smoke, but determining the flavor of foods and beverages. It is now well established that a large number of disorders and diseases, including Alzheimer's and Parkinson's, can compromise these senses, leading to depression, weight loss or weight gain, concerns for safety, changes in social behavior, and altered nutrition. This chapter provides an overview of the major diseases and disorders known to impact the functioning of these senses.},
	urldate = {2024-08-06},
	booktitle = {The {Senses}: {A} {Comprehensive} {Reference} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Doty, Richard L. and Bromley, Steven M.},
	editor = {Fritzsch, Bernd},
	month = jan,
	year = {2020},
	doi = {10.1016/B978-0-12-809324-5.23768-1},
	keywords = {Age, Alzheimer's, Diabetes, Disease, Electrogustometry, Head trauma, Kidney disease, Liver disease, Neurodegeneration, Olfaction, Parkinson's, Pollution, Psychophysics, Rhinitis, Sinusitis, Smoking, Taste, Threshold, UPSIT},
	pages = {119--147},
}

@article{alghamdi_how_2023-1,
	title = {How are websites used during development and what are the implications for the coding process?},
	volume = {205},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S016412122300198X},
	doi = {10.1016/j.jss.2023.111803},
	abstract = {Websites are frequently used to support the development process. This paper investigates how websites are used when writing code and programmers’ perceptions of the potential impact of this on their behaviour and the quality of the resulting software. We interviewed 18 programmers (13 students enrolled in undergraduate computer science courses, and 5 experienced professionals), and analysed the data thematically. The findings were used to develop a survey, which was distributed to 276 programmers (251 students, 25 experienced professionals). The results indicate that use of websites, especially Stack Overflow, is viewed as an essential part of programming by both students completing coursework and professionals developing code in industry. We also found that developers have experience of encountering a diverse set of problematic code snippets online, that copying code from websites without checking its quality or understanding how it worked is common, and that using online resources in this way had a potentially counter-productive effect on learning. Based on these findings, we make a number of recommendations, including better consideration of online code reuse in taught programmes, co-development and code-reuse practices in professional settings, and software licensing training for professional developers. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Alghamdi, Omar and Clinch, Sarah and Skeva, Rigina and Jay, Caroline},
	month = nov,
	year = {2023},
	keywords = {Stack Overflow, Computer science education, Human memory, Online code snippets, Problematic code, Professional practice},
	pages = {111803},
}

@article{silva_co-change_2019-1,
	title = {Co-change patterns: {A} large scale empirical study},
	volume = {152},
	issn = {0164-1212},
	shorttitle = {Co-change patterns},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121219300597},
	doi = {10.1016/j.jss.2019.03.014},
	abstract = {Co-Change Clustering is a modularity assessment technique that reveals how often changes are localized in modules and whether a change propagation represents design problems. This technique is centered on co-change clusters, which are highly inter-related source code files considering co-change relations. In this paper, we conduct a series of empirical analysis in a large corpus of 133 popular software projects on GitHub. We describe six co-change patterns by projecting them over the directory structure. We mine 1802 co-change clusters and 1719 co-change clusters (95\%) are covered by the six co-change patterns. In this study, we aim to answer two central questions: (i) Are co-change patterns detected in different programming languages? (ii) How do different co-change patterns relate to rippling, activity density, ownership, and team diversity on clusters? We conclude that Encapsulated and Well-Confined clusters (Wrapped) implement well-defined and confined concerns. Octopus clusters are proportionally numerous regarding to other patterns. They relate significantly with ripple effect, activity, ownership, and diversity in development teams. Although Crosscutting are scattered over directories, they implement well-defined concerns. Despite they present higher activity compared to Wrapped clusters, it is not necessarily easy to get rid of them, suggesting that support tools may play a crucial role.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Silva, Luciana L. and Valente, Marco Tulio and Maia, Marcelo A.},
	month = jun,
	year = {2019},
	keywords = {Co-change clusters, Co-change patterns, Modularity},
	pages = {196--214},
}

@article{niu_deep_2020,
	title = {A deep learning based static taint analysis approach for {IoT} software vulnerability location},
	volume = {152},
	issn = {0263-2241},
	url = {https://www.sciencedirect.com/science/article/pii/S026322411931005X},
	doi = {10.1016/j.measurement.2019.107139},
	abstract = {Computer system vulnerabilities, computer viruses, and cyber attacks are rooted in software vulnerabilities. Reducing software defects, improving software reliability and security are urgent problems in the development of software. The core content is the discovery and location of software vulnerability. However, traditional human experts-based approaches are labor-consuming and time-consuming. Thus, some automatic detection approaches are proposed to solve the problem. But, they have a high false negative rate. In this paper, a deep learning based static taint analysis approach is proposed to automatically locate Internet of Things (IoT) software vulnerability, which can relieve tedious manual analysis and improve detection accuracy. Deep learning is used to detect vulnerability since it considers the program context. Firstly, the taint from the difference file between the source program and its patched program selection rules are designed. Secondly, the taint propagation paths are got using static taint analysis. Finally, the detection model based on two-stage Bidirectional Long Short Term Memory (BLSTM) is applied to discover and locate software vulnerabilities. The Code Gadget Database is used to evaluate the proposed approach, which includes two types of vulnerabilities in C/C++ programs, buffer error vulnerability (CWE-119) and resource management error vulnerability (CWE-399). Experimental results show that our proposed approach can achieve an accuracy of 0.9732 for CWE-119 and 0.9721 for CWE-399, which is higher than that of the other three models (the accuracy of RNN, LSTM, and BLSTM is under than 0.97) and achieve a lower false negative rate and false positive rate than the other approaches.},
	urldate = {2024-08-06},
	journal = {Measurement},
	author = {Niu, Weina and Zhang, Xiaosong and Du, Xiaojiang and Zhao, Lingyuan and Cao, Rong and Guizani, Mohsen},
	month = feb,
	year = {2020},
	keywords = {Deep learning, IoT software vulnerability location, Software patching, Static taint analysis},
	pages = {107139},
}

@article{oesch_subjective_2023,
	title = {Subjective social class has a bad name, but predicts life chances well},
	volume = {83},
	issn = {0276-5624},
	url = {https://www.sciencedirect.com/science/article/pii/S0276562423000033},
	doi = {10.1016/j.rssm.2023.100759},
	abstract = {Over the last decades, the study of subjective class has been eclipsed by research on objective class. The recurrent mismatch between subjective and objective class has led to the common wisdom that self-reported class is a poor measure of people’s life chances. This article questions this common wisdom. Based on ISSP 2009 and 2019, it shows for 55 country surveys that a pre-coded question on subjective class accounts for more variance in life chances – income and wealth – than various measures of objective class. Subjective class predicts individual income equally well as does objective class, but is a much better predictor of household income and wealth. It takes the two measures of respondents’ and partners’ objective class to match the variance explained in household income by a single measure of subjective class. In contexts of limited survey space and interview time, subjective class is an excellent indicator of people’s material situation.},
	urldate = {2024-08-06},
	journal = {Research in Social Stratification and Mobility},
	author = {Oesch, Daniel and Vigna, Nathalie},
	month = feb,
	year = {2023},
	keywords = {Household income, International social survey program, Life chances, Subjective social class, Wealth},
	pages = {100759},
	file = {Texto completo:files/1243/Oesch e Vigna - 2023 - Subjective social class has a bad name, but predic.pdf:application/pdf},
}

@article{mahdavi-hezaveh_feature_2022-1,
	title = {Feature toggles as code: {Heuristics} and metrics for structuring feature toggles},
	volume = {145},
	issn = {0950-5849},
	shorttitle = {Feature toggles as code},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584921002445},
	doi = {10.1016/j.infsof.2021.106813},
	abstract = {Context:
Using feature toggles is a technique to turn a feature either on or off in program code by checking the value of a variable in a conditional statement. This technique is increasingly used by software practitioners to support continuous integration and continuous delivery (CI/CD). However, using feature toggles may increase code complexity, create dead code, and decrease the quality of a codebase.
Objective:
The goal of this research is to aid software practitioners in structuring feature toggles in the codebase by proposing and evaluating a set of heuristics and corresponding complexity, comprehensibility, and maintainability metrics based upon an empirical study of open source repositories.
Method:
We identified 80 GitHub repositories that use feature toggles in their development cycle. We conducted a qualitative analysis using 60 of the 80 repositories to identify heuristics and metrics. Then, we conducted a survey of practitioners of 80 repositories to obtain their feedback that the proposed heuristics can be used to guide the structure of feature toggles and to reduce technical debt. We also conducted a case study of the all 80 repositories to analyze relations between heuristics and metrics.
Results:
From the qualitative analysis, we proposed 7 heuristics to guide structuring feature toggles and identified 12 metrics to support the principles embodied in the heuristics. Our survey result shows that practitioners agree that managing feature toggles is difficult, and using identified heuristics can reduce technical debt. Based on our case study, we find a relationship between the adoption of heuristics and the values of metrics.
Conclusions:
Our results support that practitioners should have self-descriptive feature toggles, use feature toggles sparingly, avoid duplicate code in using feature toggles, and ensure complete removal of a feature toggle.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Mahdavi-Hezaveh, Rezvan and Ajmeri, Nirav and Williams, Laurie},
	month = may,
	year = {2022},
	keywords = {Continuous development, Continuous integration, Feature toggle, Heuristic, Metric, Open source repository},
	pages = {106813},
}

@article{li_enhancing_2024-1,
	title = {Enhancing code summarization with action word prediction},
	volume = {563},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223009001},
	doi = {10.1016/j.neucom.2023.126777},
	abstract = {Code summarization refers to automatically generating concise description in natural language from a code snippet. Good code summaries could effectively facilitate program comprehension and software maintenance. In recent years, various learning-based code summarization techniques have achieved impressive performance. Most of these models treat code summarization as an end-to-end model and directly generate the summaries, which ignores the fact that action words are crucial to code summaries. An essential characteristic of code summaries is the concentration of action word distribution. For instance, in the Funcom dataset, the top forty most-common action words account for 72\% of all samples. To incorporate this valuable prior domain knowledge into code summarization models, we develop a method for assisting code summarization through an additional action word prediction module, where an action predictor is employed to predict the primary action in the code summary, which is then used as a prompt to enhance the performance of the summary generation model. Our approach can be conveniently integrated into the existing models. We evaluate our approach on two Java datasets and a C/C++ dataset. The results show that our approach can efficiently improve the performance of the code summarization models. Furthermore, our action word prediction module can enhance the performance of a large pre-trained language model by prompting it with the predicted action words. This work suggests that a precise action word prediction model can significantly improve the performance of code summarization through the proposed action word guidance mechanism.},
	urldate = {2024-08-06},
	journal = {Neurocomputing},
	author = {Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Zhou, Ziyi and Huang, Zijie},
	month = jan,
	year = {2024},
	keywords = {Action word prediction, Code summarization, Deep learning, Multi-task learning},
	pages = {126777},
}

@article{cury_source_2024,
	title = {Source code expert identification: {Models} and application},
	volume = {170},
	issn = {0950-5849},
	shorttitle = {Source code expert identification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584924000508},
	doi = {10.1016/j.infsof.2024.107445},
	abstract = {Context:
Identifying source code expertise is useful in several situations. Activities like bug fixing and helping newcomers are best performed by knowledgeable developers. Some studies have proposed repository-mining techniques to identify source code experts. However, there is a gap in understanding which variables are most related to code knowledge and how they can be used for identifying expertise.
Objective:
This study explores models of expertise identification and how these models can be used to improve a Truck Factor algorithm.
Methods:
First, we built an oracle with the knowledge of developers from software projects. Then, we use this oracle to analyze the correlation between measures from the development history and source code knowledge. We investigate the use of linear and machine-learning models to identify file experts. Finally, we use the proposed models to improve a Truck Factor algorithm and analyze their performance using data from public and private repositories.
Results:
First Authorship and Recency of Modification have the highest positive and negative correlations with source code knowledge, respectively. Machine learning classifiers outperformed the linear techniques (F-Score = 71\% to 73\%) in the largest analyzed dataset, but this advantage is unclear in the smallest one. The Truck Factor algorithm using the proposed models could handle developers missed by the previous expertise model with the best average F-Score of 74\%. It was perceived as more accurate in computing the Truck Factor of an industrial project.
Conclusion:
If we analyze F-Score, the studied models have similar performance. However, machine learning classifiers get higher Precision while linear models obtained the highest Recall. Therefore, choosing the best technique depends on the user’s tolerance to false positives and negatives. Additionally, the proposed models significantly improved the accuracy of a Truck Factor algorithm, affirming their effectiveness in precisely identifying the key developers within software projects.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Cury, Otávio and Avelino, Guilherme and Neto, Pedro Santos and Valente, Marco Túlio and Britto, Ricardo},
	month = jun,
	year = {2024},
	keywords = {Machine learning, Source code expertise, Truck Factor},
	pages = {107445},
}

@article{ebert_exploratory_2015-1,
	title = {An exploratory study on exception handling bugs in {Java} programs},
	volume = {106},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121215000862},
	doi = {10.1016/j.jss.2015.04.066},
	abstract = {Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers’ perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat. Only 27\% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70\% of the organizations there are no specific tests for the exception handling code. Also, 61\% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40\% of the respondents consider the quality of exception handling code to be either good or very good and only 14\% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs. Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Ebert, Felipe and Castor, Fernando and Serebrenik, Alexander},
	month = aug,
	year = {2015},
	keywords = {Repository mining, Bugs, Exception handling},
	pages = {82--101},
	file = {Texto completo:files/1244/Ebert et al. - 2015 - An exploratory study on exception handling bugs in.pdf:application/pdf},
}

@article{kim_life-log_2014,
	series = {12th {International} {Conference} on {Design} and {Decision} {Support} {Systems} in {Architecture} and {Urban} {Planning}, {DDSS} 2014},
	title = {Life-log {Data}-based {Window} {Opening} and {Closing} for {Individual} {Customized} {Services} in {Symbiosis} {Houses}},
	volume = {22},
	issn = {1878-0296},
	url = {https://www.sciencedirect.com/science/article/pii/S1878029614001716},
	doi = {10.1016/j.proenv.2014.11.024},
	abstract = {Smart residential spaces designed for residents departs from simply providing conventional standardized services, to go beyond and provide personalized experiences that take individual circumstances into account. Such services play a key role in enhancing the residents’ quality of life. This study looks into such personalized services that reflect different characteristics of a diverse range of residents who have different behavior patterns. Such services can increase the satisfaction of the residents by providing flexible services that take into account the lifestyle and circumstances of each resident. A problem with offering customized services, however, is that there is a dearth of data on individuals. Sufficient amount of data must be collected in order to determine what a proper service for an individual is. As such, this study explains the life-log data and discusses its collection method. The life-log data collected serves as crucial grounds for decision-making. How decisions are made using the life log data is an intriguing research topic. This study proposes and discusses logics and processes of various decision-making methods that can be executed using the life log-data. In their homes, residents tend to regularly display certain behaviors in patterns, which allows for identifying the residents’ behavior patterns, as well as the predicting the residents’ future behavior. In this aspect, the residents’ location, needs, and current behavior must be recognized in order to provide personalized services. As such, this study proposes decision-making method by verifying in-house behavior of Korean elderly with companion dogs in symbiosis homes. Such dog's hair and foul smells cause indoor pollutions that damage elderly heath. This study proposes an automatic personalized window opening and closing service by using life-log data of the resident.},
	urldate = {2024-08-06},
	journal = {Procedia Environmental Sciences},
	author = {Kim, Yoora and Lee, Jungeun and Lee, Hyunsoo},
	month = jan,
	year = {2014},
	keywords = {Companion dog, Customized Service, Life-Log data, Symbiosis, Symbiosis House, Window Automation},
	pages = {247--256},
}

@article{pezaro_problematic_2022,
	title = {Problematic substance use in midwives registered with the {United} {Kingdom}’s {Nursing} and {Midwifery} {Council}: {A} pragmatic mixed methods study},
	volume = {112},
	issn = {0266-6138},
	shorttitle = {Problematic substance use in midwives registered with the {United} {Kingdom}’s {Nursing} and {Midwifery} {Council}},
	url = {https://www.sciencedirect.com/science/article/pii/S0266613822001619},
	doi = {10.1016/j.midw.2022.103409},
	abstract = {Objective
Use a pragmatic mixed methods approach to provide a rich understanding of the perceptions of Problematic Substance Use (PSU) and the influences of PSU on the mental and physical health of midwives registered with the Nursing and Midwifery Council (NMC).
Design
A confidential and anonymous self-administered online survey was employed to encourage wider participation.
Setting
United Kingdom
Participants
Midwives (n=623) registered with the NMC
Measurements
Open text responses were invited throughout the survey. Along with the collection of brief demographic data, PSU was also measured using the Tobacco, Alcohol, Prescription Medications, and Substance Use/Misuse (TAPS) Tool whilst mental and physical health was measured via version 2 of the Medical Outcomes Study Short-Form 12-Item Health Survey. All qualitative open text responses were analysed inductively using reflexive thematic analysis. Multiple regression was used to test whether health outcomes in the sample as a whole were predicted by PSU and Mann-Whitney U tests to compare the health dimensions between participants who met the criteria for PSU and those who did not.
Findings
PSU significantly predicted poorer general health, physical functioning, and mental functioning. Additionally, those who met criteria for PSU experienced significantly poorer general, mental, and physical health than those who did not. The influence of PSU was captured via 3 themes and 10 subthemes. Though the signs and symptoms of PSU identified remained broadly consistent, approaches to management did not. Many midwives were conflicted in how they might seek support without facing professional, personal and practical reprisal.
Key conclusions and implications for practice
PSU in midwifery populations poses professional, personal, and occupational risks. Congruence between policies and approaches to identification and management may reduce risk overall. Future interventions including educational and practitioner health programmes could also be usefully co-created with midwives, policy, and decision makers to reduce stigmatising attitudes and encourage greater awareness, compassion and help seeking to appropriate sources.},
	urldate = {2024-08-06},
	journal = {Midwifery},
	author = {Pezaro, Sally and Maher, Karen and Bailey, Elizabeth and Pearce, Gemma},
	month = sep,
	year = {2022},
	keywords = {Addiction, Impairment, Midwifery, Midwives, Occupational health, Substance use},
	pages = {103409},
	file = {Texto completo:files/1245/Pezaro et al. - 2022 - Problematic substance use in midwives registered w.pdf:application/pdf},
}

@article{perez-castillo_business_2019,
	title = {Business process model refactoring applying {IBUPROFEN}. {An} industrial evaluation},
	volume = {147},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S016412121830222X},
	doi = {10.1016/j.jss.2018.10.012},
	abstract = {Business process models are recognized as being important assets for companies, since appropriate management of them provides companies with a competitive advantage. Quality assurance of business process models has become a critical issue, especially when companies carry out reverse engineering techniques to retrieve their business process models. Thus, companies have to deal with several quality faults, such as unmeaningful elements, fine-grained granularity or incompleteness, which seriously affect understandability and modifiability of business process models. The most widely-used method to reduce these faults is refactoring. Although several refactoring operators exist in the literature, there are no refactoring techniques specially developed for business process models obtained by process mining and other reverse engineering techniques. Therefore, this paper presents the use of IBUPROFEN, a business process model refactoring technique for those models obtained by reverse engineering. IBUPROFEN is applied in an in-depth case study with a real-life information system belonging to a European bank company. The goal of this industrial evaluation is to prove that the refactoring operators improve the understandability and modifiability of the business process model after being refactored. In addition, the scalability of the technique is assessed to demonstrate the feasibility of its application.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Pérez-Castillo, Ricardo and Fernández-Ropero, María and Piattini, Mario},
	month = jan,
	year = {2019},
	keywords = {Refactoring, Case study, Business process model, Modifiability, Understandability},
	pages = {86--103},
}

@article{li_exploring_2022-1,
	title = {Exploring multi-programming-language commits and their impacts on software quality: {An} empirical study on {Apache} projects},
	volume = {194},
	issn = {0164-1212},
	shorttitle = {Exploring multi-programming-language commits and their impacts on software quality},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121222001844},
	doi = {10.1016/j.jss.2022.111508},
	abstract = {Context:
Modern software systems (e.g., Apache Spark) are usually written in multiple programming languages (PLs). There is little understanding on the phenomenon of multi-programming-language commits (MPLCs), which involve modified source files written in multiple PLs.
Objective:
This work aims to explore MPLCs and their impacts on development difficulty and software quality.
Methods:
We performed an empirical study on eighteen non-trivial Apache projects with 197,566 commits.
Results
: (1) the most commonly used PL combination consists of all the four PLs, i.e., C/C++, Java, JavaScript, and Python; (2) 9\% of the commits from all the projects are MPLCs, and the proportion of MPLCs in 83\% of the projects goes to a relatively stable level; (3) more than 90\% of the MPLCs from all the projects involve source files in two PLs; (4) the change complexity of MPLCs is significantly higher than that of non-MPLCs; (5) issues fixed in MPLCs take significantly longer to be resolved than issues fixed in non-MPLCs in 89\% of the projects; (6) MPLCs do not show significant effects on issue reopen; (7) source files undergoing MPLCs tend to be more bug-prone; and (8) MPLCs introduce more bugs than non-MPLCs.
Conclusions:
MPLCs are related to increased development difficulty and decreased software quality.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Li, Zengyang and Qi, Xiaoxiao and Yu, Qinyi and Liang, Peng and Mo, Ran and Yang, Chen},
	month = dec,
	year = {2022},
	keywords = {Bug introduction, Bug proneness, Change complexity, Issue reopen, Multi-programming-language commit, Open source software},
	pages = {111508},
	file = {Versão submetida:files/1246/Li et al. - 2022 - Exploring multi-programming-language commits and t.pdf:application/pdf},
}

@article{lenarduzzi_critical_2023-1,
	title = {A critical comparison on six static analysis tools: {Detection}, agreement, and precision},
	volume = {198},
	issn = {0164-1212},
	shorttitle = {A critical comparison on six static analysis tools},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121222002515},
	doi = {10.1016/j.jss.2022.111575},
	abstract = {Background:
Developers use Static Analysis Tools (SATs) to control for potential quality issues in source code, including defects and technical debt. Tool vendors have devised quite a number of tools, which makes it harder for practitioners to select the most suitable one for their needs. To better support developers, researchers have been conducting several studies on SATs to favor the understanding of their actual capabilities.
Aims:
Despite the work done so far, there is still a lack of knowledge regarding (1) what is their agreement, and (2) what is the precision of their recommendations. We aim at bridging this gap by proposing a large-scale comparison of six popular SATs for Java projects: Better Code Hub, CheckStyle, Coverity Scan, FindBugs, PMD, and SonarQube.
Methods:
We analyze 47 Java projects applying 6 SATs. To assess their agreement, we compared them by manually analyzing – at line – and class-level — whether they identify the same issues. Finally, we evaluate the precision of the tools against a manually-defined ground truth.
Results:
The key results show little to no agreement among the tools and a low degree of precision.
Conclusion:
Our study provides the first overview on the agreement among different tools as well as an extensive analysis of their precision that can be used by researchers, practitioners, and tool vendors to map the current capabilities of the tools and envision possible improvements.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Lenarduzzi, Valentina and Pecorelli, Fabiano and Saarimaki, Nyyti and Lujan, Savanna and Palomba, Fabio},
	month = apr,
	year = {2023},
	keywords = {Empirical study, Software quality, Static analysis tools},
	pages = {111575},
	file = {Texto completo:files/1247/Lenarduzzi et al. - 2023 - A critical comparison on six static analysis tools.pdf:application/pdf},
}

@article{santiago_microbiological_2018,
	title = {Microbiological control and antibacterial action of a propolis-containing mouthwash and control of dental plaque in humans},
	volume = {32},
	issn = {1478-6419},
	url = {https://www.sciencedirect.com/science/article/pii/S147864192101216X},
	doi = {10.1080/14786419.2017.1344664},
	abstract = {Propolis is a bee product with several biological properties. This study aimed at investigating a propolis-containing mouthwash, its organoleptic properties, microbial contamination and its antibacterial action in vitro. This mouthwash was assessed in vivo to control dental plaque in humans. The presence of microorganisms was analyzed and the minimum inhibitory concentration against Streptococcus mutans was determined. A comparative study was done in vivo using propolis, chlorhexidine, and propolis plus chlorhexidine in lower concentrations for 14 days. Dental plaque was analyzed by the Patient Hygiene Performance (PHP) index. The odontological product was yellow, cloudy, free of microbial contamination, and exerted an inhibitory action in vitro. Individuals who used a propolis-containing mouthwash for 14 consecutive days in combination or not to chlorhexidine showed a similar PHP index to chlorhexidine alone. The product exerted an antibacterial action in vitro and in vivo, exhibiting a positive action in the control of dental plaque. Unlabelled Figure},
	number = {12},
	urldate = {2024-08-06},
	journal = {Natural Product Research},
	author = {Santiago, Karina Basso and Piana, Gilce Maria and Conti, Bruno José and Cardoso, Eliza de Oliveira and Murbach Teles Andrade, Bruna Fernanda and Zanutto, Mirella Rossitto and Mores Rall, Vera Lúcia and Fernandes, Ary and Sforcin, José Maurício},
	month = jan,
	year = {2018},
	keywords = {antibacterial action, chlorhexidine, odontological product, Propolis},
	pages = {1441--1445},
	file = {Versão submetida:files/1248/Santiago et al. - 2018 - Microbiological control and antibacterial action o.pdf:application/pdf},
}

@article{szalay_practical_2021-1,
	title = {Practical heuristics to improve precision for erroneous function argument swapping detection in {C} and {C}++},
	volume = {181},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S016412122100145X},
	doi = {10.1016/j.jss.2021.111048},
	abstract = {Argument selection defects, in which the programmer chooses the wrong argument to pass to a parameter from a potential set of arguments in a function call, is a widely investigated problem. The compiler can detect such misuse of arguments only through the argument and parameter type for statically typed programming languages. When adjacent parameters have the same type or can be converted between one another, a swapped or out of order call will not be diagnosed by compilers. Related research is usually confined to exact type equivalence, often ignoring potential implicit or explicit conversions. However, in current mainstream languages, like C++, built-in conversions between numerics and user-defined conversions may significantly increase the number of mistakes to go unnoticed. We investigated the situation for C and C++ languages where developers can define functions with multiple adjacent parameters that allow arguments to pass in the wrong order. When implicit conversions – such as parameter pairs of types – are taken into account, the number of mistake-prone functions markedly increases compared to only strict type equivalence. We analysed a sample of projects and categorised the offending parameter types. The empirical results should further encourage the language and library development community to emphasise the importance of strong typing and to restrict the proliferation of implicit conversions. However, the analysis produces a hard to consume amount of diagnostics for existing projects, and there are always cases that match the analysis rule but cannot be “fixed”. As such, further heuristics are needed to allow developers to refactor effectively based on the analysis results. We devised such heuristics, measured their expressive power, and found that several simple heuristics greatly help highlight the more problematic cases.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Szalay, Richárd and Sinkovics, Ábel and Porkoláb, Zoltán},
	month = nov,
	year = {2021},
	keywords = {Argument selection defect, Error-prone constructs, Function parameters, Static analysis, Strong typing, Type safety},
	pages = {111048},
	file = {Texto completo:files/1249/Szalay et al. - 2021 - Practical heuristics to improve precision for erro.pdf:application/pdf},
}

@article{fabry_aspectj_2016-1,
	title = {{AspectJ} code analysis and verification with {GASR}},
	volume = {117},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121216300279},
	doi = {10.1016/j.jss.2016.04.014},
	abstract = {Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Fabry, Johan and De Roover, Coen and Noguera, Carlos and Zschaler, Steffen and Rashid, Awais and Jonckers, Viviane},
	month = jul,
	year = {2016},
	keywords = {Source code analysis, Aspect oriented programming, Logic program querying},
	pages = {528--544},
	file = {Texto completo:files/1250/Fabry et al. - 2016 - AspectJ code analysis and verification with GASR.pdf:application/pdf},
}

@article{zhang_diverse_2023-1,
	title = {Diverse title generation for {Stack} {Overflow} posts with multiple-sampling-enhanced transformer},
	volume = {200},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223000675},
	doi = {10.1016/j.jss.2023.111672},
	abstract = {Stack Overflow is one of the most popular programming communities where developers can seek help for their encountered problems. Nevertheless, if inexperienced developers fail to describe their problems clearly, it is hard for them to attract sufficient attention and get the anticipated answers. To address such a problem, we propose M3NSCT5, a novel approach to automatically generate multiple post titles from the given code snippets. Developers may take advantage of the generated titles to find closely related posts and complete their problem descriptions. M3NSCT5 employs the CodeT5 backbone, which is a pre-trained Transformer model with an excellent language understanding and generation ability. To alleviate the ambiguity issue that the same code snippets could be aligned with different titles under varying contexts, we propose the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from. We build a large-scale dataset with 890,000 question posts covering eight programming languages to validate the effectiveness of M3NSCT5. The automatic evaluation results on the BLEU and ROUGE metrics demonstrate the superiority of M3NSCT5 over six state-of-the-art baseline models. Moreover, a human evaluation with trustworthy results also demonstrates the great potential of our approach for real-world applications.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Zhang, Fengji and Liu, Jin and Wan, Yao and Yu, Xiao and Liu, Xiao and Keung, Jacky},
	month = jun,
	year = {2023},
	keywords = {Stack Overflow, CodeT5, Maximal marginal ranking, Nucleus sampling, Title generation},
	pages = {111672},
	file = {Versão submetida:files/1251/Zhang et al. - 2023 - Diverse title generation for Stack Overflow posts .pdf:application/pdf},
}

@article{de_dieu_characterizing_2023-1,
	title = {Characterizing architecture related posts and their usefulness in {Stack} {Overflow}},
	volume = {198},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223000031},
	doi = {10.1016/j.jss.2023.111608},
	abstract = {Context:
Stack Overflow (SO) has won the intention from software engineers (e.g., architects) to learn, practice, and utilize development knowledge, such as Architectural Knowledge (AK). But little is known about AK communicated in SO, which is a type of high-level but important knowledge in development.
Objective:
This study aims to investigate the AK in SO posts in terms of their categories and characteristics as well as their usefulness from the point of view of SO users.
Methods:
We conducted an exploratory study by qualitatively analyzing a statistically representative sample of 968 Architecture Related Posts (ARPs) from SO.
Results:
The main findings are: (1) architecture related questions can be classified into 9 core categories, in which “architecture configuration” is the most common category, followed by the “architecture decision” category, and (2) architecture related questions that provide clear descriptions together with architectural diagrams increase their likelihood of getting more than one answer, while poorly structured architecture questions tend to only get one answer.
Conclusions:
Our findings suggest that future research can focus on enabling automated approaches and tools that could facilitate the search and (re)use of AK in SO. SO users can refer to our proposed guidelines to compose architecture related questions with the likelihood of getting more responses in SO.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {de Dieu, Musengamana Jean and Liang, Peng and Shahin, Mojtaba and Khan, Arif Ali},
	month = apr,
	year = {2023},
	keywords = {Stack Overflow, Architectural knowledge, Architectural level element, Architecture solution, Usefulness},
	pages = {111608},
	file = {Versão submetida:files/1252/de Dieu et al. - 2023 - Characterizing architecture related posts and thei.pdf:application/pdf},
}

@article{chiari_doml_2024,
	title = {{DOML}: {A} new modeling approach to {Infrastructure}-as-{Code}},
	volume = {125},
	issn = {0306-4379},
	shorttitle = {{DOML}},
	url = {https://www.sciencedirect.com/science/article/pii/S0306437924000802},
	doi = {10.1016/j.is.2024.102422},
	abstract = {One of the main DevOps practices is the automation of resource provisioning and deployment of complex software. This automation is enabled by the explicit definition of Infrastructure-as-Code (IaC), i.e., a set of scripts, often written in different modeling languages, which defines the infrastructure to be provisioned and applications to be deployed. We introduce the DevOps Modeling Language (DOML), a new Cloud modeling language for infrastructure deployments. DOML is a modeling approach that can be mapped into multiple IaC languages, addressing infrastructure provisioning, application deployment and configuration. The idea behind DOML is to use a single modeling paradigm which can help to reduce the need of deep technical expertise in using different specialized IaC languages. We present the DOML’s principles and discuss the related work on IaC languages. Furthermore, the advantages of the DOML for the end-user are demonstrated in comparison with some state-of-the-art IaC languages such as Ansible, Terraform, and Cloudify, and an evaluation of its effectiveness through several examples and a case study is provided.},
	urldate = {2024-08-06},
	journal = {Information Systems},
	author = {Chiari, Michele and Xiang, Bin and Canzoneri, Sergio and Nedeltcheva, Galia Novakova and Di Nitto, Elisabetta and Blasi, Lorenzo and Benedetto, Debora and Niculut, Laurentiu and Škof, Igor},
	month = nov,
	year = {2024},
	keywords = {Evaluation, DevOps, DOML, IaC Modeling languages, Infrastructure-as-Code, Multi-layer modeling approach},
	pages = {102422},
}

@article{savic_language-independent_2014-1,
	title = {A language-independent approach to the extraction of dependencies between source code entities},
	volume = {56},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584914000925},
	doi = {10.1016/j.infsof.2014.04.011},
	abstract = {Context
Software networks are directed graphs of static dependencies between source code entities (functions, classes, modules, etc.). These structures can be used to investigate the complexity and evolution of large-scale software systems and to compute metrics associated with software design. The extraction of software networks is also the first step in reverse engineering activities.
Objective
The aim of this paper is to present SNEIPL, a novel approach to the extraction of software networks that is based on a language-independent, enriched concrete syntax tree representation of the source code.
Method
The applicability of the approach is demonstrated by the extraction of software networks representing real-world, medium to large software systems written in different languages which belong to different programming paradigms. To investigate the completeness and correctness of the approach, class collaboration networks (CCNs) extracted from real-world Java software systems are compared to CCNs obtained by other tools. Namely, we used Dependency Finder which extracts entity-level dependencies from Java bytecode, and Doxygen which realizes language-independent fuzzy parsing approach to dependency extraction. We also compared SNEIPL to fact extractors present in language-independent reverse engineering tools.
Results
Our approach to dependency extraction is validated on six real-world medium to large-scale software systems written in Java, Modula-2, and Delphi. The results of the comparative analysis involving ten Java software systems show that the networks formed by SNEIPL are highly similar to those formed by Dependency Finder and more precise than the comparable networks formed with the help of Doxygen. Regarding the comparison with language-independent reverse engineering tools, SNEIPL provides both language-independent extraction and representation of fact bases.
Conclusion
SNEIPL is a language-independent extractor of software networks and consequently enables language-independent network-based analysis of software systems, computation of design software metrics, and extraction of fact bases for reverse engineering activities.},
	number = {10},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Savić, Miloš and Rakić, Gordana and Budimac, Zoran and Ivanović, Mirjana},
	month = oct,
	year = {2014},
	keywords = {Software metrics, Dependency extraction, Enriched concrete syntax tree, Fact extraction, Reverse engineering, Software networks},
	pages = {1268--1288},
}

@article{danutama_scalable_2013,
	series = {4th {International} {Conference} on {Electrical} {Engineering} and {Informatics}, {ICEEI} 2013},
	title = {Scalable {Autograder} and {LMS} {Integration}},
	volume = {11},
	issn = {2212-0173},
	url = {https://www.sciencedirect.com/science/article/pii/S2212017313003617},
	doi = {10.1016/j.protcy.2013.12.207},
	abstract = {Autograder has been proven to be an effective and efficient judge in programming contests. The result can be delivered immediately. As an example, in International Olympiad in Informatics and ACM Inter-Collegiate Programming Contest, the autograder is used for judging complex programs for large numbers of contestants. Having experience with autograder in contests, we aim to use it in large programming course so that the student can have immediate result and feedback. In a large class, we have to manage hundreds of students where each student submits many small exercises regularly. Manual grading will be time consuming. However, the nature of a course is different from contest. In this research, we integrate an autograder and an existing Learning Management System (LMS) with the objective of taking the benefits from both applications. A dispatcher is designed and implemented to bridge the LMS and the autograder. LMS is used by the teachers to manage classes, scores, questions and other administrative tasks. Autograder enriches LMS functionality in order to grade programs automatically. Other than automatic grading functionality, the system must be robust, scalable and secure, that come from a large course size with an abundant amount of program submissions. The system is tested with various testing methodologies to prove the requirements are satisfied.},
	urldate = {2024-08-06},
	journal = {Procedia Technology},
	author = {Danutama, Karol and Liem, Inggriani},
	month = jan,
	year = {2013},
	keywords = {automatic grading, Dispatcher, large course size, LMS, programming course},
	pages = {388--395},
}

@article{paiva_accessibility_2021,
	title = {Accessibility and {Software} {Engineering} {Processes}: {A} {Systematic} {Literature} {Review}},
	volume = {171},
	issn = {0164-1212},
	shorttitle = {Accessibility and {Software} {Engineering} {Processes}},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220302168},
	doi = {10.1016/j.jss.2020.110819},
	abstract = {Guidelines, techniques, and methods have been presented in the literature in recent years to contribute to the development of accessible software and to promote digital inclusion. Considering that software product quality depends on the quality of the development process, researchers have investigated how to include accessibility during the software development process in order to obtain accessible software. Two Systematic Literature Reviews (SLR) have been conducted in the past to identify such research initiatives. This paper presents a new SLR, considering the period from 2011 to 2019. The review of 94 primary studies showed the distribution of publications on different phases of the software life cycle, mainly the design and testing phases. The study also identified, for the first time, papers about accessibility and software process establishment. This result reinforces that, in fact, accessibility is not characterized as a property of the final software only. Instead, it evolves over the software life cycle. Besides, this study aims to provide designers and developers with an updated view of methods, tools, and other assets that contribute to process enrichment, valuing accessibility, as well as shows the gaps and challenges which deserve to be investigated.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Paiva, Débora Maria Barroso and Freire, André Pimenta and de Mattos Fortes, Renata Pontin},
	month = jan,
	year = {2021},
	keywords = {Accessibility, Design for disabilities, Methods for accessibility, Software Engineering, Systematic Literature Review},
	pages = {110819},
}

@article{seraku_poi_2019,
	title = {\textit{{Poi}} in {Japanese} \textit{{Wakamono} {Kotoba}} ‘youth language’: {A} view from attenuation at the speech-act dimension},
	volume = {224},
	issn = {0024-3841},
	shorttitle = {\textit{{Poi}} in {Japanese} \textit{{Wakamono} {Kotoba}} ‘youth language’},
	url = {https://www.sciencedirect.com/science/article/pii/S0024384119300178},
	doi = {10.1016/j.lingua.2019.03.009},
	urldate = {2024-08-06},
	journal = {Lingua},
	author = {Seraku, Tohru and Akiha, Takako},
	month = jun,
	year = {2019},
	pages = {1--15},
}

@article{muallil_rapid_2024,
	title = {A rapid assessment of the status, trends and challenges in small-scale commercial sardine fisheries in the {Sulu} {Archipelago}, southern {Philippines}},
	volume = {160},
	issn = {0308-597X},
	url = {https://www.sciencedirect.com/science/article/pii/S0308597X23004980},
	doi = {10.1016/j.marpol.2023.105965},
	abstract = {Sardines are a crucial component of Philippine capture fisheries, providing economic benefits and contributing to food security. This study aimed to provide an overview of the status, trends and challenges in the sardine fishery in the Sulu Archipelago, a major fishing ground for sardines, focusing on the traditional but less-documented small-scale commercial ringnets or locally called kulibu. The modern kulibu has an average size of about 10 gross tons and is operated by 7–15 crew members. We estimated about 500 kulibu vessels operating in the entire Sulu Archipelago, with Sulu having the most, followed by Basilan and Tawi-Tawi, respectively. The normal catch rates of a kulibu ranged from 10 to 20 tubs or banggera per night, with each banggera containing about 40 kg of fish. While some fishers reported that catches are still abundant and did not change much over the past three decades, we have identified some signs of overfishing. These include using of advanced technology and bigger boats/nets, expansion of fishing to more distant areas, the increasing frequency of low to zero catches, and a change in species composition indicative of fishing down marine foodwebs. Fishers have identified major issues and challenges confronting the fishery, such as intrusion of larger purse seiners into their fishing grounds, lack or inefficiency of post-harvest facilities, absence of a reliable market for their catches, and security risks at sea. To provide a comprehensive perspective, we adopted the Sustainable Livelihood Approach (SLA) framework, highlighting the broader implications of our findings for fishery management. Bottom of Form},
	urldate = {2024-08-06},
	journal = {Marine Policy},
	author = {Muallil, Richard N. and Irilis, Roger A. and Habibuddin, Argamar A. and Abdulmajid, Nur-aisa S. and Ajik, Jaro O. and Ancheta, Ronaldo A.},
	month = feb,
	year = {2024},
	keywords = {Commercial fishery, Livelihood, Overfishing, Sardine fishery, Small-scale fishery, Sulu Archipelago},
	pages = {105965},
}

@article{lane-loney_cognitive-behavioral_2022,
	title = {A {Cognitive}-{Behavioral} {Family}-{Based} {Protocol} for the {Primary} {Presentations} of {Avoidant}/{Restrictive} {Food} {Intake} {Disorder} ({ARFID}): {Case} {Examples} and {Clinical} {Research} {Findings}},
	volume = {29},
	issn = {1077-7229},
	shorttitle = {A {Cognitive}-{Behavioral} {Family}-{Based} {Protocol} for the {Primary} {Presentations} of {Avoidant}/{Restrictive} {Food} {Intake} {Disorder} ({ARFID})},
	url = {https://www.sciencedirect.com/science/article/pii/S1077722920300833},
	doi = {10.1016/j.cbpra.2020.06.010},
	abstract = {Avoidant/restrictive food intake disorder (ARFID) is a feeding and eating disorder with evidence for distinct but overlapping presentations characterized by avoidance of eating or narrow dietary range related to poor appetite, selective eating, or fear of aversive consequences of eating. The current paper describes a flexible, cognitive-behavioral, family-oriented treatment approach that has been applied to each of these ARFID presentations within a larger partial hospitalization program (PHP) for eating disorders. We provide composite case examples for the presentations and retrospective outcome data on a sample of 81 patients treated with the protocol. Overall, patients with ARFID exhibited significant increases in body weight and the number of foods accepted, and significant decreases on measures assessing food fears, oral control behavior, anxiety, and depression. At baseline, patients with co-occurring poor appetite and selective eating exhibited significantly lower body weight than those with fear of aversive consequences, while the latter group of patients were eating significantly fewer foods and feared a larger number of foods according to parent report. Patients with the fear of aversive consequences presentation also experienced greater increases in the number of foods they were willing to eat and greater decreases in the number of foods they feared over the course of treatment relative to the other two groups. Our findings provide preliminary support for the effectiveness of our family-centered, cognitive-behavioral PHP for children and adolescents with ARFID. Future studies with more sophisticated methods, including randomized controlled designs and ARFID-specific measures, are recommended to help establish evidence-based psychosocial interventions for this clinical population.},
	number = {2},
	urldate = {2024-08-06},
	journal = {Cognitive and Behavioral Practice},
	author = {Lane-Loney, Susan E. and Zickgraf, Hana F. and Ornstein, Rollyn M. and Mahr, Fauzia and Essayli, Jamal H.},
	month = may,
	year = {2022},
	keywords = {ARFID, ARFID subtypes, Avoidant/restrictive food intake disorder, CBT-AR, exposure therapy},
	pages = {318--334},
}

@article{espectato_when_2024,
	title = {When outsiders are resource users: {Fishers}’ migration in {Southwest} {Panay}, {Philippines}, and its impact on small-scale fisheries},
	volume = {253},
	issn = {0964-5691},
	shorttitle = {When outsiders are resource users},
	url = {https://www.sciencedirect.com/science/article/pii/S0964569124001273},
	doi = {10.1016/j.ocecoaman.2024.107142},
	abstract = {This paper examined the fishers’ migration in Southwest Panay, Philippines, how it has shaped the small-scale fisheries management in the receiving area, influenced local resource access and utilization, and the potential impacts of the phenomenon on fishing technology and practices. The study was conducted in two municipalities in the province of Iloilo (Miagao and San Joaquin) and two in the province of Antique (Anini-y and San Jose de Buenavista). It utilized household surveys, key informant interviews (KII), and focus group discussion (FGD) as research methodologies. Results show that there are various push and pull factors that influence the migrant fishers (locally known as pangayaw) to migrate to other areas. The most common reason cited is the presence of a richer fishing ground or better livelihood opportunities in their host community. Migrant fishers play a vital role in the local fishing economy as sources of new fishing technology. However, results revealed that most of the technologies that they introduced were either more efficient gears (e.g., use of purse seine and gill nets) or modifications (e.g., use of kati or decoy) that would increase the efficiency of the fishing gears. This may have a consequent negative impact on the sustainability of the fishery resources in the host community. It is evident from the available data of the local government units and the observations of the participants through their local ecological knowledge that migration has a potential impact on local resource access and utilization patterns, and may influence the socio-cultural landscape of the receiving area. It poses a challenge to the sustainable utilization of the resources, and exerts additional pressure on the social services of the receiving local governments. Instead of treating them as “outsiders”, they should be treated as partners of the local community and must share the responsibilities of sustainable utilization and management of the fisheries resources.},
	urldate = {2024-08-06},
	journal = {Ocean \& Coastal Management},
	author = {Espectato, Liberty N. and Napata, Ruby P. and Espia, Juhn Chris and Prieto-Carolino, Alice},
	month = jul,
	year = {2024},
	keywords = {Fishers' migration, Philippines, Small-scale fisheries management, Southwest Panay},
	pages = {107142},
}

@article{li_systematic_2015,
	title = {A systematic mapping study on technical debt and its management},
	volume = {101},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121214002854},
	doi = {10.1016/j.jss.2014.12.027},
	abstract = {Context
Technical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system.
Objective
This work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM.
Method
A systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013.
Results
Ninety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected.
Conclusions
The term “debt” has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need for more empirical studies with high-quality evidence on the whole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Li, Zengyang and Avgeriou, Paris and Liang, Peng},
	month = mar,
	year = {2015},
	keywords = {Systematic mapping study, Technical debt, Technical debt management},
	pages = {193--220},
	file = {Texto completo:files/1253/Li et al. - 2015 - A systematic mapping study on technical debt and i.pdf:application/pdf},
}

@article{ferenc_automatically_2020,
	title = {An automatically created novel bug dataset and its validation in bug prediction},
	volume = {169},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220301436},
	doi = {10.1016/j.jss.2020.110691},
	abstract = {Bugs are inescapable during software development due to frequent code changes, tight deadlines, etc.; therefore, it is important to have tools to find these errors. One way of performing bug identification is to analyze the characteristics of buggy source code elements from the past and predict the present ones based on the same characteristics, using e.g. machine learning models. To support model building tasks, code elements and their characteristics are collected in so-called bug datasets which serve as the input for learning. We present the BugHunter Dataset: a novel kind of automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information. Other available bug datasets follow the traditional approach of gathering the characteristics of all source code elements (buggy and non-buggy) at only one or more pre-selected release versions of the code. Our approach, on the other hand, captures the buggy and the fixed states of the same source code elements from the narrowest timeframe we can identify for a bug’s presence, regardless of release versions. To show the usefulness of the new dataset, we built and evaluated bug prediction models and achieved F-measure values over 0.74.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Ferenc, Rudolf and Gyimesi, Péter and Gyimesi, Gábor and Tóth, Zoltán and Gyimóthy, Tibor},
	month = nov,
	year = {2020},
	keywords = {GitHub, Machine learning, Code metrics, Bug dataset, Bug prediction, Static code analysis},
	pages = {110691},
	file = {Versão submetida:files/1254/Ferenc et al. - 2020 - An automatically created novel bug dataset and its.pdf:application/pdf},
}

@article{sutton_spiking_2021-1,
	title = {Spiking neural networks and hippocampal function: {A} web-accessible survey of simulations, modeling methods, and underlying theories},
	volume = {70},
	issn = {1389-0417},
	shorttitle = {Spiking neural networks and hippocampal function},
	url = {https://www.sciencedirect.com/science/article/pii/S1389041721000589},
	doi = {10.1016/j.cogsys.2021.07.008},
	abstract = {Computational modeling has contributed to hippocampal research in a wide variety of ways and through a large diversity of approaches, reflecting the many advanced cognitive roles of this brain region. The intensively studied neuron type circuitry of the hippocampus is a particularly conducive substrate for spiking neural models. Here we present an online knowledge base of spiking neural network simulations of hippocampal functions. First, we overview theories involving the hippocampal formation in subjects such as spatial representation, learning, and memory. Then we describe an original literature mining process to organize published reports in various key aspects, including: (i) subject area (e.g., navigation, pattern completion, epilepsy); (ii) level of modeling detail (Hodgkin-Huxley, integrate-and-fire, etc.); and (iii) theoretical framework (attractor dynamics, oscillatory interference, self-organizing maps, and others). Moreover, every peer-reviewed publication is also annotated to indicate the specific neuron types represented in the network simulation, establishing a direct link with the Hippocampome.org portal. The web interface of the knowledge base enables dynamic content browsing and advanced searches, and consistently presents evidence supporting every annotation. Moreover, users are given access to several types of statistical reports about the collection, a selection of which is summarized in this paper. This open access resource thus provides an interactive platform to survey spiking neural network models of hippocampal functions, compare available computational methods, and foster ideas for suitable new directions of research.},
	urldate = {2024-08-06},
	journal = {Cognitive Systems Research},
	author = {Sutton, Nate M. and Ascoli, Giorgio A.},
	month = dec,
	year = {2021},
	keywords = {Knowledge base, Computational, Hippocampus, Modeling, Spiking neural network},
	pages = {80--92},
	file = {Versão aceita:files/1255/Sutton e Ascoli - 2021 - Spiking neural networks and hippocampal function .pdf:application/pdf},
}

@article{janes_open_2023-1,
	title = {Open tracing tools: {Overview} and critical comparison},
	volume = {204},
	issn = {0164-1212},
	shorttitle = {Open tracing tools},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223001887},
	doi = {10.1016/j.jss.2023.111793},
	abstract = {Background:
Coping with the rapid growing complexity in contemporary software architecture, tracing has become an increasingly critical practice and been adopted widely by software engineers. By adopting tracing tools, practitioners are able to monitor, debug, and optimize distributed software architectures easily. However, with excessive number of valid candidates, researchers and practitioners have a hard time finding and selecting the suitable tracing tools by systematically considering their features and advantages.
Objective:
To such a purpose, this paper aims to provide an overview of popular Open tracing tools via comparison.
Methods:
Herein, we first identified 30 tools in an objective, systematic, and reproducible manner adopting the Systematic Multivocal Literature Review protocol. Then, we characterized each tool looking at the 1) measured features, 2) popularity both in peer-reviewed literature and online media, and 3) benefits and issues. We used topic modeling and sentiment analysis to extract and summarize the benefits and issues. Specially, we adopted ChatGPT to support the topic interpretation.
Results:
As a result, this paper presents a systematic comparison amongst the selected tracing tools in terms of their features, popularity, benefits and issues.
Conclusion:
The result mainly shows that each tracing tool provides a unique combination of features with also different pros and cons. The contribution of this paper is to provide the practitioners better understanding of the tracing tools facilitating their adoption.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Janes, Andrea and Li, Xiaozhou and Lenarduzzi, Valentina},
	month = oct,
	year = {2023},
	keywords = {ChatGPT, Multivocal literature review, Open tracing tool, Telemetry},
	pages = {111793},
	file = {Versão submetida:files/1256/Janes et al. - 2023 - Open tracing tools Overview and critical comparis.pdf:application/pdf},
}

@incollection{gloukhovtsev_chapter_2024,
	title = {Chapter four - {Green} and sustainable software engineering},
	isbn = {978-0-443-13597-2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780443135972000042},
	abstract = {The growing energy consumption of software has garnered attention in recent years within the realm of green IT initiatives. The surge in digitalization has led to an increased overall power consumption generated by software, giving rise to terms such as "Green and sustainable software" and "power-aware software." This Chapter explores conceptual and methodological approaches to assess the indirect material impacts of immaterial software products. One avenue towards green and sustainable software engineering involves optimizing the software development process itself. By streamlining the software development lifecycle, developers can mitigate the environmental impact of applications. Practices such as reducing code complexity, employing open-source software, minimizing data storage requirements, and designing for energy efficiency play pivotal roles in this approach. Another perspective centers on the software architecture of applications. Designing applications with an emphasis on energy efficiency and minimal resource requirements offers a pathway to reducing their environmental impact. While existing studies on green software development often concentrate on enhancing the energy efficiency of hardware, there is a growing realization that software lifecycle needs to encompass energy efficiency as a fundamental goal. The Green Software Foundation has outlined eight principles that position software development methodologies at the core of sustainable software development. Determining the sustainability of software products requires well-established and accepted metrics. Various methodologies, such as the green performance indicators system (GPIs), have been developed for this purpose. The GPI system includes categories like IT resource usage GPIs, application lifecycle KPIs, energy impact GPIs, and organizational GPIs, each contributing to a comprehensive evaluation of software sustainability.},
	urldate = {2024-08-06},
	booktitle = {Making {IT} {Sustainable}},
	publisher = {Academic Press},
	author = {Gloukhovtsev, Mikhail},
	editor = {Gloukhovtsev, Mikhail},
	month = jan,
	year = {2024},
	doi = {10.1016/B978-0-443-13597-2.00004-2},
	keywords = {green coding practices, Green software development, sustainable software development, sustainable software engineering},
	pages = {91--109},
}

@incollection{doty_index_2019,
	series = {Smell and {Taste}},
	title = {Index},
	volume = {164},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444638557099871},
	urldate = {2024-08-06},
	booktitle = {Handbook of {Clinical} {Neurology}},
	publisher = {Elsevier},
	editor = {Doty, Richard L.},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-444-63855-7.09987-1},
	pages = {481--490},
}

@article{sheikhaei_study_2023-1,
	title = {A study of update request comments in {Stack} {Overflow} answer posts},
	volume = {198},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121222002667},
	doi = {10.1016/j.jss.2022.111590},
	abstract = {Comments play an important role in updating Stack Overflow (SO) posts. They are used to point out a problem (e.g., obsolete answer and buggy code) in a SO answer or ask for more details about a proposed answer. We refer to this type of comment as update request comments (URCs), which may trigger an update to the answer post and thus improve its quality. In this study, we manually analyze a set of 384 sampled SO answer posts and their associated 1,221 comments to investigate the prevalence of URCs and how URCs are addressed. We find that around half of the analyzed comments are URCs. While 55.3\% of URCs are addressed within 24 h, 36.5\% of URCs remain unaddressed after a year. Moreover, we find that the current community-vote mechanism could not differentiate URCs from non-URCs. Thus many URCs might not be aware by users who can address the issue or improve the answer quality. As a first step to enhance the awareness of URCs and support future research on URCs, we investigate the feasibility of URC detection by proposing a set of features extracted from different aspects of SO comments and using them to build supervised classifiers that can automatically identify URCs. Our experiments on 377 and 289 comments posted on answers to JavaScript and Python questions show that the proposed URC classifier can achieve an accuracy of 90\% and an AUC of 0.96, on average.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Sheikhaei, Mohammad Sadegh and Tian, Yuan and Wang, Shaowei},
	month = apr,
	year = {2023},
	keywords = {Answer quality, Classification, Commenting, Crowd-sourced knowledge sharing, Knowledge maintenance and update, Stack overflow},
	pages = {111590},
	file = {Versão submetida:files/1257/Sheikhaei et al. - 2023 - A study of update request comments in Stack Overfl.pdf:application/pdf},
}

@incollection{choras_chapter_2018,
	series = {Intelligent {Data}-{Centric} {Systems}},
	title = {Chapter 8 - {Machine} {Learning} {Techniques} for {Threat} {Modeling} and {Detection}},
	isbn = {978-0-12-811373-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128113738000082},
	abstract = {The major goal of this chapter is to overview and present the practical realizations of the bio-inspired concepts for cyber security. Hereby, we do not discuss the bio-inspired concepts for cyber security on a high and abstract level, but we present our own concrete solutions and practical implementations and also briefly mention other relevant works. Our goal is to prove that the bio-inspired techniques can be implemented to cyber security and that readiness level of such technology is constantly increasing. Particularly, we have investigated and presented the practical solutions for the evolutionary-based optimization techniques, collective intelligence, and techniques that mimic social behavior of species. In this chapter, we present and focus on our own results and give references to our past and ongoing cyber security projects where we successfully implemented different nature-inspired solutions. The proposed genetic algorithms improve detection of SQL injection attacks and anomalies within HTTP requests. Similarly, the proposed ensemble of classifiers and correlation techniques allow for the improved cyberattack detection. Furthermore, the collective intelligence concept has been successfully implemented in the Federated Networks Protection System.},
	urldate = {2024-08-06},
	booktitle = {Security and {Resilience} in {Intelligent} {Data}-{Centric} {Systems} and {Communication} {Networks}},
	publisher = {Academic Press},
	author = {Choraś, Michał and Kozik, Rafał},
	editor = {Ficco, Massimo and Palmieri, Francesco},
	month = jan,
	year = {2018},
	doi = {10.1016/B978-0-12-811373-8.00008-2},
	keywords = {Machine learning, Bio-inspired cyber security, Threat modeling},
	pages = {179--192},
}

@article{imtiaz_automated_2021,
	title = {An automated model-based approach to repair test suites of evolving web applications},
	volume = {171},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121220302314},
	doi = {10.1016/j.jss.2020.110841},
	abstract = {Capture–Replay tools are widely used for the automated testing of web applications The scripts written for these Capture–Replay tools are strongly coupled with the web elements of web applications. These test scripts are sensitive to changes in web elements and require repairs as the web pages evolve. In this paper, we propose an automated model-based approach to repair the Capture–Replay test scripts that are broken due to such changes. Our approach repairs the test scripts that may be broken due to the breakages (e.g., broken locators, missing web elements) reported in the existing test breakage taxonomy. Our approach is based on a DOM-based strategy and is independent of the underlying Capture–Replay tool. We developed a tool to demonstrate the applicability of the approach. We perform an empirical study on seven subject applications. The results show that the approach successfully repairs the broken test scripts while maintaining the same DOM coverage and fault-finding capability. We also evaluate the usefulness of the repaired test scripts according to the opinion of professional testers. We conduct an experiment to compare our approach with the state-of-the-art DOM-based test repair approach, WATER. The comparison results show that our approach repairs more test breakages than WATER.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Imtiaz, Javaria and Iqbal, Muhammad Zohaib and khan, Muhammad Uzair},
	month = jan,
	year = {2021},
	keywords = {Web testing, Automated test scripts, Model-based, Regression testing, Web test repair},
	pages = {110841},
}

@article{kritikos_survey_2019,
	title = {A survey on vulnerability assessment tools and databases for cloud-based web applications},
	volume = {3-4},
	issn = {2590-0056},
	url = {https://www.sciencedirect.com/science/article/pii/S2590005619300116},
	doi = {10.1016/j.array.2019.100011},
	abstract = {Due to its various offered benefits, an ever increasing number of applications are migrated to the cloud. However, such a migration should be carefully performed due to the cloud’s public nature. Further, due to the agile development cycle that applications follow, their security level might not be the best possible, exhibiting various sorts of vulnerability. As such, to better support application migration and runtime provisioning, this article supplies three main contributions. First, it attempts to connect vulnerability management to the application lifecycle so as to highlight the exact moments where application vulnerability assessment must be performed. Second, it analyses the state-of-the-art open-source tools and databases so as to enable developers to make an informed decision about which ones to select. In this sense, discovering such vulnerabilities will enable to better secure applications before or after migrating them to the cloud. The analysis conducted is quite rich, covering various aspects and a rich sets of criteria. Third, it explores the claim that vulnerability scanning tools need to be orchestrated to reach the highest possible vulnerability coverage, both in terms of extend and breadth. Finally, this article concludes with some challenges that current vulnerability tools and databases need to face to increase their added-value and applicability level.},
	urldate = {2024-08-06},
	journal = {Array},
	author = {Kritikos, Kyriakos and Magoutis, Kostas and Papoutsakis, Manos and Ioannidis, Sotiris},
	month = sep,
	year = {2019},
	keywords = {Survey, Tools, Evaluation, Database, Accuracy, Assessment, Performance, Scanning, Vulnerability},
	pages = {100011},
}

@article{suslina_use_2020,
	series = {Postproceedings of the 10th {Annual} {International} {Conference} on {Biologically} {Inspired} {Cognitive} {Architectures}, {BICA} 2019 ({Tenth} {Annual} {Meeting} of the {BICA} {Society}), held {August} 15-19, 2019 in {Seattle}, {Washington}, {USA}},
	title = {Use of {Digital} {Technologies} for {Optimizing} the {Handling} of {Trademark} {Applications}},
	volume = {169},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050920303665},
	doi = {10.1016/j.procs.2020.02.242},
	abstract = {The article considers the analysis of the role of digital technologies in the handling of trademark applications. The article describes the main types of trademarks, the process of trademark registration in the Russian Federation and the application requirements. It also provides the main achievements in the use of technologies in the field of trademarks and in the facilitation of filing procedure for the applicants.},
	urldate = {2024-08-06},
	journal = {Procedia Computer Science},
	author = {Suslina, Irina and Mineeva, Polina},
	month = jan,
	year = {2020},
	keywords = {artificial intelligence in IP, digital technologies, intellectual property, technologies in IP, trademarks},
	pages = {435--439},
}

@article{nunez-varela_source_2017,
	title = {Source code metrics: {A} systematic mapping study},
	volume = {128},
	issn = {0164-1212},
	shorttitle = {Source code metrics},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121217300663},
	doi = {10.1016/j.jss.2017.03.044},
	abstract = {Context
Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics.
Objectives
This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends.
Method
A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed.
Results
Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines.
Conclusions
Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Nuñez-Varela, Alberto S. and Pérez-Gonzalez, Héctor G. and Martínez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
	month = jun,
	year = {2017},
	keywords = {Systematic mapping study, Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics},
	pages = {164--197},
}

@article{boccalini_distinctive_2024,
	title = {Distinctive clinical and imaging trajectories in {SWEDD} and {Parkinson}’s disease patients},
	volume = {42},
	issn = {2213-1582},
	url = {https://www.sciencedirect.com/science/article/pii/S2213158224000317},
	doi = {10.1016/j.nicl.2024.103592},
	abstract = {A proportion of patients clinically diagnosed with Parkinson’s disease (PD) can have a 123I-FP-CIT-SPECT scan without evidence of dopaminergic deficit (SWEDD), generating a debate about the underlying biological mechanisms. This study investigated differences in clinical features, 123I-FP-CIT binding, molecular connectivity, as well as clinical and imaging progression between SWEDD and PD patients. We included 36 SWEDD, 49 de novo idiopathic PD, and 49 healthy controls with 123I-FP-CIT-SPECT from the Parkinson’s Progression Markers Initiative. Clinical and imaging 2-year follow-ups were available for 27 SWEDD and 40 PD. Regional-based and voxel-wise analysis assessed dopaminergic integrity in dorsal and ventral striatal, as well as extrastriatal regions, at baseline and follow-up. Molecular connectivity analyses evaluated dopaminergic pathways. Spatial correlation analyses tested whether 123I-FP-CIT-binding alterations would also pertain to the serotoninergic system. SWEDD and PD patients showed comparable symptoms at baseline, except for hyposmia, which was more severe for PD. PD showed significantly lower striatal and extrastriatal 123I-FP-CIT-binding compared to SWEDD and controls. SWEDD exhibited lower binding than controls in striatal regions, insula, and olfactory cortex. Both PD and SWEDD showed extensive altered connectivity of dopaminergic pathways, however, with major impairment in the mesocorticolimbic system for SWEDD. Motor symptoms and dopaminergic deficits worsened after 2 years for PD only. The limited dopaminergic impairment and its stability over time observed for SWEDD, as well as the presence of extrastriatal 123I-FP-CIT binding alterations and prevalent mesocorticolimbic connectivity impairment, suggest other mechanisms contributing to SWEDD pathophysiology.},
	urldate = {2024-08-06},
	journal = {NeuroImage: Clinical},
	author = {Boccalini, Cecilia and Nicastro, Nicolas and Perani, Daniela and Garibotto, Valentina},
	month = jan,
	year = {2024},
	keywords = {I-FP-CIT SPECT, Molecular connectivity, Neurotransmission, Parkinson's disease, Scans without evidence of dopaminergic deficit (SWEDD)},
	pages = {103592},
}

@article{mondal_survey_2020-1,
	title = {A survey on clone refactoring and tracking},
	volume = {159},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121219302031},
	doi = {10.1016/j.jss.2019.110429},
	abstract = {Code clones, identical or nearly similar code fragments in a software system’s code-base, have mixed impacts on software evolution and maintenance. Focusing on the issues of clones researchers suggest managing them through refactoring, and tracking. In this paper we present a survey on the state-of-the-art of clone refactoring and tracking techniques, and identify future research possibilities in these areas. We define the quality assessment features for the clone refactoring and tracking tools, and make a comparison among these tools considering these features. To the best of our knowledge, our survey is the first comprehensive study on clone refactoring and tracking. According to our survey on clone refactoring we realize that automatic refactoring cannot eradicate the necessity of manual effort regarding finding refactoring opportunities, and post refactoring testing of system behaviour. Post refactoring testing can require a significant amount of time and effort from the quality assurance engineers. There is a marked lack of research on the effect of clone refactoring on system performance. Future investigations in this direction will add much value to clone refactoring research. We also feel the necessity of future research towards real-time detection, and tracking of code clones in a big-data environment.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
	month = jan,
	year = {2020},
	keywords = {Clone refactoring, Clone tracking, Clone-types, Code clones},
	pages = {110429},
}

@article{martinez_model-based_2017,
	title = {Model-based analysis of {Java} {EE} web security misconfigurations},
	volume = {49},
	issn = {1477-8424},
	url = {https://www.sciencedirect.com/science/article/pii/S1477842416301348},
	doi = {10.1016/j.cl.2017.02.001},
	abstract = {The Java EE framework, a popular technology of choice for the development of web applications, provides developers with the means to define access-control policies to protect application resources from unauthorized disclosures and manipulations. Unfortunately, the definition and manipulation of such security policies remains a complex and error prone task, requiring expert-level knowledge on the syntax and semantics of the Java EE access-control mechanisms. Thus, misconfigurations that may lead to unintentional security and/or availability problems can be easily introduced. In response to this problem, we present a (model-based) reverse engineering approach that automatically evaluates a set of security properties on reverse engineered Java EE security configurations, helping to detect the presence of anomalies. We evaluate the efficacy and pertinence of our approach by applying our prototype tool on a sample of real Java EE applications extracted from GitHub.},
	urldate = {2024-08-06},
	journal = {Computer Languages, Systems \& Structures},
	author = {Martínez, Salvador and Cosentino, Valerio and Cabot, Jordi},
	month = sep,
	year = {2017},
	keywords = {Security, Model-driven engineering, Reverse-engineering},
	pages = {36--61},
	file = {Versão submetida:files/1258/Martínez et al. - 2017 - Model-based analysis of Java EE web security misco.pdf:application/pdf},
}

@incollection{abdelrazek_chapter_2017,
	address = {Boston},
	title = {Chapter 5 - {Adaptive} {Security} for {Software} {Systems}},
	isbn = {978-0-12-802855-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128028551000058},
	abstract = {With continuously changing operational and business needs, system security is one of the key system capabilities that need to be updated as well. Most security engineering efforts focus on engineering security requirements of software systems at design time and existing adaptive security engineering efforts require complex design-time preparation. In this chapter we discuss the needs for adaptive software security, and key efforts in this area. We then introduce a new runtime adaptive security engineering approach, which enables adapting software security capabilities at runtime based on new security objectives, risks/threats, requirements as well as newly reported vulnerabilities. We categorize the source of adaptation in terms of manual adaptation (managed by end users), and automated adaption (automatically triggered by the platform). The new platform makes use of new ideas we built for vulnerability analysis, security engineering using aspect-oriented programming, and model-driven engineering techniques.},
	urldate = {2024-08-06},
	booktitle = {Managing {Trade}-{Offs} in {Adaptable} {Software} {Architectures}},
	publisher = {Morgan Kaufmann},
	author = {Abdelrazek, M. and Grundy, J. and Ibrahim, A.},
	editor = {Mistrik, Ivan and Ali, Nour and Kazman, Rick and Grundy, John and Schmerl, Bradley},
	month = jan,
	year = {2017},
	doi = {10.1016/B978-0-12-802855-1.00005-8},
	keywords = {Vulnerability analysis, Adaptive security, Security analysis, Security engineering, User-driven security adaptation},
	pages = {99--127},
}

@article{pottier_household_2023,
	title = {Household resilience to slow onset flooding: {A} study of evacuation decision triggers in high-rise buildings along the {Seine} in {Paris}},
	volume = {95},
	issn = {2212-4209},
	shorttitle = {Household resilience to slow onset flooding},
	url = {https://www.sciencedirect.com/science/article/pii/S2212420923003382},
	doi = {10.1016/j.ijdrr.2023.103858},
	abstract = {Are high-rise residents prepared to evacuate on their own in the event of a major flood that result in prolonged shutdown of utility networks? If a slow-moving flood of the Seine affecting Paris and its suburbs is predicted, crisis management services plan for preventive power cuts, evacuation of high-rise buildings, and halting of public transportation. This article explores the triggers for household evacuations through a survey of 533 households in 11 high-rise buildings located along the Seine River in Paris. Three main factors were studied: the ability to evacuate oneself, to temporarily relocate and to reach this temporary relocation by one's own means. The triggers for the decision to evacuate, for 7 types of households interviewed according to their willingness and ability to evacuate, reveal that most respondents are partially dependent on public authorities due to the lack of housing alternatives. Those who can evacuate preemptively attach great importance to receiving warning messages and their accuracy in deciding whether to evacuate. Many are unfamiliar with the consequences of prolonged urban power outages on their daily lives and the need for preemptive evacuation. These results show that more targeted information based on each household's profile could facilitate preventive evacuation. This would reduce people's dependence on the community and avoid cascading effects that could worsen crisis management. This pilot survey database is needed to calibrate agent-based simulation models that generate reliable and well-informed predictions for evacuation of high-rise buildings in the event of slow flooding and shutdown of urban service networks.},
	urldate = {2024-08-06},
	journal = {International Journal of Disaster Risk Reduction},
	author = {Pottier, Nathalie and Vuillet, Marc and Rabemalanto, Nathalie and Edjossan-Sossou, Abla Mimi},
	month = sep,
	year = {2023},
	keywords = {Survey, High-rise building, Household resilience, Large scale flood, Paris, Preventive evacuation, Triggers},
	pages = {103858},
}

@incollection{conrad_chapter_2012,
	address = {Boston},
	title = {Chapter 5 - {Domain} 4: {Software} {Development} {Security}},
	isbn = {978-1-59749-961-3},
	shorttitle = {Chapter 5 - {Domain} 4},
	url = {https://www.sciencedirect.com/science/article/pii/B9781597499613000054},
	urldate = {2024-08-06},
	booktitle = {{CISSP} {Study} {Guide} ({Second} {Edition})},
	publisher = {Syngress},
	author = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	editor = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	month = jan,
	year = {2012},
	doi = {10.1016/B978-1-59749-961-3.00005-4},
	pages = {169--211},
}

@article{nam_bug_2019-1,
	title = {A bug finder refined by a large set of open-source projects},
	volume = {112},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584919300977},
	doi = {10.1016/j.infsof.2019.04.014},
	abstract = {Context
Static bug detection techniques are commonly used to automatically detect software bugs. The biggest obstacle to the wider adoption of static bug detection tools is false positives, i.e., reported bugs that developers do not have to act on.
Objective
The objective of this study is to reduce false positives resulting from static bug detection tools and to detect new bugs by exploring the effectiveness of a feedback-based bug detection rule design.
Method
We explored a large number of software projects and applied an iterative feedback-based process to design bug detection rules. The outcome of the process is a set of ten bug detection rules, which we used to build a feedback-based bug finder, FeeFin. Specifically, we manually examined 1622 patches to identify bugs and fix patterns, and implement bug detection rules. Then, we refined the rules by repeatedly using feedback from a large number of software projects.
Results
We applied FeeFin to the latest versions of the 1880 projects on GitHub to detect previously unknown bugs. FeeFin detected 98 new bugs, 63 of which have been reviewed by developers: 57 were confirmed as true bugs, and 9 were confirmed as false positives. In addition, we investigated the benefits of our FeeFin process in terms of new and improved bug patterns. We verified our bug patterns with four existing tools, namely PMD, FindBugs, Facebook Infer, and Google Error Prone, and found that our FeeFin process has the potential to identify new bug patterns and also to improve existing bug patterns.
Conclusion
Based on the results, we suggest that static bug detection tool designers identify new bug patterns by mining real-world patches from a large number of software projects. In addition, the FeeFin process is helpful in mitigating false positives generated from existing tools by refining their bug detection rules.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Nam, Jaechang and Wang, Song and Xi, Yuan and Tan, Lin},
	month = aug,
	year = {2019},
	keywords = {bug detection rules, bug patterns, Static bug finder},
	pages = {164--175},
}

@incollection{conrad_chapter_2016,
	address = {Boston},
	title = {Chapter 9 - {Domain} 8: {Software} {Development} {Security} ({Understanding}, {Applying}, and {Enforcing} {Software} {Security})},
	isbn = {978-0-12-802437-9},
	shorttitle = {Chapter 9 - {Domain} 8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128024379000096},
	abstract = {Chapter 9 introduces Domain 8 of the CISSP, Software Development Security. The most important aspects of this domain are related to managing the development of software and applications. Approaches to software development that attempt to reduce the likelihood of defects or flaws are a key topic in this domain. In particular, the Waterfall, Spiral, and Rapid Application Development (RAD) models of the software development are considered. Another significant portion of this chapter is dedicated to understanding the principles of Object Oriented programming and design. A basic discussion of several types of software vulnerabilities and the issues surrounding disclosure of the vulnerabilities are also a topic for this domain. Finally, databases, being a key component of many applications, are considered.},
	urldate = {2024-08-06},
	booktitle = {{CISSP} {Study} {Guide} ({Third} {Edition})},
	publisher = {Syngress},
	author = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	editor = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	month = jan,
	year = {2016},
	doi = {10.1016/B978-0-12-802437-9.00009-6},
	keywords = {Extreme Programming, Object, Object-Oriented Programming, Procedural languages, Spiral Model, Systems Development Life Cycle, Waterfall Model},
	pages = {429--477},
}

@article{krishnamoorthy_deployment_2021,
	series = {International {Conference} on {Advances} in {Materials} {Research} - 2019},
	title = {Deployment of {IoT} for smart home application and embedded real-time control system},
	volume = {45},
	issn = {2214-7853},
	url = {https://www.sciencedirect.com/science/article/pii/S2214785320394177},
	doi = {10.1016/j.matpr.2020.11.741},
	abstract = {Expeditious enhancements in technology and consumer electronics push towards the unavoidable usage of intelligent devices in everyday life. These smart devices play a vital role in enhancing human comfort and ensures better standard of living. The refrigerator is one of primary appliance that is utilized in everyday life where many food items and grocery things are placed. The items that are placed inside the refrigerator need to be monitored for better utilization and also to avoid the wastage. This work aims to convert the traditional refrigerators to smart refrigerator that detects the shortage or spoilage of items using the item information from the database created. The proposed system automatically control the refrigerator temperature according to varying conditions using thermostat control and provides the motorized control of the door for energy saving and efficient cooling. The system also provide motorized control of the door for energy saving and efficient cooling. This internet of things (IoT) based system allows to monitor the food items that are placed inside the refrigerator and utilization of other grocery items before the expiry date. In addition to, this works also aims to place the online order of expired items by providing the app notification and also sends a short message service (SMS) text message to the user smart phone. We have identified and loaded the items available online search engines in our database. Based on the user input, this system will identify the link with less cost and expected delivery time of the needed items, therefore the user can select ordering of items accordingly. The characteristics of utilized components and materials are presented.},
	urldate = {2024-08-06},
	journal = {Materials Today: Proceedings},
	author = {Krishnamoorthy, Ramesh and Krishnan, Kalimuthu and Bharatiraja, C.},
	month = jan,
	year = {2021},
	keywords = {Internet of things, Embedded systems, Sensor and device materials, Smart refrigerator},
	pages = {2777--2783},
}

@article{alfayez_how_2023-1,
	title = {How {SonarQube}-identified technical debt is prioritized: {An} exploratory case study},
	volume = {156},
	issn = {0950-5849},
	shorttitle = {How {SonarQube}-identified technical debt is prioritized},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584923000010},
	doi = {10.1016/j.infsof.2023.107147},
	abstract = {Context:
Repaying all technical debt (TD) in a system may be unviable, as there is typically a shortage of resources allocated for TD repayment activities. Therefore, TD prioritization is essential to best allocate such limited resources. Fortunately, one can utilize a static code analysis tool, such as SonarQube, to aid in expediting the TD prioritization process.
Objective:
Given that SonarQube is one of the most utilized tools in the context of TD, this exploratory case study seeks to explore how SonarQube-identified TD items are perceived and prioritized for repayment.
Methods:
The study was designed, replicated, and conducted in four companies and a master’s level course, with a total of 89 participants. The participants were requested to select TD items to include for repayment under a resources constraint.
Results:
The results revealed that the overwhelming majority of participants prioritized TD by factoring in a TD item’s value and cost, a smaller number prioritized higher value TD items, and only one participant prioritized lower cost TD items. Furthermore, it was revealed that the value of a TD item is subjective and context-dependent, and the majority of participants perceive the cost estimations provided by SonarQube for repaying TD items to be reliable and trustworthy when prioritizing TD.
Conclusion:
Based on the results, one can conclude that there is no silver bullet TD prioritization approach that addresses all of a developer’s objectives and needs. New TD prioritization approaches should be designed without concentrating on a specific prioritization perspective and should be independent of value estimation methods.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Alfayez, Reem and Winn, Robert and Alwehaibi, Wesam and Venson, Elaine and Boehm, Barry},
	month = apr,
	year = {2023},
	keywords = {Technical debt, Case study, Software, SonarQube, Technical debt prioritization},
	pages = {107147},
}

@article{kester_s_ong_service_2024,
	title = {Service quality and customer satisfaction analysis among motorcycle taxi transportation in the {Philippines} through {SERVQUAL} dimensions and social exchange theory},
	volume = {15},
	issn = {2213-624X},
	url = {https://www.sciencedirect.com/science/article/pii/S2213624X23001931},
	doi = {10.1016/j.cstp.2023.101139},
	abstract = {With the rise of different transportation modalities worldwide, companies have gained the ability to consider different aspects of moving or delivering. Motorcycle taxis or MTHS have been widely available but has little literatures available when it comes to their services. This research employed Structural Equation Modelling to examine the service quality and customer satisfaction of MTHS in the Philippines. An integrated Social Exchange Theory (SET) and SERVQUAL 5 dimensions were employed with 1037 valid respondents and the causal analysis showed how trust on service quality was the most influential factor affecting customer satisfaction after service quality on customer satisfaction. Economic benefit, traffic management, and accessibility were deemed significant under SET, while tangibles, empathy, reliability, and assurance were significant under the SERVQUAL domains. It was indicating that MTHS in the Philippines as a whole, provides good service, provides excellent value for money, and have less concerns compared to other public transportation aside from safety concerns. In addition, it was indicated from the findings that drivers of MTHS have good manners, well trained, and that the transportation service can get them to designated places. Assessing consumer satisfaction may improve the overall business and country transportation through economic impact by the increase in customer satisfaction and continuous utilization. The output of this study can be therefore applied and even extended among transportation services worldwide.},
	urldate = {2024-08-06},
	journal = {Case Studies on Transport Policy},
	author = {Kester S. Ong, Ardvin and German, Josephine D. and Dangaran, Pauline C. and Jethro B. Paz, Johannes and Roniel G. Macatangay, Renz},
	month = mar,
	year = {2024},
	keywords = {Customer satisfaction, Motorcycle taxis, Service Quality, SERVQUAL, Social exchange theory},
	pages = {101139},
}

@incollection{hamilton_chapter_2023,
	series = {Woodhead {Publishing} {Series} in {Food} {Science}, {Technology} and {Nutrition}},
	title = {Chapter 16 - {Natural} {Language} {Processing}},
	isbn = {978-0-12-821936-2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128219362000042},
	abstract = {Sensory evaluation is predicated on the use and interpretation of human language. We ask our subjects to describe their sensory experiences and affective responses, which we cannot directly observe. This formulation of sensory science encourages direct engagement with linguistics and in particular, a recent subfield of linguistics, computer science, and artificial intelligence called “Natural Language Processing” (NLP, sometimes “computational linguistics”). In this chapter we will provide an introduction to Natural Language Processing (NLP) for sensory scientists who wish to employ NLP as a rapid method for sensory evaluation. Because NLP is a large, diverse, and rapidly evolving field, we will begin with a brief, pragmatic overview of the discipline, with an emphasis on key historical and current methods and applications. We will then briefly discuss the linguistic perspective and its application to sensory evaluation, with an aim to motivating the remaining chapter. Following that, we will discuss key areas of NLP, from data collection to processing to analysis to advanced applications. Throughout the chapter, we will use a consistent case study of natural-language descriptions for a food product to provide examples and illustrate NLP methods.},
	urldate = {2024-08-06},
	booktitle = {Rapid {Sensory} {Profiling} {Techniques} ({Second} {Edition})},
	publisher = {Woodhead Publishing},
	author = {Hamilton, Leah Marie and Lahne, Jacob},
	editor = {Delarue, Julien and Lawlor, J. Ben},
	month = jan,
	year = {2023},
	doi = {10.1016/B978-0-12-821936-2.00004-2},
	keywords = {Deep learning, Machine learning, Computational linguistics, Descriptive analysis, Natural Language Processing, Sensory evaluation, Text analysis},
	pages = {371--410},
}

@article{caldeira_unveiling_2022-1,
	title = {Unveiling process insights from refactoring practices},
	volume = {81},
	issn = {0920-5489},
	url = {https://www.sciencedirect.com/science/article/pii/S0920548921000829},
	doi = {10.1016/j.csi.2021.103587},
	abstract = {Context: Software comprehension and maintenance activities, such as refactoring, are said to be negatively impacted by software complexity. The methods used to measure software product and processes complexity have been thoroughly debated in the literature. However, the discernment about the possible links between these two dimensions, particularly on the benefits of using the process perspective, has a long journey ahead. Objective: To improve the understanding of the liaison of developers’ activities and software complexity within a refactoring task, namely by evaluating if process metrics gathered from the IDE, using process mining methods and tools, are suitable to accurately classify different refactoring practices and the resulting software complexity. Method: We mined source code metrics from a software product after a quality improvement task was given in parallel to (117) software developers, organized in (71) teams. Simultaneously, we collected events from their IDE work sessions (320) and used process mining to model their processes and extract the correspondent metrics. Results: Most teams using a plugin for refactoring (JDeodorant) reduced software complexity more effectively and with simpler processes than the ones that performed refactoring using only Eclipse native features. We were able to find moderate correlations (≈43\%) between software cyclomatic complexity and process cyclomatic complexity. Using only process driven metrics, we computed ≈30,000 models aiming to predict the type of refactoring method (automatic or manual) teams had used and the expected level of software cyclomatic complexity reduction after their work sessions. The best models found for the refactoring method and cyclomatic complexity level predictions, had an accuracy of 92.95\% and 94.36\%, respectively. Conclusions: We have demonstrated the feasibility of an approach that allows building cross-cutting analytical models in software projects, such as the one we used for detecting manual or automatic refactoring practices. Events from the development tools and support activities can be collected, transformed, aggregated, and analyzed with fewer privacy concerns or technical constraints than source code-driven metrics. This makes our approach agnostic to programming languages, geographic location, or development practices, making it suitable for challenging contexts, such as, in modern global software development where many projects adopt agile methodologies, and low/no code platforms. Initial findings are encouraging, and lead us to suggest practitioners may use our method in other development tasks, such as, defect analysis and unit or integration tests.},
	urldate = {2024-08-06},
	journal = {Computer Standards \& Interfaces},
	author = {Caldeira, João and Brito e Abreu, Fernando and Cardoso, Jorge and dos Reis, José Pereira},
	month = apr,
	year = {2022},
	keywords = {Refactoring practices, Software complexity, Software development process mining, Software process complexity},
	pages = {103587},
	file = {Versão aceita:files/1259/Caldeira et al. - 2022 - Unveiling process insights from refactoring practi.pdf:application/pdf},
}

@article{escolar_multiple-attribute_2019,
	series = {Understanding {Smart} {Cities}: {Innovation} ecosystems, technological advancements, and societal challenges},
	title = {A {Multiple}-{Attribute} {Decision} {Making}-based approach for smart city rankings design},
	volume = {142},
	issn = {0040-1625},
	url = {https://www.sciencedirect.com/science/article/pii/S0040162517318437},
	doi = {10.1016/j.techfore.2018.07.024},
	abstract = {Rankings are a valuable element for city-comparison purposes since results withdrawn from these comparisons can, eventually, support the evaluation of strategic decisions taken by cities. Smart city rankings are not an exception and, as they draw more attention, the number of them exponentially increases. This paper evaluates the appropriateness of existing smart city rankings for quantifying the materialization degree of the smart city concept. The analysis reveals that current rankings generally overlook indicators of the Information and Communication Technologies dimension. To bridge this gap, this work proposes a methodology based on Multiple-Attribute Decision Making that uses technological criteria for designing smart city rankings. The proposed methodology is evaluated against the cities of New York, Seoul, and Santander. Imbalances between results provided by the studied rankings and our evaluation are detected, which suggests the need for a new insight into more suitable and precise evaluation of the smartness degree of cities.},
	urldate = {2024-08-06},
	journal = {Technological Forecasting and Social Change},
	author = {Escolar, Soledad and Villanueva, Félix J. and Santofimia, Maria J. and Villa, David and Toro, Xavier del and López, Juan Carlos},
	month = may,
	year = {2019},
	keywords = {Internet of Things, City indicators, City rankings, Information and Communication Technologies, Smart cities},
	pages = {42--55},
}

@article{carruthers_longitudinal_2024,
	title = {A longitudinal study on the temporal validity of software samples},
	volume = {168},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584924000090},
	doi = {10.1016/j.infsof.2024.107404},
	abstract = {Context
In Empirical Software Engineering, it is crucial to work with representative samples that reflect the current state of the software industry. An important consideration, especially in rapidly changing fields like software development, is that if we use a sample collected years ago, it should continue to represent the same population in the present day to produce generalizable results. However, it is seldom the case in which a software sample built several years ago accurately depicts the current state of the development industry. Nevertheless, many recent studies rely on rather old datasets (seven or more years of age) to conduct their investigations.
Objective
To analyze the evolution of a population of open-source projects, determine the likelihood of detecting significant differences over time, and study the activity history of the projects.
Method
We performed a longitudinal study with 72 snapshots of quality projects from Github, covering the period between July 1st 2017 and June 1st 2023. We recorded monthly values of seven repository metrics (contributors, commits, closed pull-requests, merged pull-requests, closed issues, number of stars and forks), encompassing data from a total of 1991 repositories.
Results
We observed significant changes in all the metrics evaluated, with most cases showing negligible to small effect sizes. Notably, merged pull-requests registered medium effect sizes. The evolution was not equal in all the metrics, however, after five years it was unlikely that a sample of projects remained representative for any of the analyzed metrics, showing probabilities below 25\%.
Conclusion
Although the temporal validity of a sample depends on the specific data being studied, employing datasets created several years ago does not appear to be a sound strategy if the aim is to produce results that can be extrapolated to the current state of the population.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Carruthers, Juan Andrés and Diaz-Pace, Jorge Andrés and Irrazábal, Emanuel},
	month = apr,
	year = {2024},
	keywords = {Longitudinal study, Sample evolution, Software samples, Temporal validity},
	pages = {107404},
}

@incollection{furda_chapter_2017,
	address = {Boston},
	title = {Chapter 13 - {Reengineering} {Data}-{Centric} {Information} {Systems} for the {Cloud} – {A} {Method} and {Architectural} {Patterns} {Promoting} {Multitenancy}},
	isbn = {978-0-12-805467-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000132},
	abstract = {Enterprise applications are data-centric information systems that are being increasingly deployed as Software-as-a-Service (SaaS) Cloud offerings. Such service-oriented enterprise applications allow multiple tenants (i.e., groups of service consumers) to share the computational and storage capabilities of a single Cloud application instance. Compared to a more traditional single-tenant application deployment model, a multitenant SaaS architecture promises to lower both deployment and maintenance costs. Such cost reductions motivate architects to reengineer existing enterprise applications to support multitenancy at the application level. However, in order to preserve data integrity and data confidentiality, the reengineering process must guarantee that different tenants allocated to the same application instance cannot access one another's data, including both persistent values stored in databases and transient values created during calculations. This chapter presents a method and a set of architectural patterns for systematically reengineering data-sensitive enterprise applications into secure multitenant software services that can be deployed to public and private cloud offerings seamlessly. Architectural refactoring is introduced as a novel reengineering practice and the necessary steps in multitenant refactoring are described from planning to execution to validation (including testing and code reviews). The method and patterns are validated in a fictitious, but realistic and representative case study that was distilled from real-world requirements and application architectures.},
	urldate = {2024-08-06},
	booktitle = {Software {Architecture} for {Big} {Data} and the {Cloud}},
	publisher = {Morgan Kaufmann},
	author = {Furda, Andrei and Fidge, Colin and Barros, Alistair and Zimmermann, Olaf},
	editor = {Mistrik, Ivan and Bahsoon, Rami and Ali, Nour and Heisel, Maritta and Maxim, Bruce},
	month = jan,
	year = {2017},
	doi = {10.1016/B978-0-12-805467-3.00013-2},
	keywords = {Testing, Security, Architectural patterns, Architectural refactoring, Cloud computing, Reengineering, Software evolution and maintenance},
	pages = {227--251},
}

@article{zaytsev_grammar_2015-1,
	series = {Fifth issue of {Experimental} {Software} and {Toolkits} ({EST}): {A} special issue on {Academics} {Modelling} with {Eclipse} ({ACME2012})},
	title = {Grammar {Zoo}: {A} corpus of experimental grammarware},
	volume = {98},
	issn = {0167-6423},
	shorttitle = {Grammar {Zoo}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167642314003347},
	doi = {10.1016/j.scico.2014.07.010},
	abstract = {In this paper we describe composition of a corpus of grammars in a broad sense in order to enable reuse of knowledge accumulated in the field of grammarware engineering. The Grammar Zoo displays the results of grammar hunting for big grammars of mainstream languages, as well as collecting grammars of smaller DSLs and extracting grammatical knowledge from other places. It is already operational and publicly supplies its users with grammars that have been recovered from different sources of grammar knowledge, varying from official language standards to community-created wiki pages. We summarise recent achievements in the discipline of grammarware engineering, that made the creation of such a corpus possible. We also describe in detail the technology that is used to build and extend such a corpus. The current contents of the Grammar Zoo are listed, as well as some possible future uses for them.},
	urldate = {2024-08-06},
	journal = {Science of Computer Programming},
	author = {Zaytsev, Vadim},
	month = feb,
	year = {2015},
	keywords = {Curated corpus, Experimental infrastructure, Grammar recovery, Grammarware engineering},
	pages = {28--51},
}

@article{garousi_model-based_2021-1,
	title = {Model-based testing in practice: {An} experience report from the web applications domain},
	volume = {180},
	issn = {0164-1212},
	shorttitle = {Model-based testing in practice},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221001291},
	doi = {10.1016/j.jss.2021.111032},
	abstract = {In the context of a software testing company, we have deployed the model-based testing (MBT) approach to take the company’s test automation practices to higher levels of maturity and capability. We have chosen, from a set of open-source/commercial MBT tools, an open-source tool named GraphWalker, and have pragmatically used MBT for end-to-end test automation of several large web and mobile applications under test. The MBT approach has provided, so far in our project, various tangible and intangible benefits in terms of improved test coverage (number of paths tested), improved test-design practices, and also improved real-fault detection effectiveness. The goal of this experience report (applied research report), done based on “action research”, is to share our experience of applying and evaluating MBT as a software technology (technique and tool) in a real industrial setting. We aim at contributing to the body of empirical evidence in industrial application of MBT by sharing our industry-academia project on applying MBT in practice, the insights that we have gained, and the challenges and questions that we have faced and tackled so far. We discuss an overview of the industrial setting, provide motivation, explain the events leading to the outcomes, discuss the challenges faced, summarize the outcomes, and conclude with lessons learned, take-away messages, and practical advices based on the described experience. By learning from the best practices in this paper, other test engineers could conduct more mature MBT in their test projects.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Garousi, Vahid and Keleş, Alper Buğra and Balaman, Yunus and Güler, Zeynep Özdemir and Arcuri, Andrea},
	month = oct,
	year = {2021},
	keywords = {Web applications, Software testing, Applied research report, Experience report, Model-based testing, Test automation},
	pages = {111032},
	file = {Texto completo:files/1260/Garousi et al. - 2021 - Model-based testing in practice An experience rep.pdf:application/pdf},
}

@article{boccalini_gender_2022,
	title = {Gender differences in dopaminergic system dysfunction in de novo {Parkinson}'s disease clinical subtypes},
	volume = {167},
	issn = {0969-9961},
	url = {https://www.sciencedirect.com/science/article/pii/S0969996122000596},
	doi = {10.1016/j.nbd.2022.105668},
	abstract = {Parkinson's disease (PD) is characterized by heterogeneity in clinical syndromes, prognosis, and pathophysiology mechanisms. Gender differences in neural anatomy and function are emerging as fundamental determinants of phenotypic variability. Different clinical subtypes, defined as mild motor predominant, intermediate, and diffuse-malignant, have been recently proposed in PD. This study investigated gender influence on clinical features, dopaminergic dysfunction, and connectivity in patients with de novo idiopathic PD stratified according to the clinical criteria for subtypes (i.e., mild motor, intermediate, and diffuse-malignant). We included 286 drug-naïve patients (Males/Females: 189/97, age [mean ± standard deviation]: 61.99 ± 9.67; disease duration: 2.08 ± 2.21) with available [123I]FP-CIT-SPECT and high-resolution T1-weighted MRI from the Parkinson's Progression Markers Initiative. We assessed gender differences for clinical and cognitive features, and dopaminergic presynaptic dysfunction in striatal or extra-striatal regions using molecular analysis of [123I]FP-CIT-bindings. We applied an advanced multivariate analytical approach – partial correlations molecular connectivity analyses – to assess potential gender differences in the vulnerability of the nigrostriatal and mesolimbic dopaminergic pathways. In the mild motor and intermediate subtypes, male patients with idiopathic PD showed poorer cognitive performances than females, who – in contrast – presented more severe anxiety symptoms. The male vulnerability emerged also in the motor system in the same subtypes with motor impairment associated with a lower dopamine binding in the putamen and more severe widespread connectivity alterations in the nigrostriatal dopaminergic pathway in males than in females. In the diffuse-malignant subtype, males showed more severe motor impairments, consistent with a lower dopamine uptake in the putamen than females. On the other hand, a severe dopaminergic depletion in several dopaminergic targets of the mesolimbic pathway, together with extensive altered connectivity in the same system, characterized females with idiopathic PD in all the subtypes. The anxiety level was associated with a lower dopaminergic binding in the amygdala only in females. This study provides evidence on gender differences in idiopathic PD across clinical subtypes, and, remarkably, since the early phase. The clinical correlations with the nigrostriatal or mesolimbic systems in males and females support different vulnerabilities and related disease expressions. Gender differences must be considered in a precision medicine approach to preventing, diagnosing, and treating idiopathic PD.},
	urldate = {2024-08-06},
	journal = {Neurobiology of Disease},
	author = {Boccalini, Cecilia and Carli, Giulia and Pilotto, Andrea and Padovani, Alessandro and Perani, Daniela},
	month = jun,
	year = {2022},
	keywords = {Gender, Molecular connectivity, [123I]FP-CIT SPECT, Dopaminergic pathways, Idiopathic Parkinson's disease subtypes},
	pages = {105668},
}

@incollection{ducasse_foreword_2015,
	address = {Boston},
	title = {Foreword by {Dr}. {Stéphane} {Ducasse}},
	isbn = {978-0-12-801397-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128013977060021},
	urldate = {2024-08-06},
	booktitle = {Refactoring for {Software} {Design} {Smells}},
	publisher = {Morgan Kaufmann},
	author = {Ducasse, Stéphane},
	editor = {Suryanarayana, Girish and Samarthyam, Ganesh and Sharma, Tushar},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801397-7.06002-1},
	pages = {xi},
}

@article{lomio_just--time_2022-1,
	title = {Just-in-time software vulnerability detection: {Are} we there yet?},
	volume = {188},
	issn = {0164-1212},
	shorttitle = {Just-in-time software vulnerability detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121222000437},
	doi = {10.1016/j.jss.2022.111283},
	abstract = {Background:
Software vulnerabilities are weaknesses in source code that might be exploited to cause harm or loss. Previous work has proposed a number of automated machine learning approaches to detect them. Most of these techniques work at release-level, meaning that they aim at predicting the files that will potentially be vulnerable in a future release. Yet, researchers have shown that a commit-level identification of source code issues might better fit the developer’s needs, speeding up their resolution.
Objective:
To investigate how currently available machine learning-based vulnerability detection mechanisms can support developers in the detection of vulnerabilities at commit-level.
Method:
We perform an empirical study where we consider nine projects accounting for 8991 commits and experiment with eight machine learners built using process, product, and textual metrics.
Results:
We point out three main findings: (1) basic machine learners rarely perform well; (2) the use of ensemble machine learning algorithms based on boosting can substantially improve the performance; and (3) the combination of more metrics does not necessarily improve the classification capabilities.
Conclusion:
Further research should focus on just-in-time vulnerability detection, especially with respect to the introduction of smart approaches for feature selection and training strategies.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Lomio, Francesco and Iannone, Emanuele and De Lucia, Andrea and Palomba, Fabio and Lenarduzzi, Valentina},
	month = jun,
	year = {2022},
	keywords = {Software vulnerabilities, Machine learning, Empirical SE},
	pages = {111283},
	file = {Texto completo:files/1261/Lomio et al. - 2022 - Just-in-time software vulnerability detection Are.pdf:application/pdf},
}

@incollection{ghiabi_5_2020,
	title = {5 - {A} {History} of the {Present} on {Drugs}: {Opium}, {Heroin} and {Methamphetamine}},
	isbn = {978-1-78548-317-2},
	shorttitle = {5 - {A} {History} of the {Present} on {Drugs}},
	url = {https://www.sciencedirect.com/science/article/pii/B9781785483172500052},
	abstract = {What does the history of drug use tell us about the lives of Iranians and their place in the world? In this context, drugs are a historical microcosm, a microphenomenon, where the human, social and political trajectories of a population can be understood. Drug users are nameless people in modern history; nevertheless, elites and the State have systematically referred to them in the political game to discipline, modernize, and bring order to society. Therefore, the little rhapsodies and disruptions that characterize the history of drugs in Iran (and, in fact, also elsewhere) narrate a changing world and a new flow of time. This is what the poet and intellectual Pier Paolo Pasolini defined as the “anthropological mutation” of the consumer society (civiltà dei consumi). Pasolini mentioned this in the particular case of the Italian rural world, which proved to be radically transformed after the Second World War and the socio-economic change of the 1950s and 1960s. The fantasies, desires, language, fears and loves of common people, he argued, were irreparably transformed by the creation of a “consumer society”. He argued that this model of society, consumerism, was more totalitarian than fascism. In a similar spirit, I suggest that Iranian society underwent a profound anthropological mutation through the experience of consumption of intoxicants. Here, I am trying to study the micro-dimension of historical change through the prism of drug use, a prism that is particularly significant for Iranians.},
	urldate = {2024-08-06},
	booktitle = {Living with {Drugs}},
	publisher = {ISTE},
	author = {Ghiabi, Maziyar},
	editor = {Stella, Alessandro and Coppel, Anne},
	month = jan,
	year = {2020},
	doi = {10.1016/B978-1-78548-317-2.50005-2},
	keywords = {Ambivalence, Anthropology, Anti-narcotics strategy, Heroin, Intoxicating effects, Methamphetamine, Modernization, Opium, Poppy economy, Psychoactive revolution},
	pages = {51--73},
}

@article{baroroh_systematic_2021,
	title = {Systematic literature review on augmented reality in smart manufacturing: {Collaboration} between human and computational intelligence},
	volume = {61},
	issn = {0278-6125},
	shorttitle = {Systematic literature review on augmented reality in smart manufacturing},
	url = {https://www.sciencedirect.com/science/article/pii/S0278612520301862},
	doi = {10.1016/j.jmsy.2020.10.017},
	abstract = {Smart manufacturing offers a high level of adaptability and autonomy to meet the ever-increasing demands of product mass customization. Although digitalization has been used on the shop floor of modern factory for decades, some manufacturing operations remain manual and humans can perform these better than machines. Under such circumstances, a feasible solution is to have human operators collaborate with computational intelligence (CI) in real time through augmented reality (AR). This study conducts a systematic review of the recent literature on AR applications developed for smart manufacturing. A classification framework consisting of four facets, namely interaction device, manufacturing operation, functional approach, and intelligence source, is proposed to analyze the related studies. The analysis shows how AR has been used to facilitate various manufacturing operations with intelligence. Important findings are derived from a viewpoint different from that of the previous reviews on this subject. The perspective here is on how AR can work as a collaboration interface between human and CI. The outcome of this work is expected to provide guidelines for implementing AR assisted functions with practical applications in smart manufacturing in the near future.},
	urldate = {2024-08-06},
	journal = {Journal of Manufacturing Systems},
	author = {Baroroh, Dawi Karomati and Chu, Chih-Hsing and Wang, Lihui},
	month = oct,
	year = {2021},
	keywords = {Computational intelligence, Augmented reality, Industry 4.0, Smart manufacturing},
	pages = {696--711},
}

@incollection{conrad_chapter_2023,
	title = {Chapter 9 - {Domain} 8: {Software} {Development} {Security}},
	isbn = {978-0-443-18734-6},
	shorttitle = {Chapter 9 - {Domain} 8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780443187346000088},
	abstract = {This chapter introduces Domain 8 of the CISSP®, Software Development Security. The most important aspects of this domain are related to managing the development of software and applications. Approaches to software development that attempt to reduce the likelihood of defects or flaws are a key topic in this domain. In particular, the Waterfall, Spiral, and Rapid Application Development (RAD) models of software development are considered. Another significant portion of this chapter is dedicated to understanding the principles of Object-Oriented programming and design. A basic discussion of several types of software vulnerabilities and the issues surrounding disclosure of the vulnerabilities are also a topic for this domain. Finally, databases, being a key component of many applications, are considered.},
	urldate = {2024-08-06},
	booktitle = {{CISSP}® {Study} {Guide} ({Fourth} {Edition})},
	publisher = {Syngress},
	author = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	editor = {Conrad, Eric and Misenar, Seth and Feldman, Joshua},
	month = jan,
	year = {2023},
	doi = {10.1016/B978-0-443-18734-6.00008-8},
	keywords = {DevOps, Extreme Programming, Object, Object-Oriented Programming, Procedural languages, Spiral Model, Systems Development Life Cycle, Waterfall Model},
	pages = {459--508},
}

@article{rosa_comprehensive_2023-1,
	title = {A comprehensive evaluation of {SZZ} {Variants} through a developer-informed oracle},
	volume = {202},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223001243},
	doi = {10.1016/j.jss.2023.111729},
	abstract = {Automatically linking bug-fixing changes to bug-inducing ones (BICs) is one of the key data-extraction steps behind several empirical studies in software engineering. The SZZ algorithm is the de facto standard to achieve this goal, with several improvements proposed over time. Evaluating the performance of SZZ implementations is, however, far from trivial. In previous works, researchers (i) manually assessed whether the BICs identified by the SZZ implementation were correct or not, or (ii) defined oracles in which they manually determined BICs from bug-fixing commits. However, ideally, the original developers should be involved in defining a labeled dataset to evaluate SZZ implementations. We propose a methodology to define a “developer-informed” oracle for evaluating SZZ implementations, without requiring a manual inspection from the original developers. We use Natural Language Processing (NLP) to identify bug-fixing commits in which developers explicitly reference the commit(s) that introduced the fixed bug. We use the built oracle to extensively evaluate existing SZZ variants defined in the literature. We also introduce and evaluate two new variants aimed at addressing two weaknesses we observed in state-of-the-art implementations (i.e., processing added lines and handling of revert commits).},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Rosa, Giovanni and Pascarella, Luca and Scalabrino, Simone and Tufano, Rosalia and Bavota, Gabriele and Lanza, Michele and Oliveto, Rocco},
	month = aug,
	year = {2023},
	keywords = {Empirical study, SZZ, Defect prediction},
	pages = {111729},
}

@article{bansilan_descriptive_2024,
	title = {A descriptive analytics of the {COVID}-19 pandemic in a middle-income country with forward-looking insights},
	volume = {5},
	issn = {2772-4425},
	url = {https://www.sciencedirect.com/science/article/pii/S2772442524000224},
	doi = {10.1016/j.health.2024.100320},
	abstract = {The outbreak of COVID-19 unleashed an unprecedented global pandemic, profoundly impacting lives and economies worldwide. Recognizing its severity, the World Health Organization (WHO) swiftly declared it a public health emergency of international concern. In response to this crisis, collaborative efforts have been underway to control the disease and minimize its health and socio-economic impacts worldwide. The COVID-19 epidemic curve holds vital insights into the history of exposure, transmission, testing, tracing, social distancing measures, community lockdowns, quarantine, isolation, and treatment, offering a comprehensive perspective on the nation’s response. One approach to gaining crucial insights is through meticulous analysis of available datasets, empowering us to effectively inform future strategies and responses. This study aims to provide descriptive data analytics of the COVID-19 pandemic in the Philippines, summarizing the country’s fight by visualizing epidemiological and mobility datasets, revisiting scientific papers and news articles, and creating a timeline of the critical issues faced during the pandemic. By leveraging these multifaceted analyses, policymakers and health authorities can make informed decisions to enhance preparedness, expand inter-agency cooperation, and effectively combat future public health crises. This study seeks to serve as a valuable resource, guiding nations worldwide in comprehending and responding to the challenges posed by COVID-19 and beyond.},
	urldate = {2024-08-06},
	journal = {Healthcare Analytics},
	author = {Bansilan, Norvin P. and Rabajante, Jomar F.},
	month = jun,
	year = {2024},
	keywords = {COVID-19, Philippines, Descriptive analytics, Epidemic curve, Timeline, Visualization},
	pages = {100320},
}

@article{assuncao_how_2023-1,
	title = {How do microservices evolve? {An} empirical analysis of changes in open-source microservice repositories},
	volume = {204},
	issn = {0164-1212},
	shorttitle = {How do microservices evolve?},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121223001838},
	doi = {10.1016/j.jss.2023.111788},
	abstract = {Context.
Microservice architectures are an emergent service-oriented paradigm widely used in industry to develop and deploy scalable software systems. The underlying idea is to design highly independent services that implement small units of functionality and can interact with each other through lightweight interfaces.
Objective.
Even though microservices are often used with success, their design and maintenance pose novel challenges to software engineers. In particular, it is questionable whether the intended independence of microservices can actually be achieved in practice.
Method.
So, it is important to understand how and why microservices evolve during a system’s life-cycle, for instance, to scope refactorings and improvements of a system’s architecture or to develop supporting tools. To provide insights into how microservices evolve, we report a large-scale empirical study on the (co-)evolution of microservices in 11 open-source systems, involving quantitative and qualitative analyses of 7,319 commits.
Findings.
Our quantitative results show that there are recurring patterns of (co-)evolution across all systems, for instance, “shotgun surgery” commits and microservices that are largely independent, evolve in tuples, or are evolved in almost all changes. We refine our results by analyzing service-evolving commits qualitatively to explore the (in-)dependence of microservices and the causes for their specific evolution.
Conclusion.
The contributions in this article provide an understanding for practitioners and researchers on how microservices evolve in what way, and how microservice-based systems may be improved.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Assunção, Wesley K. G. and Krüger, Jacob and Mosser, Sébastien and Selaoui, Sofiane},
	month = oct,
	year = {2023},
	keywords = {Software evolution, Mining software repositories, Microservices, Service-oriented architecture, Software architecture},
	pages = {111788},
	file = {Texto completo:files/1262/Assunção et al. - 2023 - How do microservices evolve An empirical analysis.pdf:application/pdf},
}

@article{galich_categories_2015,
	series = {{XVth} {International} {Conference} "{Linguistic} and {Cultural} {Studies}: {Traditions} and {Innovations}"},
	title = {Categories of {Immaterial} {Objects} in {Mind} and {Language}},
	volume = {206},
	issn = {1877-0428},
	url = {https://www.sciencedirect.com/science/article/pii/S1877042815051484},
	doi = {10.1016/j.sbspro.2015.10.015},
	abstract = {Study of major ontological categories as a form of knowledge and finding the characteristics of their objectivation in language is one of the most promising areas of cognitive linguistics. It is related to the functions of categories in cognitive processes in general: their role in structuring and organizing knowledge, as well as in reflecting the significance of summarizing categorical concepts for the disclosure of the structure of the linguistic world view. The set of these categories is still not worked out, some of them must be first preliminary researched. Categories of immaterial objects, represented in German by the nouns «Eigenschaft» - «property», «Qualität»- «quality», «Merkmal» - «attribute», «feature», «sign», «Erscheinung» – «event» and «Zustand» - «state», which are dialectically interconnected and described in detail in the philosophical literature, are of interest to linguists, because the study of word combination nominating these categories enables the scientist to understand the problem of intercategorial relations in linguistic thinking.},
	urldate = {2024-08-06},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Galich, Galina and Shnyakina, Natalia},
	month = oct,
	year = {2015},
	keywords = {attribute, categorisation, Category, event, feature, immaterial objects, intercategorial relations, objectivation in German., property, state},
	pages = {30--35},
}

@article{v_group_2018,
	title = {Group decision-making in software architecture: {A} study on industrial practices},
	volume = {101},
	issn = {0950-5849},
	shorttitle = {Group decision-making in software architecture},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584918300740},
	doi = {10.1016/j.infsof.2018.04.009},
	abstract = {Context
A Software Architecture results from a comprehensive process in which several stakeholders deliberate upon the key requirements, issues, solutions and make architectural design decisions. Literature shows that most architectural decisions, in practice, are made in groups. Still, there is a limited understanding of industrial group decision-making practices in software architecture and the challenges that software architecture groups face.
Objective
Our study, by drawing inspiration from group decision-making theories and models, aims at understanding (i) Existing decision-making practices in software architecture groups (ii) the comparison between practice and theory, (iii) the challenges that the groups face, and (iv) the satisfaction of group members with various aspects of Group Decision Making.
Method
The study has been conducted through a questionnaire-based survey. 35 practitioners participated in this survey and the responses were analyzed qualitative and quantitatively.
Results
The analysis of individual responses reveal that software architecture groups (composed, on average, of 3–5 co-located or dispersed members) adopt a discussion based approach while evaluating alternatives, thereby lacking a structured way of decision-making. In these groups, despite the involvement of group members in the discussions, the final decision is made by an individual of authority. Not only is structured decision-making less common, the usage of dedicated software tools for decision-making too is rare. These groups face challenges that are indicative of Groupthink and Group Polarization. Group members feel that quantity of alternatives generated during discussions and tool availability are below satisfactory and they have low satisfaction with the tool support available.
Conclusion
This study has helped us develop an understanding of software architecture groups, their decision-making practices and challenges faced together with the satisfaction of group members. What the industry needs is integration of group decision-making principles into software architecture decision-making and design of decision-making tools that assist the architecture groups.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {V, Smrithi Rekha and Muccini, Henry},
	month = sep,
	year = {2018},
	keywords = {Software architecture, Architectural design decisions, Group decision making},
	pages = {51--63},
}

@article{dersot_how_2013,
	title = {How to recognize the eight signs of periodontitis?},
	volume = {11},
	issn = {1761-7227},
	url = {https://www.sciencedirect.com/science/article/pii/S176172271300020X},
	doi = {10.1016/j.ortho.2013.02.003},
	abstract = {Orthodontics contributes to improving self-esteem and function. However, a common refrain in ortho-perio relationships states that orthodontics has no deleterious effects on the healthy or reduced and treated periodontium. Though supported for decades, this idea has now been heavily challenged by a recent systematic review of the literature. The conclusion of this article is unfortunately very clear. There is a lack of reliable evidence showing the beneficial effects of orthodontic treatment on periodontal health with, at best, mild adverse effects. How can we reduce the periodontal cost of orthodontic treatment to a minimum? How can we ensure that this “at best” does not turn into “at worst”? To minimize the adverse effects of orthodontic treatment on periodontal tissues, the orthodontist must be able to determine to which patients he/she can consider providing orthodontic treatment and those for whom prior periodontal treatment is mandatory. In addition to the items collected for an orthodontic diagnosis, the orthodontist must be able to recognize the eight signs of attachment loss.},
	number = {2},
	urldate = {2024-08-06},
	journal = {International Orthodontics},
	author = {Dersot, Jean-Marc},
	month = jun,
	year = {2013},
	keywords = {Perio-ortho relationships},
	pages = {166--176},
}

@article{basciftci_expert_2018-1,
	title = {An expert system design to diagnose cancer by using a new method reduced rule base},
	volume = {157},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260717310210},
	doi = {10.1016/j.cmpb.2018.01.020},
	abstract = {Background and objectives
A Medical Expert System (MES) was developed which uses Reduced Rule Base to diagnose cancer risk according to the symptoms in an individual. A total of 13 symptoms were used. With the new MES, the reduced rules are controlled instead of all possibilities (213= 8192 different possibilities occur). By controlling reduced rules, results are found more quickly. The method of two-level simplification of Boolean functions was used to obtain Reduced Rule Base. Thanks to the developed application with the number of dynamic inputs and outputs on different platforms, anyone can easily test their own cancer easily.
Methods
More accurate results were obtained considering all the possibilities related to cancer. Thirteen different risk factors were determined to determine the type of cancer. The truth table produced in our study has 13 inputs and 4 outputs. The Boolean Function Minimization method is used to obtain less situations by simplifying logical functions. Diagnosis of cancer quickly thanks to control of the simplified 4 output functions.
Results
Diagnosis made with the 4 output values obtained using Reduced Rule Base was found to be quicker than diagnosis made by screening all 213= 8192 possibilities. With the improved MES, more probabilities were added to the process and more accurate diagnostic results were obtained. As a result of the simplification process in breast and renal cancer diagnosis 100\% diagnosis speed gain, in cervical cancer and lung cancer diagnosis rate gain of 99\% was obtained.
Conclusions
With Boolean function minimization, less number of rules is evaluated instead of evaluating a large number of rules. Reducing the number of rules allows the designed system to work more efficiently and to save time, and facilitates to transfer the rules to the designed Expert systems. Interfaces were developed in different software platforms to enable users to test the accuracy of the application. Any one is able to diagnose the cancer itself using determinative risk factors. Thereby likely to beat the cancer with early diagnosis.},
	urldate = {2024-08-06},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Başçiftçi, Fatih and Avuçlu, Emre},
	month = apr,
	year = {2018},
	keywords = {Cancer symptoms and types, Expert system, Minimization method, Mobile programming},
	pages = {113--120},
}

@article{xu_robotization_2023,
	title = {Robotization and intelligent digital systems in the meat cutting industry: {From} the perspectives of robotic cutting, perception, and digital development},
	volume = {135},
	issn = {0924-2244},
	shorttitle = {Robotization and intelligent digital systems in the meat cutting industry},
	url = {https://www.sciencedirect.com/science/article/pii/S0924224423001012},
	doi = {10.1016/j.tifs.2023.03.018},
	abstract = {Background
Meat, as protein-rich food, provides essential nutrition to humans. Meat consumption is expected to increase by 15\% over the next decade, requiring the meat industry to boost its capacity. However, the meat cutting industry's harsh working environment and long-term heavy workload result in a labor shortage. Low-margin and low capacity require innovative processing methods. Hence, some novel automation methods are urgently demanded to meet these challenges. Accompanied by the development of Industry 4.0, robotization and intelligent systems are progressively applied in the meat cutting industry, which can provide a flexible, adaptive, and sustainable processing approach.
Scope and approach
The paper selected 50 papers using the systematic literature review method. This paper provides an overview of the current research and commercial applications of cutting robotization from livestock, poultry, and fish in the meat industry. Additionally, intelligent sensing technologies, advanced cutting techniques, and a novel manufacturing concept called the meat factory cell, are emphasized to promote efficiency, scalability, and modularization. Finally, we discuss the potential applications of digital technologies in the meat cutting industry and the focal points of future research aiming to promote cutting robotization.
Key findings and conclusions
The meat cutting process can be highly robotized by integrating dexterous cutting robots, advanced sensing techniques, and digital systems, driving the transformation from manual labor to robotic, efficient, and intelligent manufacturing. Consequently, robotization and intelligent digital systems can provide a brand-new manufacturing method to the meat cutting industry and advance the Meat factory 4.0.},
	urldate = {2024-08-06},
	journal = {Trends in Food Science \& Technology},
	author = {Xu, Weidong and He, Yingchao and Li, Jiaheng and Zhou, Jianwei and Xu, Enbo and Wang, Wenjun and Liu, Donghong},
	month = may,
	year = {2023},
	keywords = {Automation, Digital technology, Intelligent perception, Meat cutting industry, Robotics},
	pages = {234--251},
}

@article{magyar_scilab_2012,
	series = {9th {IFAC} {Symposium} {Advances} in {Control} {Education}},
	title = {{SciLab} {Based} {Remote} {Control} of {Experiments}},
	volume = {45},
	issn = {1474-6670},
	url = {https://www.sciencedirect.com/science/article/pii/S1474667015376047},
	doi = {10.3182/20120619-3-RU-2024.00099},
	abstract = {The paper deals with building of a remote control laboratory consisting of two experiments: the thermo-optical and the hydraulic plant. Both devices are connected to the control computer via USB interface. The implementation of the presented solution is based on open technologies whereby the kernel of the application is formed by SciLab/Xcos environment. The possible method for communication between the remote experiment and SciLab and between SciLab and the web application is also presented.},
	number = {11},
	urldate = {2024-08-06},
	journal = {IFAC Proceedings Volumes},
	author = {Magyar, Zoltán and Žáková, Katarína},
	month = jan,
	year = {2012},
	keywords = {computer aided engineering, computer experiments, control systems, remote control},
	pages = {206--211},
}

@article{lambiase_empirical_2024,
	title = {An {Empirical} {Investigation} {Into} the {Influence} of {Software} {Communities}’ {Cultural} and {Geographical} {Dispersion} on {Productivity}},
	volume = {208},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S016412122300273X},
	doi = {10.1016/j.jss.2023.111878},
	abstract = {Estimating and understanding software development productivity represent crucial tasks for researchers and practitioners. Although different works focused on evaluating the impact of human factors on productivity, a few explored the influence of cultural/geographical diversity in software development communities. More particularly, all previous treatise addresses cultural aspects as abstract concepts without providing a quantitative representation. Improved knowledge of these matters might help project managers to assemble more productive teams and tool vendors to design software analytics toolkits that may better estimate productivity. This paper has the goal of enlarging the existing body of knowledge on the factors affecting productivity by focusing on cultural and geographical dispersion of a development community—namely, how diverse a community is in terms of cultural attitudes and geographical collocation of the members who belong to it. To reach this goal, we performed a mixed-method empirical study. First, we built a statistical model relating dispersion metrics with the productivity of 25 open-source communities on Github. Then, we performed a confirmatory survey with 140 practitioners. The key results of our study indicate that cultural and geographical dispersion considerably impact productivity, thus encouraging managers and practitioners to consider such aspects during all the phases of the software development lifecycle. We conclude our paper by elaborating on the main insights from our analyses and instilling implications that may drive further research.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Lambiase, Stefano and Catolino, Gemma and Pecorelli, Fabiano and Tamburri, Damian A. and Palomba, Fabio and van den Heuvel, Willem-Jan and Ferrucci, Filomena},
	month = feb,
	year = {2024},
	keywords = {Empirical software engineering, Global Software Engineering, Productivity, Social aspects in software engineering},
	pages = {111878},
}

@incollection{hossain_wound_2020,
	address = {Oxford},
	title = {Wound {Care}: {A} {Material} {Solution}},
	isbn = {978-0-12-813196-1},
	shorttitle = {Wound {Care}},
	url = {https://www.sciencedirect.com/science/article/pii/B978012803581810997X},
	abstract = {Wounds in critical condition may severely affect not only the patient’s social and personal life but also bother the carers and clinicians. This article discusses the complications and characteristics of different acute and chronic wounds. Complicated chronic wounds are very difficult to treat and manage. However, depending on the main three characteristics, for example, infection, exudates, and malodor, many dressings are available to choose from in the market. However, none of the dressings can provide all the benefits to manage a large range of wounds. For instance, the wounds with infection need to be treated with dressings containing antibacterial agents, the wounds containing exudates need to be managed using absorptive dressings, and malodorous wounds are usually handled using dressings containing activated carbon or cyclodextrins. Depending on the functions, the wound dressings are categorized in three types: Inert, interactive, and bioactive dressing. These dressings are found in different forms such as fiber, sliver, yarn, fabric, braid, nonwoven, nanofiber, and foam. This article finds the scientific solutions to manage a wide range of wounds. A number of wound care dressings, and their structures and functions, are addressed and discussed in order. This article also focuses on some specialty commercial dressings that are used for controlling infection and malodor, and skin regeneration.},
	urldate = {2024-08-06},
	booktitle = {Encyclopedia of {Renewable} and {Sustainable} {Materials}},
	publisher = {Elsevier},
	author = {Hossain, Mohammad F.},
	editor = {Hashmi, Saleem and Choudhury, Imtiaz Ahmed},
	month = jan,
	year = {2020},
	doi = {10.1016/B978-0-12-803581-8.10997-X},
	keywords = {Characteristics, Dressing, Exudates, Infection, Malodor, Wound},
	pages = {915--929},
}

@article{sampaio_exploring_2016-1,
	title = {Exploring context-sensitive data flow analysis for early vulnerability detection},
	volume = {113},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121215002873},
	doi = {10.1016/j.jss.2015.12.021},
	abstract = {Secure programming is the practice of writing programs that are resistant to attacks by malicious people or programs. Programmers of secure software have to be continuously aware of security vulnerabilities when writing their program statements. In order to improve programmers’ awareness, static analysis techniques have been devised to find vulnerabilities in the source code. However, most of these techniques are built to encourage vulnerability detection a posteriori, only when developers have already fully produced (and compiled) one or more modules of a program. Therefore, this approach, also known as late detection, does not support secure programming but rather encourages posterior security analysis. The lateness of vulnerability detection is also influenced by the high rate of false positives yielded by pattern matching, the underlying mechanism used by existing static analysis techniques. The goal of this paper is twofold. First, we propose to perform continuous detection of security vulnerabilities while the developer is editing each program statement, also known as early detection. Early detection can leverage his knowledge on the context of the code being created, contrary to late detection when developers struggle to recall and fix the intricacies of the vulnerable code they produced from hours to weeks ago. Second, we explore context-sensitive data flow analysis (DFA) for improving vulnerability detection and mitigate the limitations of pattern matching. DFA might be suitable for finding if an object has a vulnerable path. To this end, we have implemented a proof-of-concept Eclipse plugin for continuous DFA-based detection of vulnerabilities in Java programs. We also performed two empirical studies based on several industry-strength systems to evaluate if the code security can be improved through DFA and early vulnerability detection. Our studies confirmed that: (i) the use of context-sensitive DFA significantly reduces the rate of false positives when compared to existing techniques, without being detrimental to the detector performance, and (ii) early detection improves the awareness among developers and encourages programmers to fix security vulnerabilities promptly.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Sampaio, Luciano and Garcia, Alessandro},
	month = mar,
	year = {2016},
	keywords = {Secure programming, Data flow analysis, Early detection},
	pages = {337--361},
}

@article{mikhaylov_virtual_2012,
	series = {9th {IFAC} {Symposium} {Advances} in {Control} {Education}},
	title = {From {Virtual} {Lab} to {Virtual} {Development} {Lab}},
	volume = {45},
	issn = {1474-6670},
	url = {https://www.sciencedirect.com/science/article/pii/S1474667015375996},
	doi = {10.3182/20120619-3-RU-2024.00018},
	abstract = {The paper introduces the concept of a virtual development lab that was born from cooperation between academia and industry. Shortage of young well-trained engineers and a wish to share the industrial experience that the authors have accumulated over a decade has led one of the authors to teach satellite navigation at a University whilst simultaneously leading an industrial R \& D team. The paper presents the results of the use of a virtual lab (VL) that was developed to provide students with hands-on experience in the application of signal processing and automatic control. It is shown that VL has partly bridged the gap between the University training and the needs of industry. However, only an extension of the VL to the virtual development lab (VDL) has allowed the requirements of industry to be fully addressed. VDL is a web-based platform which assists learning by enabling the whole development cycle (design, development, verification) to be conducted in a controlled environment which is similar to industrial one. VDL combines ease of use with real life task setting, thus allowing a student to focus on the essentials of the engineering task. The ease of use is provided by incorporating routine engineering methodology (compilation, linking, testing, and result evaluation) into a graphical user interface (GUI), and a simple and intuitive application user interface (API). Realistic task setting is ensured by the nature of the tasks. The paper provides an explanation of all aspects of the VDL including the initial motivation, the technical and educational framework, its advantages and drawbacks, and the VDL design details. It also provides practical advice for VDL realization. The paper will help future developers of VDLs to deliver this approach.},
	number = {11},
	urldate = {2024-08-06},
	journal = {IFAC Proceedings Volumes},
	author = {Mikhaylov, Nikolay and Chernov, Dmitry},
	month = jan,
	year = {2012},
	keywords = {automatic control, education, GPS, virtual development lab, virtual lab},
	pages = {177--182},
}

@article{porter_discoverframework_2021-1,
	title = {The {DiscoverFramework} freeware toolkit for multivariate spatio-temporal environmental data visualization and evaluation},
	volume = {143},
	issn = {1364-8152},
	url = {https://www.sciencedirect.com/science/article/pii/S136481522100147X},
	doi = {10.1016/j.envsoft.2021.105104},
	abstract = {The freeware DiscoverFramework provides new tools to build spatial and temporal data visualization applications accessible to stakeholders, policy makers, scientists, and educators. By focusing on environmental data and supporting applications accessible via laptops, tablets, and cell phones, the DiscoverFramework can be used to increase public awareness and inspire responsible use of complex environmental systems upon which human society depends. DiscoverFramework enables computer-savvy domain scientists to develop interactive applications using “Elements” and workflows defined to make visualization easy and address common problems such as spatio-temporal scales and user engagement. Two applications are used to demonstrate DiscoverFramework: DiscoverWater and DiscoverHABs. DiscoverWater uses Map, Chart, and Text Elements to relate streamflow changes to groundwater withdrawals. DiscoverHABs uses the Scenario Element to aid stakeholders, such as resource managers and users, struggling to identify when and where harmful algal blooms (HABs) are likely given that causal relations in these systems remain poorly understood.},
	urldate = {2024-08-06},
	journal = {Environmental Modelling \& Software},
	author = {Porter, Misty E. and Hill, Mary C. and Harris, Ted and Brookfield, Andrea and Li, Xingong},
	month = sep,
	year = {2021},
	keywords = {Data visualization, Harmful algal blooms (HABs), Knowledge discovery, Water resources, Web application},
	pages = {105104},
}

@incollection{kalhan_chapter_2020,
	title = {Chapter 13 - {Clinical} molecular endocrinology},
	isbn = {978-0-12-809356-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128093566000137},
	abstract = {Clinical endocrinology incorporates the diagnosis and management of disorders associated with hormone synthesis and/or function. Recent advances in the ever so expanding field of molecular biology have provided clinicians with a novel insight into molecular mechanism leading to endocrine dysfunction. This chapter on clinical molecular endocrinology brings together commonly encountered scenarios from clinical practice in endocrinology with emphasis on molecular mechanisms that underpin the clinical presentation. The clinical cases that have been discussed in detail cover relevant molecular and genetic pathways leading to pituitary, parathyroid, adrenal, and gonadal dysfunction.},
	urldate = {2024-08-06},
	booktitle = {Clinical {Molecular} {Medicine}},
	publisher = {Academic Press},
	author = {Kalhan, Atul},
	editor = {Kumar, Dhavendra},
	month = jan,
	year = {2020},
	doi = {10.1016/B978-0-12-809356-6.00013-7},
	keywords = {Endocrinology, etiology, hormone deficiency, hyperparathyroidism, molecular systems, mutations},
	pages = {217--244},
}

@article{lah_metaphysical_2015,
	series = {{ASLI} {QoL2014} ({Annual} {Serial} {Landmark} {International} {Conference} on {Quality} of {Life}) / {AQoL} 2014 {Istanbul} ({ABRA} {International} {Conference} on {Quality} of {Life}), {Istanbul} {Technical} {University}, {Istanbul}, {Turkey}, 26 - 28 {December} 2014},
	title = {Metaphysical {Approach} for {Design} {Functionality} in {Malay}-{Islamic} {Architecture}},
	volume = {202},
	issn = {1877-0428},
	url = {https://www.sciencedirect.com/science/article/pii/S187704281504879X},
	doi = {10.1016/j.sbspro.2015.08.231},
	abstract = {This paper presents the findings of a study on metaphysical approaches to building design. Three major Asian cultures, the Chinese-Buddhist, Indian-Hindu, and Malay-Islam, are reviewed. There are similarities found in principles towards achieving the occupants’ well-being. Functionality became priority and rituals are performed at ensuring the well-being and prosperity of future occupants. Whereas, the Chinese-Buddhist practice is called Feng Shui, the Indian-Hindu tradition is based on Vastu-Vidya. The Malay-Islam is extractions from religious teachings written in a manuscript titled Tajul Muluk. The paper concludes that metaphysical approach could still play its roles in the design today.},
	urldate = {2024-08-06},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Lah, Nor Aniswati Awang and Wahab, Mohamad Hanif Abdul and Koh, David and Saruwono, Masran},
	month = aug,
	year = {2015},
	keywords = {environology, functional, geomancy, Metaphysics},
	pages = {273--284},
}

@article{arshad_analysis_2021-1,
	title = {Analysis of security and privacy challenges for {DNA}-genomics applications and databases},
	volume = {119},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046421001441},
	doi = {10.1016/j.jbi.2021.103815},
	abstract = {DNA technology is rapidly moving towards digitization. Scientists use software tools and applications for sequencing, synthesizing, analyzing and sharing of DNA and genomic data, operate lab equipment and store genetic information in shared datastores. Using cutting-edge computing methods and techniques, researchers have decoded human genome, created organisms with new capabilities, automated drug development and transformed food safety. Such software applications are typically developed to progress scientific understanding and as such cyber security is never a concern for these applications. However, with the increasing commercialisation of DNA technologies, coupled with the sensitivity of DNA data, there is a need to adopt a security-by-design approach. In this paper we investigate bio-cyber security threats to genomic-DNA data and software applications making use of such data to advance scientific research. Specifically, we adopt an empirical approach to analyse and identify vulnerabilities within genomic-DNA databases and bioinformatics software applications that can lead to cyber-attacks affecting the confidentiality, integrity and availability of such sensitive data. We present a detailed analysis of these threats and highlight potential protection mechanisms to help researchers pursue these research directions.},
	urldate = {2024-08-06},
	journal = {Journal of Biomedical Informatics},
	author = {Arshad, Saadia and Arshad, Junaid and Khan, Muhammad Mubashir and Parkinson, Simon},
	month = jul,
	year = {2021},
	keywords = {Bioinformatics, Cyber-attacks, Cyberbiosecurity, DNA, Genomics, Vulnerabilities},
	pages = {103815},
	file = {Versão aceita:files/1270/Arshad et al. - 2021 - Analysis of security and privacy challenges for DN.pdf:application/pdf},
}

@article{campos_scales_2023,
	title = {Scales of solar energy: {Exploring} citizen satisfaction, interest, and values in a comparison of regions in {Portugal} and {Spain}},
	volume = {97},
	issn = {2214-6296},
	shorttitle = {Scales of solar energy},
	url = {https://www.sciencedirect.com/science/article/pii/S2214629623000129},
	doi = {10.1016/j.erss.2023.102952},
	abstract = {Solar energy installations are transforming urban and rural landscapes, with diverse socioeconomic and environmental impacts on local populations. Social acceptance of solar energy has been found to change depending on the scale of the installations. Yet, the conditions that lead citizens to become actively engaged in different solar energy “size” projects are still underexplored. Drawing on both social acceptance and energy citizenship literature, this study focuses on two case study regions in Southern Europe (i.e., Alentejo in Portugal and Andalusia in Spain), where a multi-scale solar expansion is advancing, with significant investments in large centralized solar photovoltaic systems (i.e., {\textgreater}50MWp). The aim of the article is to explore how citizens perceive the importance of the energy transition and gain further insight into the conditions that make citizens most satisfied with, and interested in, actively participating in the development of solar energy projects. The study draws on a representative survey (n = 832) collected in the two study regions and includes a vignette experiment. Statistical data analysis supports an understanding of the relational nature of social acceptance, which is suggested to be also applicable to energy citizenship, across different scales of solar energy production, from large and centralized to small-scale decentralized installations. The conclusions offer new findings on the intersection of social acceptance and energy citizenship research, as well as insights into energy policies related to solar energy expansion, useful for other regions and countries with comparable geographies in the Mediterranean and Southern Europe.},
	urldate = {2024-08-06},
	journal = {Energy Research \& Social Science},
	author = {Campos, Inês and Brito, Miguel and Luz, Guilherme},
	month = mar,
	year = {2023},
	keywords = {Energy citizenship, Portugal, Quantitative studies, Scale, Social acceptance, Solar energy systems, Spain},
	pages = {102952},
}

@article{stapleton_systemic_2017,
	series = {20th {IFAC} {World} {Congress}},
	title = {Systemic {Control}, {Cultural} {Values} and {Religious} {Institutions} {An} {Assessment} of {Semi}-{Automatic} {Human} {Values} {Systems} {Analysis} in {Religious} {Institutional} {Diagnostics}},
	volume = {50},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896317315094},
	doi = {10.1016/j.ifacol.2017.08.1028},
	abstract = {Investigations of systems engineering failures from the aviation, nuclear and other sectors demonstrate the link between human factors and technical failures which may have lessons for other institutions. The term “safety culture” refers to background factors which impinge upon safety management systems, drawing particular attention to cultural features of organisations and these impinge upon effective control and risk management processes. Is it possible to formally or semi-formally analyse qualitative institutional cultural traits? This paper presents findings of a study in which automatic systems are used to provide an ethically-informed values analysis of a large values-driven institution. Tests showed that the system was capable of gathering, processing and presenting robust values congruency data capable of exposing deep axiological traits which may be out of alignment in a religious institution. Implications are drawn for control systems research, limitations are exposed and future research directions presented.},
	number = {1},
	urldate = {2024-08-06},
	journal = {IFAC-PapersOnLine},
	author = {Stapleton, Larry and Marques, Dawton and Thakar, Tejan},
	month = jul,
	year = {2017},
	keywords = {Complex systems, culture, developing countries, ethics, international stability},
	pages = {6373--6378},
}

@article{de_almeida_lima_use_2016,
	series = {{ICSDEC} 2016 – {Integrating} {Data} {Science}, {Construction} and {Sustainability}},
	title = {Use of {Contaminated} {Sludge} in {Concrete}},
	volume = {145},
	issn = {1877-7058},
	url = {https://www.sciencedirect.com/science/article/pii/S187770581630162X},
	doi = {10.1016/j.proeng.2016.04.155},
	abstract = {Brazil, the 5th largest country by landmass and the 8th largest by Gross Domestic Product, continues to grow both in population as well as in construction. Major construction investments in Brazil has increased the demand for concrete. Taking into account the constant growth in population in the past 20 years, this paper presents an innovative solution that could potentially decrease the cost of concrete, while also helping to preserve the environment. The solution proposes using the hazardous waste (sludge) from Brazil's water purification system, Estação de Tratamento de Água (ETA), as an alternative aggregate for concrete. Sludge is a type of hazardous waste produced by the water purification process. With the dramatic growth in Brazil's population, the demand for drinking water has caused there to be an overabundance of hazardous waste. Sludge, if disposed inappropriately, harms the environment. Using sludge in concrete will minimize the amount harming the environment and provide a low-cost solution to aggregates needed in concrete. The research reviews and analyzes the implementation and feasibility of this solution.},
	urldate = {2024-08-06},
	journal = {Procedia Engineering},
	author = {de Almeida Lima, Daniel and Zulanas, Charles},
	month = jan,
	year = {2016},
	keywords = {alternative aggregates, Brazil, Concrete, contaminated sludge, disposal of hazardous waste},
	pages = {1201--1208},
}

@article{lopez-fernandez_combining_2016-1,
	title = {Combining unit and specification-based testing for meta-model validation and verification},
	volume = {62},
	issn = {0306-4379},
	url = {https://www.sciencedirect.com/science/article/pii/S0306437916301934},
	doi = {10.1016/j.is.2016.06.008},
	abstract = {Meta-models play a cornerstone role in Model-Driven Engineering as they are used to define the abstract syntax of modelling languages, and so models and all sorts of model transformations depend on them. However, there are scarce tools and methods supporting their Validation and Verification (V\&V), which are essential activities for the proper engineering of meta-models. In order to fill this gap, we propose two complementary meta-model V\&V languages. The first one has similar philosophy to the xUnit framework, as it enables the definition of meta-model unit test suites comprising model fragments and assertions on their (in-)correctness. The second one is directed to express and verify expected properties of a meta-model, including domain and design properties, quality criteria and platform-specific requirements. As a proof of concept, we have developed tooling for both languages in the Eclipse platform, and illustrate its use within an example-driven approach for meta-model construction. The expressiveness of our languages is demonstrated by their application to build a library of meta-model quality issues, which has been evaluated over the ATL zoo of meta-models and some OMG specifications. The results show that integrated support for meta-model V\&V (as the one we propose here) is urgently needed in meta-modelling environments.},
	urldate = {2024-08-06},
	journal = {Information Systems},
	author = {López-Fernández, Jesús J. and Guerra, Esther and de Lara, Juan},
	month = dec,
	year = {2016},
	keywords = {Domain-specific modelling languages, Meta-model quality, Meta-modelling, Model-driven engineering, Validation \& verification},
	pages = {104--135},
	file = {Versão aceita:files/1317/López-Fernández et al. - 2016 - Combining unit and specification-based testing for.pdf:application/pdf},
}

@incollection{gage_chapter_2018,
	address = {San Diego},
	title = {Chapter 4 - {The} {Art} of {Seeing}},
	isbn = {978-0-12-803813-0},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128038130000040},
	abstract = {In this chapter, we explore the brain bases for human vision. Although we typically have the notion that our eyes record the world around us much like a video recorder or a camera, it turns out that is not the case! This chapter provides a clear and concise discussion of how human visual processing is truly the “art” of seeing. Our brain “constructs” the visual world around us in a complex way. This chapter steps through aspects of visual processing from understanding how neurons—brain cells—encode features in visual scenes to how faces are processed in a special way and finally through to what happens when visual areas in the brain are damaged. A fascinating aspect of vision—visual illusions—is presented with an explanation of why the brain is “fooled” by these illusions. The chapter ends with a discussion of the conscious—and unconscious—perception of visual objects.},
	urldate = {2024-08-06},
	booktitle = {Fundamentals of {Cognitive} {Neuroscience} ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Gage, Nicole M. and Baars, Bernard J.},
	editor = {Gage, Nicole M. and Baars, Bernard J.},
	month = jan,
	year = {2018},
	doi = {10.1016/B978-0-12-803813-0.00004-0},
	keywords = {Binocular rivalry, Blindsight, Constructive perception, Object recognition, Primary visual cortex (V1), Vision, Visual awareness, Visual consciousness, Visual illusions, Visual perception},
	pages = {99--141},
}

@article{shi_chinese_2019,
	title = {Chinese medicine {JQ} granule combined with half-dose omeprazole for nonerosive reflux disease: {A} multicenter, randomized, double-blind, placebo-controlled trial study protocol},
	volume = {31},
	issn = {1876-3820},
	shorttitle = {Chinese medicine {JQ} granule combined with half-dose omeprazole for nonerosive reflux disease},
	url = {https://www.sciencedirect.com/science/article/pii/S1876382018309971},
	doi = {10.1016/j.eujim.2019.100974},
	abstract = {Introduction
Nonerosive reflux disease (NERD) is a kind of esophageal disease manifesting mainly as heartburn and regurgitation. The prevalence of NERD in patients with gastroesophageal reflux disease is up to 70\%, and it significantly affects the quality of life of patients. In China, besides proton-pump inhibitors (PPIs), a herbal formula is widely used in nonerosive reflux disease, which can improve intestinal and extra-intestinal symptoms of patients. This study will evaluate in a large, independent, randomized controlled trial (RCT) the comparative effectiveness of a specific type of Chinese herbal granule with half-dose PPI and full-dose PPI in managing nonerosive reflux disease.
Methods and analysis
This trial will be a multicenter, randomized, double-blind, placebo-controlled RCT. A total of 204 participants will be enrolled and randomized (1:1) to each group: herb granules plus 10 mg omeprazole plus 10 mg dummy omeprazole; and dummy herb granules plus 20 mg omeprazole. The efficacy measures will be the changes in scores in the Gastroesophageal Reflux Disease Questionnaire, the Short-Form 36, the Patient Report Outcome, the 17-item Hamilton Depression Scale, and the Signs and Symptoms Scale.
Discussion
This study will focus on the quality control of a Chinese medical formulation. The RCT research procedure will be conducted under the supervision of the Good Clinical Practice (GCP) center. This trial will reduce the routine dosage of omeprazole while adding Jianpi Qinghua JQ granule, with the aim to discuss the safety and effectiveness of the Chinese herbal granule on esophageal and extraesophageal symptoms while meeting the ethical requirements.},
	urldate = {2024-08-06},
	journal = {European Journal of Integrative Medicine},
	author = {Shi, Xiaoshuang and Li, Xia and Ma, Jinxin and Che, Hui and Ma, Xiangxue and Xie, Jingyi and Yin, Xiaolan and Wu, Haomeng and Lv, Lin and Chen, Ting and Zhang, Jiaqi and Zeng, Enjin and Tang, Xudong and Wang, Fengyun},
	month = oct,
	year = {2019},
	keywords = {Chinese medicine, JQ granule, Nonerosive reflux disease, Omeprazole, Protocol, Randomized controlled trial},
	pages = {100974},
}

@article{kuosmanen_exploring_2023-1,
	title = {Exploring crowdsourced self-care techniques: {A} study on {Parkinson}’s disease},
	volume = {177},
	issn = {1071-5819},
	shorttitle = {Exploring crowdsourced self-care techniques},
	url = {https://www.sciencedirect.com/science/article/pii/S107158192300071X},
	doi = {10.1016/j.ijhcs.2023.103062},
	abstract = {Living with Parkinson’s Disease introduces a range of significant challenges into one’s daily life. While medical interventions exist to overcome some of these challenges, patient self-care techniques often form an essential complement to the treatments recommended by medical doctors. Knowledge on these self-care techniques often originates from those living with Parkinson’s themselves or their close caregivers, as they have the knowledge and experience required to assess self-care techniques. This so-called ‘patient knowledge’ is usually exchanged in peer meetings or discussion forums. Although vital to the Parkinson’s Disease community, this information is often difficult to access due to its unstructured format and the difficulty of navigating through online forums. We present an online tool that allows for contributing, assessing, and finally discovering Parkinson’s Disease self-care techniques. The custom discovery tool was populated with self-care knowledge by over 300 people with Parkinson’s and dozens of their carers, spanning areas such as daily well-being and using assistive equipment. Then, we invited patients to explore the discover features in a smaller scale trial. While well-received, our deployment highlighted several challenges that we further discuss in this paper. Overall, our study contributes to crowdsourced digital health solutions and provides both design and research implications to this challenging domain with a vulnerable user group.},
	urldate = {2024-08-06},
	journal = {International Journal of Human-Computer Studies},
	author = {Kuosmanen, Elina and Huusko, Eetu and van Berkel, Niels and Nunes, Francisco and Vega, Julio and Goncalves, Jorge and Khamis, Mohamed and Esteves, Augusto and Ferreira, Denzil and Hosio, Simo},
	month = sep,
	year = {2023},
	keywords = {Crowdsourcing, Knowledge base, Parkinson’s disease, Self-care},
	pages = {103062},
	file = {Texto completo:files/1318/Kuosmanen et al. - 2023 - Exploring crowdsourced self-care techniques A stu.pdf:application/pdf},
}

@incollection{baars_chapter_2013,
	address = {San Diego},
	title = {Chapter 6 - {The} art of seeing},
	isbn = {978-0-12-415805-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9780124158054000060},
	urldate = {2024-08-06},
	booktitle = {Fundamentals of {Cognitive} {Neuroscience}},
	publisher = {Academic Press},
	editor = {Baars, Bernard J. and Gage, Nicole M.},
	month = jan,
	year = {2013},
	doi = {10.1016/B978-0-12-415805-4.00006-0},
	pages = {141--173},
}

@incollection{pradilla_chapter_2016,
	title = {Chapter 7 - {Micro} {Virtual} {Machines} ({MicroVMs}) for {Cloud}-assisted {Cyber}-{Physical} {Systems} ({CPS})},
	isbn = {978-0-12-805395-9},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128053959000071},
	abstract = {Cyber-Physical Systems (CPS) need to adapt to the changing physical world and expand their capabilities dynamically. To meet this need, this chapter proposes a three-tier architecture that integrates: cloud computing, fog computing, and networks of sensors/actuators. It also provides an implementation of the proposed architecture, based on the use of Micro Virtual Machines (MicroVM) and the Sensor Observation Service (SOS), combining the isolation of virtual machines with the standardization of storage and information-exchange under the Sensor Web Enablement (SWE) framework. Subsequently, the proposed architecture is coupled to the Internet of Things (IoT), and three use-cases that can be applied are addressed: eHealth, precision agriculture, and domotics; these three use-cases clarify the benefits of the implementation of the architecture and illustrate the interactions between its composing levels.},
	urldate = {2024-08-06},
	booktitle = {Internet of {Things}},
	publisher = {Morgan Kaufmann},
	author = {Pradilla, J. V. and Palau, C. E.},
	editor = {Buyya, Rajkumar and Vahid Dastjerdi, Amir},
	month = jan,
	year = {2016},
	doi = {10.1016/B978-0-12-805395-9.00007-1},
	keywords = {Cyber-Physical Systems (CPS), domotic, eHealth, Internet of Things (IoT), Micro Virtual Machines (MicroVM), Precision Agriculture (PA), Sensor Observation Service (SOS), Sensor Web Enablement (SWE)},
	pages = {125--142},
}

@article{getchell_network_2016-1,
	title = {A network analysis of official {Twitter} accounts during the {West} {Virginia} water crisis},
	volume = {54},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563215300054},
	doi = {10.1016/j.chb.2015.06.044},
	abstract = {Online networks using Web 2.0 technologies have proven useful for communication among all parties involved in managing crises. These networks rapidly disseminate information allowing for coordination among organizations responding to the needs of those whose safety and wellbeing are threatened by the crisis and its aftermath. This study provides a network analysis of official Twitter accounts activated during the Charleston, West Virginia, water contamination crisis in 2014. The city’s water supply was rendered unfit for drinking or bathing after 7500 gallons of a toxic chemical leaked into the Elk River. The network created by the 41 Twitter accounts associated with the West Virginia water contamination lacked density, contained several isolates, exchanged information quickly (geodesic distance diameter), and contained both national and local accounts. The lack of density indicates limited exchange of information, particularly between national and federal accounts. The rapid dissemination of the information that was shared and the fact that some accounts did bridge the local and national gap, however, show the positive potential for such networks in responding to crises.},
	urldate = {2024-08-06},
	journal = {Computers in Human Behavior},
	author = {Getchell, Morgan C. and Sellnow, Timothy L.},
	month = jan,
	year = {2016},
	keywords = {Crisis communication, Network analysis, Risk communication, Social media},
	pages = {597--606},
}

@article{martins_efficacy_2020,
	title = {Efficacy of natural antimicrobials derived from phenolic compounds in the control of biofilm in children and adolescents compared to synthetic antimicrobials: {A} systematic review and meta-analysis},
	volume = {118},
	issn = {0003-9969},
	shorttitle = {Efficacy of natural antimicrobials derived from phenolic compounds in the control of biofilm in children and adolescents compared to synthetic antimicrobials},
	url = {https://www.sciencedirect.com/science/article/pii/S0003996920302223},
	doi = {10.1016/j.archoralbio.2020.104844},
	abstract = {Aim
To evaluate the efficacy of natural antimicrobials derived from phenolic compounds (NAPs), compared to synthetic antimicrobials (SAs), in the biofilm control and microorganisms (MOs) count among children and adolescents at different intervention times through a systematic review and meta-analysis.
Methods
Electronic searches were carried out in PubMed, Scopus, Cochrane Library, Web of Science, VHL, and Grey Literature. Randomized and non-randomized clinical trials were included. Methodological quality and risk of bias were assessed using the tools ROBINS-I and RoB 2.0. Meta-analyses (MAs) were performed according to three parameters: the influence of NAPs on the plaque index (PI) mean; the period of NAPs administration (≤15 days/{\textgreater}15 days) on the biofilm reduction; and the influence of NAPs on the MOs count subgrouping according to the type of MO (total MOs, S. mutans, and Streptococcus spp.). The standard mean differences were calculated (p ≤ 0.05) for all analyses, and the heterogeneity was tested through the I2 index. The evidence was certainty-tested using the GRADE approach.
Results
Sixteen studies were selected for qualitative synthesis, and 12 studies were included in the MAs. NAPs were less efficacious in improving the PI (p {\textless} 0.0001, I2{\textgreater}87 \%) and reducing biofilm over time (p {\textless} 0.01, I2{\textgreater}87 \%) but presented a reduction in MOs count similar to that of SAs (p = 0.3, I2 = 0\%). The quality of the evidence ranged from moderate to low.
Conclusion
Although the use of NAPs is similar to the use of SAs in reducing MOs count, it is less effective than SAs in improving PI mean and for biofilm reduction over time.},
	urldate = {2024-08-06},
	journal = {Archives of Oral Biology},
	author = {Martins, Mariana Leonel and Ribeiro-Lages, Mariana Batista and Masterson, Daniele and Magno, Marcela Baraúno and Cavalcanti, Yuri Wanderley and Maia, Lucianne Cople and Fonseca-Gonçalves, Andréa},
	month = oct,
	year = {2020},
	keywords = {Anti-bacterial agents, Biolfims, Plant preparations},
	pages = {104844},
}

@article{hackert_academy_2020,
	title = {Academy of {Nutrition} and {Dietetics}: {Revised} 2020 {Standards} of {Practice} and {Standards} of {Professional} {Performance} for {Registered} {Dietitian} {Nutritionists} ({Competent}, {Proficient}, and {Expert}) in {Eating} {Disorders}},
	volume = {120},
	issn = {2212-2672},
	shorttitle = {Academy of {Nutrition} and {Dietetics}},
	url = {https://www.sciencedirect.com/science/article/pii/S2212267220309047},
	doi = {10.1016/j.jand.2020.07.014},
	abstract = {Eating disorders (ED) are complex mental illnesses and are not a result of personal choice. Full recovery from an ED is possible. The severity and inherent lethality of an ED is undisputed, and the role of the registered dietitian nutritionist (RDN) is essential. Clinical symptomology presents at varying developmental milestones and is perpetuated through a sociocultural evaluation of beauty and drive for ascetic idealism. ED are globally prevalent in 4.4\% of the population aged 5 to 17 years, yet affect individuals across the entire lifespan, including all cultures and genders. The Behavioral Health Nutrition Dietetic Practice Group, along with the Academy of Nutrition and Dietetics Quality Management Committee, revised the Standards of Practice (SOP) and Standards of Professional Performance (SOPP) for RDNs in Eating Disorders. Including the RDN in ED treatment is vital for all levels of care. The RDN must be perceptive to negative symptoms indicative of psychological triggers when exploring food belief systems, patterns of disinhibition, and nutrition misinformation with clients. Through a conscious awareness of medical, psychological, and behavioral strategies, the implementation of the SOP and SOPP supports a dynamic and holistic view of ED treatment by the RDN. The SOP and SOPP are complementary resources for RDNs and are intended to be used as self-evaluation tools for assuring competent practice in ED and for determining potential education, training, supervision, and mentorship needs for advancement to a higher practice level in a variety of settings.},
	number = {11},
	urldate = {2024-08-06},
	journal = {Journal of the Academy of Nutrition and Dietetics},
	author = {Hackert, April N. and Kniskern, Megan A. and Beasley, Tammy M.},
	month = nov,
	year = {2020},
	pages = {1902--1919.e54},
}

@article{salama_evaluation_2023,
	title = {Evaluation of the cytotoxicity and genotoxicity potential of synthetic diacetyl food flavoring \textit{in silico,} in vivo\textit{, and in vitro}},
	volume = {178},
	issn = {0278-6915},
	url = {https://www.sciencedirect.com/science/article/pii/S0278691523003253},
	doi = {10.1016/j.fct.2023.113923},
	abstract = {Diacetyl is a common ingredient that creates a buttery flavor in baked goods and other food products. The cytotoxic impact of diacetyl on a normal human liver cell line (THLE2) indicated an IC50 value of 41.29 mg/ml through MTT assay and a cell cycle arrest in the G0/G1 phase relative to the control. Administration of diacetyl at two-time points (acute-chronic) led to a significant increase in DNA damage indicated by the increase in tail length, tail DNA\%, and tail moment. The mRNA and protein expression levels of genes in the rats' livers were then measured using real-time PCR and western blotting. The results showed an activation of the apoptotic and necrosis mechanism, with an upregulation of p53, Caspase 3, and RIP1 and a downregulation of Bcl-2 at the mRNA level. The ingestion of diacetyl disrupted the liver's oxidant/antioxidant balance, as evidenced by alterations in levels of GSH, SOD, CAT, GPx, GR, MDA, NO, and peroxynitrite. Additionally, heightened levels of inflammatory cytokines were shown. Histopathological examinations revealed necrotic foci and congested portal areas in the rats' liver cells after treatment with diacetyl. Diacetyl may interact moderately with Caspase, RIP1, and p53 core domain through In-silico, possibly resulting in upregulated gene expression.},
	urldate = {2024-08-06},
	journal = {Food and Chemical Toxicology},
	author = {Salama, Mohamed and Mohammed, Dina Mostafa and Fahmy, Khaled and Al-Senosy, Neima K. and Ebeed, Naglaa M. and Farouk, Amr},
	month = aug,
	year = {2023},
	keywords = {Biochemical parameters, Comet assay, Diacetyl, MTT assay, Real time-PCR, Western blots},
	pages = {113923},
}

@article{dwivedi_opinion_2023-1,
	title = {Opinion {Paper}: “{So} what if {ChatGPT} wrote it?” {Multidisciplinary} perspectives on opportunities, challenges and implications of generative conversational {AI} for research, practice and policy},
	volume = {71},
	issn = {0268-4012},
	shorttitle = {Opinion {Paper}},
	url = {https://www.sciencedirect.com/science/article/pii/S0268401223000233},
	doi = {10.1016/j.ijinfomgt.2023.102642},
	abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.},
	urldate = {2024-08-06},
	journal = {International Journal of Information Management},
	author = {Dwivedi, Yogesh K. and Kshetri, Nir and Hughes, Laurie and Slade, Emma Louise and Jeyaraj, Anand and Kar, Arpan Kumar and Baabdullah, Abdullah M. and Koohang, Alex and Raghavan, Vishnupriya and Ahuja, Manju and Albanna, Hanaa and Albashrawi, Mousa Ahmad and Al-Busaidi, Adil S. and Balakrishnan, Janarthanan and Barlette, Yves and Basu, Sriparna and Bose, Indranil and Brooks, Laurence and Buhalis, Dimitrios and Carter, Lemuria and Chowdhury, Soumyadeb and Crick, Tom and Cunningham, Scott W. and Davies, Gareth H. and Davison, Robert M. and Dé, Rahul and Dennehy, Denis and Duan, Yanqing and Dubey, Rameshwar and Dwivedi, Rohita and Edwards, John S. and Flavián, Carlos and Gauld, Robin and Grover, Varun and Hu, Mei-Chih and Janssen, Marijn and Jones, Paul and Junglas, Iris and Khorana, Sangeeta and Kraus, Sascha and Larsen, Kai R. and Latreille, Paul and Laumer, Sven and Malik, F. Tegwen and Mardani, Abbas and Mariani, Marcello and Mithas, Sunil and Mogaji, Emmanuel and Nord, Jeretta Horn and O’Connor, Siobhan and Okumus, Fevzi and Pagani, Margherita and Pandey, Neeraj and Papagiannidis, Savvas and Pappas, Ilias O. and Pathak, Nishith and Pries-Heje, Jan and Raman, Ramakrishnan and Rana, Nripendra P. and Rehm, Sven-Volker and Ribeiro-Navarrete, Samuel and Richter, Alexander and Rowe, Frantz and Sarker, Suprateek and Stahl, Bernd Carsten and Tiwari, Manoj Kumar and van der Aalst, Wil and Venkatesh, Viswanath and Viglia, Giampaolo and Wade, Michael and Walton, Paul and Wirtz, Jochen and Wright, Ryan},
	month = aug,
	year = {2023},
	keywords = {ChatGPT, Conversational agent, Generative AI, Generative artificial intelligence, Large language models},
	pages = {102642},
	file = {Texto completo:files/1303/Dwivedi et al. - 2023 - Opinion Paper “So what if ChatGPT wrote it” Mult.pdf:application/pdf},
}

@article{borges_cloud_2018,
	title = {Cloud restriction solver: {A} refactoring-based approach to migrate applications to the cloud},
	volume = {95},
	issn = {0950-5849},
	shorttitle = {Cloud restriction solver},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584917301799},
	doi = {10.1016/j.infsof.2017.11.014},
	abstract = {Context
The migration of legacy systems to the Platform as a Service (PaaS) model provides several benefits, but also brings new challenges, such as dealing with the restrictions imposed by the service provider. Furthermore, factors such as time, training and the extensive reengineering activities make the migration process time consuming and error prone. Although there exist several techniques for partial or total migration of legacy applications to the cloud, only a few specifically address the resolution of these constraints.
Objective
This paper proposes a novel semi-automatic approach, called Cloud Restriction Solver (CRS), for migrating applications to a PaaS environment that avoids the cloud restrictions through user-defined refactorings.
Methods
The approach is supported by two open and extensible tools. The first one, called CRSAnalyzer, identifies the pieces of code that violate the restrictions of the chosen PaaS platform, while the second one, CRSRefactor, changes those pieces by equivalent cloud-enabled services.
Results
The applicability of the proposed approach is presented by showing its instantiation for Google App Engine as an Eclipse plugin and by migrating three Java applications to that PaaS successfully. In addition, an instantiation for IBM Bluemix has been created and used to compare the migration of the same application using the developed tools for both cloud providers.
Conclusion
The proposed approach fosters software reuse, is cloud-independent, and facilitates the migration of applications to PaaS platforms.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {Borges, Marcos and Barros, Erick and Maia, Paulo Henrique},
	month = mar,
	year = {2018},
	keywords = {Software evolution, Refactoring, Cloud migration},
	pages = {346--365},
}

@article{bozkurt_2022_2022,
	title = {2022 {AHA}/{ACC} {Key} {Data} {Elements} and {Definitions} for {Cardiovascular} and {Noncardiovascular} {Complications} of {COVID}-19: {A} {Report} of the {American} {College} of {Cardiology}/{American} {Heart} {Association} {Task} {Force} on {Clinical} {Data} {Standards}},
	volume = {80},
	issn = {0735-1097},
	shorttitle = {2022 {AHA}/{ACC} {Key} {Data} {Elements} and {Definitions} for {Cardiovascular} and {Noncardiovascular} {Complications} of {COVID}-19},
	url = {https://www.sciencedirect.com/science/article/pii/S073510972204579X},
	doi = {10.1016/j.jacc.2022.03.355},
	number = {4},
	urldate = {2024-08-06},
	journal = {Journal of the American College of Cardiology},
	author = {Bozkurt, Biykem and Das, Sandeep R. and Addison, Daniel and Gupta, Aakriti and Jneid, Hani and Khan, Sadiya S. and Koromia, George Augustine and Kulkarni, Prathit A. and LaPoint, Kathleen and Lewis, Eldrin F. and Michos, Erin D. and Peterson, Pamela N. and Turagam, Mohit K. and Wang, Tracy Y. and Yancy, Clyde W.},
	month = jul,
	year = {2022},
	keywords = {COVID-19, ACC/AHA Clinical Data Standards, cardiogenic shock, cardiovascular diseases, coronavirus infections, extracorporeal membrane oxygenation, medical informatics, myocarditis, SARS-CoV-2},
	pages = {388--465},
}

@article{verbrugge_cyanide_2021,
	title = {The cyanide revolution: {Efficiency} gains and exclusion in artisanal- and small-scale gold mining},
	volume = {126},
	issn = {0016-7185},
	shorttitle = {The cyanide revolution},
	url = {https://www.sciencedirect.com/science/article/pii/S0016718521002323},
	doi = {10.1016/j.geoforum.2021.07.030},
	abstract = {Since its advent at the end of the nineteenth century, cyanide processing facilitated the intensification and global expansion of industrial gold mining. Today, there are important indications that artisanal and small-scale gold mining (ASGM) is on the verge of a similar cyanide revolution: while ASGM is typically associated with mercury-based processing, mercury amalgamation is increasingly replaced with, or complemented by, cyanidation. Relying on evidence from the Philippines, Indonesia, and Burkina Faso, we demonstrate how this transition is having a deeply transformative impact on ASGM communities. On the one hand, cyanidation produces clear efficiency gains. Together with rising gold prices, it is fueling a dramatic expansion of ASGM by enabling the profitable extraction of lower-grade gold deposits. On the other hand, it contributes to the emergence of new and often highly unequal labor and revenue-sharing arrangements. More broadly, these findings demonstrate the highly uneven impact of socio-technical transformations. Consequently, the growing number of efforts to intervene in the technological make-up of ASGM, usually in the name of efficiency and sustainability, should be wary of having unintended consequences.},
	urldate = {2024-08-06},
	journal = {Geoforum},
	author = {Verbrugge, Boris and Lanzano, Cristiano and Libassi, Matthew},
	month = nov,
	year = {2021},
	keywords = {Artisanal and small-scale mining (ASM), Cyanide, Gold mining, Gold processing, Mining labor},
	pages = {267--276},
	file = {Texto completo:files/1304/Verbrugge et al. - 2021 - The cyanide revolution Efficiency gains and exclu.pdf:application/pdf},
}

@article{rizzato_is_2016,
	title = {Is food desirability affected by social interaction?},
	volume = {50},
	issn = {0950-3293},
	url = {https://www.sciencedirect.com/science/article/pii/S0950329316300179},
	doi = {10.1016/j.foodqual.2016.02.005},
	abstract = {The experience of food can be affected by visual perception and the context in which food is presented. In this study, we investigated the effect of specific emotional contexts on food desirability. To this purpose, we selected a highly familiar Italian food, Pizza Margherita, whose sauce color was digitally manipulated to create different desirability levels. The participants rated the pizza desirability on the basis of its visual features alone (no-context condition) and, most critically, after viewing images representing facial expressions of happiness, anger and neutrality. Anger was selected, as opposed to disgust, as a food-unrelated emotion to assess whether influential effects on food desirability evoked by the observed individual’s emotion state needs to be necessarily related to the eating context. Confirming previous results, we found an effect of the social context (facial expressions) on food desirability. Similarly to the no-context condition, happiness evoked the highest desirability ratings for all items compared to neutral and angry expressions, whereas the neutral and angry faces negatively affected food desirability relative to no-context and happy face conditions. These findings show that happiness is commensurate with a situation in which food is presented alone, suggesting that approach to food in healthy individuals is intrinsically related to positive emotions. Additionally, we show that food perception can be also affected by emotions, which are not necessarily related to food, but which nevertheless frame food delivery. We argue that this emotion-related effect may be grounded on resonance ‘mirror’ mechanisms allowing people to spontaneously tune to others’ emotions.},
	urldate = {2024-08-06},
	journal = {Food Quality and Preference},
	author = {Rizzato, Matteo and Di Dio, Cinzia and Fasano, Fabrizio and Gilli, Gabriella and Marchetti, Antonella and Sensidoni, Alessandro},
	month = jun,
	year = {2016},
	keywords = {Anger, Emotion, Faces, Food, Food desirability, Happiness, Social context},
	pages = {109--116},
}

@incollection{kublaoui_chapter_2014,
	title = {{CHAPTER} 3 - {Receptor} transduction pathways mediating hormone action},
	isbn = {978-1-4557-4858-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9781455748587000123},
	urldate = {2024-08-06},
	booktitle = {Pediatric {Endocrinology} ({Fourth} {Edition})},
	publisher = {W.B. Saunders},
	author = {Kublaoui, Bassil and Levine, Michael A.},
	editor = {Sperling, Mark A.},
	month = jan,
	year = {2014},
	doi = {10.1016/B978-1-4557-4858-7.00012-3},
	pages = {34--89.e2},
}

@article{jiang_content_2021,
	title = {Content changes of \textit{{Jiupei}} tripeptide {Tyr}-{Gly}-{Asp} during simulated distillation process of baijiu and the potential \textit{in vivo} antioxidant ability investigation},
	volume = {102},
	issn = {0889-1575},
	url = {https://www.sciencedirect.com/science/article/pii/S0889157521002349},
	doi = {10.1016/j.jfca.2021.104034},
	abstract = {Baijiu is the national liquor of China. Fermented grain (Jiupei) is the main material for baijiu distillation process. A tripeptide Tyr-Gly-Asp (YGD) with in vitro antioxidant ability was isolated from Jiupei. As one of the non-volatile components, existence of Jiupei peptides after baijiu distillation is still unclear. Study aims to investigate the content changes of total peptides and YGD during the high temperature of distillation process, YGD stability in baijiu and its influence on baijiu characters. AAPH induced oxidative stress Sprague Dawley (SD) rat model was used to evaluate YGD antioxidant and anti-inflammatory activities in vivo. Results showed that YGD content changed under the high temperature of simulated distillation process, exist stably in baijiu and show little influence on baijiu quality. YGD was able to attenuate AAPH-induced oxidative stress by elevating the expressions of Nrf2/Keap1-p38MAPK/PI3K-MafK signaling pathway and downstream antioxidant enzymes. In addition, YGD significantly inhibited the overproduction of inflammatory cytokines and mediator caused by AAPH in rat serum. In light of these findings, YGD is a tripeptide with in vivo antioxidant activity isolated from Jiupei, which proves functional peptides in baijiu are traced from Jiupei and lay a foundation for the development of healthy baijiu.},
	urldate = {2024-08-06},
	journal = {Journal of Food Composition and Analysis},
	author = {Jiang, Yunsong and Kang, Qiao and Yin, Zhongtian and Sun, Jinyuan and Wang, Bowen and Zeng, Xin-an and Zhao, Dongrui and Li, Hehe and Huang, Mingquan},
	month = sep,
	year = {2021},
	keywords = {AAPH-induced SD rat model, Antioxidant, Nrf2/Keap1, Simulated distillation, Tyr-Gly-Asp (YGD)},
	pages = {104034},
}

@article{pereira_learning_2021-1,
	title = {Learning software configuration spaces: {A} systematic literature review},
	volume = {182},
	issn = {0164-1212},
	shorttitle = {Learning software configuration spaces},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221001412},
	doi = {10.1016/j.jss.2021.111044},
	abstract = {Most modern software systems (operating systems like Linux or Android, Web browsers like Firefox or Chrome, video encoders like ffmpeg, x264 or VLC, mobile and cloud applications, etc.) are highly configurable. Hundreds of configuration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, security, energy consumption, etc. Due to the combinatorial explosion and the cost of executing software, it is quickly impossible to exhaustively explore the whole configuration space. Hence, numerous works have investigated the idea of learning it from a small sample of configurations’ measurements. The pattern “sampling, measuring, learning” has emerged in the literature, with several practical interests for both software developers and end-users of configurable systems. In this systematic literature review, we report on the different application objectives (e.g., performance prediction, configuration optimization, constraint mining), use-cases, targeted software systems, and application domains. We review the various strategies employed to gather a representative and cost-effective sample. We describe automated software techniques used to measure functional and non-functional properties of configurations. We classify machine learning algorithms and how they relate to the pursued application. Finally, we also describe how researchers evaluate the quality of the learning process. The findings from this systematic review show that the potential application objective is important; there are a vast number of case studies reported in the literature related to particular domains or software systems. Yet, the huge variant space of configurable systems is still challenging and calls to further investigate the synergies between artificial intelligence and software engineering.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and Jézéquel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
	month = dec,
	year = {2021},
	keywords = {Systematic literature review, Configurable systems, Machine learning, Software product lines},
	pages = {111044},
	file = {Versão submetida:files/1305/Pereira et al. - 2021 - Learning software configuration spaces A systemat.pdf:application/pdf},
}

@article{nikitakos_autonomous_2018,
	series = {18th {IFAC} {Conference} on {Technology}, {Culture} and {International} {Stability} {TECIS} 2018},
	title = {Autonomous {Robotic} {Platform} in {Harm} {Environment} {Onboard} of {Ships}},
	volume = {51},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S240589631833012X},
	doi = {10.1016/j.ifacol.2018.11.337},
	abstract = {Structural and machinery failures in the day-to-day ship operations may lead to major accidents, endangering crew onboard, posing a threat to the environment, damaging the ship itself and having a great impact in terms of business losses. Maintenance is a primary service to be performed in complex systems, especially those whose failures can compromise personnel and environmental safety, such as large ships on sailing. In this paper we present a proposed system for Harm Environment Onboard of Ships applications. It is low cost, open source software using robotic, embedded systems and IoT technologies. It offers protection of human resources in applications that are hazardous to human work.},
	number = {30},
	urldate = {2024-08-06},
	journal = {IFAC-PapersOnLine},
	author = {Nikitakos, Nikitas and Tsaganos, George and Papachristos, Dimitrios},
	month = jan,
	year = {2018},
	keywords = {Autonomous Robotics, Environment},
	pages = {390--395},
}

@incollection{caron_43_2015,
	title = {43 - {International} {Perspectives} on {Security} in the {Twenty}-{First} {Century}},
	isbn = {978-0-12-800113-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128001134000432},
	abstract = {A security professional working in today’s dynamic operating environment has a daunting task. The supervisor will be ultimately responsible for advising and working in areas ranging from risk mitigation and personnel security to the identification of environmental considerations in international settings. The supervisor will be advising customers on matters such as active and passive threat solutions, all while serving in the capacity of the adaptive leader that the client is looking for them to be. This chapter aims to provide a comprehensive understanding of the current international security concerns, ranging from civil considerations to terrorist activities and insurgencies, to security management concerns on the international stage.},
	urldate = {2024-08-06},
	booktitle = {Security {Supervision} and {Management} ({Fourth} {Edition})},
	publisher = {Butterworth-Heinemann},
	author = {Caron, Paul A. and Goswami, K. C. and Ekhomu, Ona and Oey, H. D. G. T. and Dobbins, Bruce W. and Erikson, Erik D.},
	editor = {Davies, Sandi J. and Hertig, Christopher A. and Gilbride, Brion P.},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-800113-4.00043-2},
	keywords = {Security, International security concerns, Security consultancy, Security management, Terrorism},
	pages = {581--615},
}

@incollection{sakka_chapter_2013,
	address = {Oxford},
	title = {Chapter 11.1.2 - {Sol}–{Gel} {Process} and {Applications}},
	isbn = {978-0-12-385469-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123854698000484},
	urldate = {2024-08-06},
	booktitle = {Handbook of {Advanced} {Ceramics} ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Sakka, Sumio},
	editor = {Somiya, Shigeyuki},
	month = jan,
	year = {2013},
	doi = {10.1016/B978-0-12-385469-8.00048-4},
	pages = {883--910},
}

@article{lawo_withdrawn_2021,
	title = {{WITHDRAWN}: {From} {Farms} to {Fridges}: {A} {Consumer}-{Oriented} {Design} {Approach} to {Sustainable} {Food} {Traceability}},
	volume = {27},
	issn = {2352-5509},
	shorttitle = {{WITHDRAWN}},
	url = {https://www.sciencedirect.com/science/article/pii/S2352550920313816},
	doi = {10.1016/j.spc.2020.11.007},
	abstract = {This article has been withdrawn at the request of the author(s) and/or editor. The Publisher apologizes for any inconvenience this may cause. The full Elsevier Policy on Article Withdrawal can be found at http://www.elsevier.com/locate/withdrawalpolicy.},
	urldate = {2024-08-06},
	journal = {Sustainable Production and Consumption},
	author = {Lawo, Dennis and Neifer, Thomas and Esau, Margarita and Vonholdt, Stephanie and Stevens, Gunnar},
	month = jul,
	year = {2021},
	pages = {282--297},
}

@article{papoutsoglou_analysis_2022,
	title = {An analysis of open source software licensing questions in {Stack} {Exchange} sites},
	volume = {183},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221002107},
	doi = {10.1016/j.jss.2021.111113},
	abstract = {Free and open source software is widely used in the creation of software systems, whereas many organizations choose to provide their systems as open source. Open source software carries licenses that determine the conditions under which the original software can be used. Appropriate use of licenses requires relevant expertise by the practitioners, and has an important legal angle. Educators and employers need to ensure that developers have the necessary training to understand licensing risks and how they can be addressed. At the same time, it is important to understand which issues practitioners face when they are using a specific open source license, when they are developing new open source software products or when they are reusing open source software. In this work, we examine questions posed about open source software licensing using data from the following Stack Exchange sites: Stack Overflow, Software Engineering, Open Source and Law. We analyze the indication of specific licenses and topics in the questions, investigate the attention the posts receive and trends over time, whether appropriate answers are provided and which type of questions are asked. Our results indicate that practitioners need, among other, clarifications about licensing specific software when other licenses are used, and for understanding license content. The results of the study can be useful for educators and employers, organizations that are authoring open source software licenses and developers for understanding the issues faced when using licenses, whereas they are relevant to other software engineering research areas, such as software reusability.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Papoutsoglou, Maria and Kapitsaki, Georgia M. and German, Daniel and Angelis, Lefteris},
	month = jan,
	year = {2022},
	keywords = {Open source software, Software licenses, Stack Exchange, Topic modeling},
	pages = {111113},
	file = {Versão submetida:files/1306/Papoutsoglou et al. - 2022 - An analysis of open source software licensing ques.pdf:application/pdf},
}

@article{liang_glucosinolates_2023,
	title = {Glucosinolates or erucic acid, which one contributes more to volatile flavor of fragrant rapeseed oil?},
	volume = {412},
	issn = {0308-8146},
	url = {https://www.sciencedirect.com/science/article/pii/S0308814623002108},
	doi = {10.1016/j.foodchem.2023.135594},
	abstract = {This study aims to investigate the effect of three rapeseed varieties with different erucic acid (EA) and glucosinolates (GLSs) content, and different degumming methods on the volatile flavor profiles of fragrant rapeseed oil (FRO). A total of 171 volatile compounds were identified by headspace solid-phase microextraction combine with gas chromatography-mass spectrometry (HS-SPME/GC–MS), and 87 compounds were identified as key odorants owing to their relative odor activity values (ROAV) ≥ 1. Methyl furfuryl disulfide was identified in rapeseed oil for the first time, with highest ROAVs (up to 26805.46). The volatile flavor profile of rapeseed oil was affected by GLSs content to a certain extent rather than EA content. Rapeseed varieties with low-EA and high-GLSs are suitable to produce FRO. Silicon dioxide adsorbing was an effective alternative method to water degumming in FRO. This work provided a new idea for selection of raw materials and degumming methods in FRO production.},
	urldate = {2024-08-06},
	journal = {Food Chemistry},
	author = {Liang, Qiang and Xiong, Wei and Zhou, Qi and Cui, Cheng and Xu, Xia and Zhao, Ling and Xuan, Pu and Yao, Yingzheng},
	month = jun,
	year = {2023},
	keywords = {Erucic acid, Fragrant rapeseed oil, Glucosinolate, Key odorants, Silicon dioxide degumming},
	pages = {135594},
}

@article{wang_influence_2020,
	title = {Influence of fixation methods on the chestnut-like aroma of green tea and dynamics of key aroma substances},
	volume = {136},
	issn = {0963-9969},
	url = {https://www.sciencedirect.com/science/article/pii/S0963996920305044},
	doi = {10.1016/j.foodres.2020.109479},
	abstract = {Fixation is the key process to ensure green tea quality; however, the effect of various fixation methods on the formation of green tea with a chestnut-like aroma and the evolution of key volatile compounds has not been assessed to date. In this study, we compared four types of fixation methods for green tea: roller-hot air–steam, roller-hot air, roller-steam, and single roller. Infrared-assisted headspace solid-phase microextraction and gas chromatography-tandem dual mass spectrometry technology were used to detect the volatile compounds of green tea samples during processing. Partial least-squares discriminant analysis (PLS-DA), multiple experiment viewer (MEV), odor activity value (OAV), and least-significant difference analyzes were then applied to clarify the best fixation method for forming a chestnut-like aroma and associated compounds, and to explore the change law of key volatile compounds using different green tea fixation processes. One hundred and eighty-four volatile compounds were detected in the processed samples, with roller-hot air fixation found as the optimal method for generating an intense and long-lasting chestnut-like aroma and floral taste, based on sensory evaluation. The PLS-DA model clearly distinguished the four kinds of fixation samples and obtained 32 differential volatile compounds. Combining OAVs with screening by MEV analysis, 2,6,10,10-tetramethyl-1-oxaspiro [4.5] dec-6-ene, linalool, cedrol, 3-methyl-butanal, trans-β-ionone, and τ-cadinol emerged as key differential volatile compounds between green teas with and without a chestnut-like aroma. The evolution of these six differential volatile compounds throughout the tea-making process confirmed that rolling-hot air coupling treatment is most conducive to produce a chestnut-like aroma, which is beneficial to form and transform 2,6,10,10-tetramethyl-1-oxaspiro[4.5] dec-6-ene, 3-methyl-butanal, and τ-cadinol with baking aromas and fruity substances. These results provide a theoretical basis and technical guidance for the precise and directional processing of high-quality green tea with a chestnut-like aroma.},
	urldate = {2024-08-06},
	journal = {Food Research International},
	author = {Wang, Huajie and Hua, Jinjie and Jiang, Yongwen and Yang, Yanqin and Wang, Jinjin and Yuan, Haibo},
	month = oct,
	year = {2020},
	keywords = {Electromagnetic roller-hot air–steam triple fixation, Gas chromatography-tandem mass spectrometry (GC–MS), Infrared-assisted headspace coupled to solid-phase microextraction, Multiple experiment viewer (MEV), Odor activity value (OAV), Partial least-squares discriminant analysis (PLS-DA)},
	pages = {109479},
}

@article{schaumberg_conceptualizing_2021,
	title = {Conceptualizing eating disorder psychopathology using an anxiety disorders framework: {Evidence} and implications for exposure-based clinical research},
	volume = {83},
	issn = {0272-7358},
	shorttitle = {Conceptualizing eating disorder psychopathology using an anxiety disorders framework},
	url = {https://www.sciencedirect.com/science/article/pii/S0272735820301409},
	doi = {10.1016/j.cpr.2020.101952},
	abstract = {Eating disorders (EDs) and anxiety disorders (ADs) evidence shared risk and significant comorbidity. Recent advances in understanding of anxiety-based disorders may have direct application to research and treatment efforts for EDs. The current review presents an up-to-date, behavioral conceptualization of the overlap between anxiety-based disorders and EDs. We identify ways in which anxiety presents in EDs, consider differences between EDs and ADs relevant to treatment adaptions, discuss how exposure-based strategies may be adapted for use in ED treatment, and outline directions for future mechanistic, translational, and clinical ED research from this perspective. Important research directions include: simultaneous examination of the extent to which EDs are characterized by aberrant avoidance-, reward-, and/or habit-based neurobiological and behavioral processes; improvement in understanding of how nutritional status interacts with neurobiological characteristics of EDs; incorporation of a growing knowledge of biobehavioral signatures in ED treatment planning; development of more comprehensive exposure-based treatment approaches for EDs; testing whether certain exposure interventions for AD are appropriate for EDs; and improvement in clinician self-efficacy and ability to use exposure therapy for EDs.},
	urldate = {2024-08-06},
	journal = {Clinical Psychology Review},
	author = {Schaumberg, Katherine and Reilly, Erin E. and Gorrell, Sasha and Levinson, Cheri A. and Farrell, Nicholas R. and Brown, Tiffany A. and Smith, Kathryn M. and Schaefer, Lauren M. and Essayli, Jamal H. and Haynos, Ann F. and Anderson, Lisa M.},
	month = feb,
	year = {2021},
	keywords = {Anxiety, Eating disorder, Exposure therapy},
	pages = {101952},
}

@article{jacobson_temporality_2014,
	title = {The temporality of intimacy: {Promise}, world, and death},
	volume = {13},
	issn = {1755-4586},
	shorttitle = {The temporality of intimacy},
	url = {https://www.sciencedirect.com/science/article/pii/S1755458613000844},
	doi = {10.1016/j.emospa.2013.08.006},
	abstract = {This paper explores the temporal character of intimacy. I begin by examining the significance of “promise” and “habit” in intimate relationships. These themes are developed through the work of M. Merleau-Ponty and J.H. van den Berg to reveal the embedded or en-worlded character of intimacy. These analyses help to articulate and to problematize the sense we often have of “established” relationships as possessing a fixed, already determined character. The final section discusses the issues of intimacy that surround the situation of dying. Specifically, it analyses (1) ways in which the issue in death is the stripping away of one's world, but also ways in which the meaning of one's death is still something futural, and thus “to be shaped”; and, (2) ways in which the shaping of this meaning with intimate others is significant both for the one manifestly dying and for those whose death seems distant.},
	urldate = {2024-08-06},
	journal = {Emotion, Space and Society},
	author = {Jacobson, Kirsten},
	month = nov,
	year = {2014},
	keywords = {Dying, Intimacy, J.H. van den Berg, Maurice Merleau-Ponty, Phenomenology, Temporality},
	pages = {103--110},
}

@incollection{messier_chapter_2014,
	address = {Boston},
	title = {Chapter 4 - {Storage} in the {Cloud}},
	isbn = {978-0-12-417040-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780124170407000046},
	abstract = {Description of storage services, concerns around data protection, and policy concerns.},
	urldate = {2024-08-06},
	booktitle = {Collaboration with {Cloud} {Computing}},
	publisher = {Syngress},
	author = {Messier, Ric},
	editor = {Messier, Ric},
	month = jan,
	year = {2014},
	doi = {10.1016/B978-0-12-417040-7.00004-6},
	keywords = {Software, applications, cloud, providers, Web},
	pages = {57--75},
}

@incollection{catania_7_2021,
	title = {7 - {AI} applications in prevalent diseases and disorders},
	isbn = {978-0-12-824477-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128244777000079},
	abstract = {Health care, biosciences and artificial intelligence (AI) represent a group of highly sophisticated and disruptive sciences and technologies. None, however, would mean much if they didn’t offer succor to the prevalent diseases and disorders that afflict humankind. Chapter 7 begins with the 2 biosciences of immunology and genetics that use AI to address chronic inflammation, autoimmune diseases, gene editing (CRISPR Cas9), gene replacement (CAR-T) and stem cell transplantation. The benefits discussed are incalculable as well as their value to all other prevailing diseases and disorders we face in health care. Some of those prevalent conditions and their AI applications are discussed throughout Chapter 7 and include cancer, cardio and cerebrovascular disease, diabetes, neurological disorders and the more common diseases of our bodily systems. Finally, AI’s role in prevalent issues like injury, infectious disease, chronic disease, mental health, depression, aging, exercise and nutrition are discussed.},
	urldate = {2024-08-06},
	booktitle = {Foundations of {Artificial} {Intelligence} in {Healthcare} and {Bioscience}},
	publisher = {Academic Press},
	author = {Catania, Louis J.},
	editor = {Catania, Louis J.},
	month = jan,
	year = {2021},
	doi = {10.1016/B978-0-12-824477-7.00007-9},
	keywords = {Autoimmune disease, Cancer, Cardiovascular disease, Cerebrovascular disease, Chronic disease, Chronic inflammation, Gene editing, Gene replacement, Immunology, Infectious disease},
	pages = {293--444},
}

@article{garcia-mireles_interactions_2018,
	title = {Interactions between environmental sustainability goals and software product quality: {A} mapping study},
	volume = {95},
	issn = {0950-5849},
	shorttitle = {Interactions between environmental sustainability goals and software product quality},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584917303142},
	doi = {10.1016/j.infsof.2017.10.002},
	abstract = {Context
Sustainability is considered as either a quality requirement or a quality characteristic that should be included in software when environmental protection concerns are being taken into account. However, addressing sustainability in software projects might have an impact on the quality of the software product delivered. Conflicting goals between sustainability and particular software product characteristics should be studied when developing application software, since achieving users’ requirements can be a hindrance in the quest to meet sustainability goals.
Objective
This paper aims to provide an overview of the approaches found in the literature for dealing with interactions between software product quality and sustainability in the context of application software.
Method
A systematic mapping study is conducted to identify practices for managing interactions between software quality characteristics and sustainability. The selected papers are classified according to the quality characteristic considered and their influence on sustainability.
Results
Most of the 66 selected papers focused on validating current technologies concerning their support for sustainability (46\%\%). The interaction between performance efficiency and energy efficiency is what is reported most and there is a fairly positive interaction. In addition, reliability and usability point to a positive interaction with energy efficiency, while security shows a conflicting interaction with energy efficiency. Functional suitability and maintainability can present both positive and negative interaction, with different goals derived from environmental sustainability.
Conclusions
Interactions between software quality and sustainability have been addressed within an explorative approach. There is a need for additional research work to characterize the impact of interaction on both software quality and sustainability. Furthermore, proposals should be validated in industrial settings.},
	urldate = {2024-08-06},
	journal = {Information and Software Technology},
	author = {García-Mireles, Gabriel Alberto and Moraga, Mª Ángeles and García, Félix and Calero, Coral and Piattini, Mario},
	month = mar,
	year = {2018},
	keywords = {Environmental sustainability, Greenability, Interaction, ISO/IEC 25010, Software product quality},
	pages = {108--129},
}

@article{lasco_drug_2023,
	title = {Drug testing in {Philippine} schools: {Historical} overview and implications for drug policy},
	volume = {113},
	issn = {0955-3959},
	shorttitle = {Drug testing in {Philippine} schools},
	url = {https://www.sciencedirect.com/science/article/pii/S0955395923000105},
	doi = {10.1016/j.drugpo.2023.103961},
	abstract = {With the stated aims of promoting “drug-free” campuses and “instilling in the minds of students” that drugs are harmful, drug testing in schools has been a feature of the Philippines’ punitive drug regime for two decades, gaining prominence during the Duterte administration's war on drugs (2016-2022). Drawing on key informant interviews and a desk review of news articles and official documents, this paper presents a historical overview of this policy as well as its impacts on students, educational institutions, and Philippine society. The paper finds that the group most affected by drug testing in schools are the students themselves, who are placed at risk of discrimination and alienation. Schools are also affected by the policy, as it requires expending their human and financial resources. More broadly, the policy perpetuates longstanding popular notions on drugs, children, and the overall idea that individuals carry the “burden of proof” to demonstrate their worthiness for societal inclusion. Drug testing in Philippine schools is ineffective and misguided in its objectives, but it has received widespread support because of its social and political efficacies.},
	urldate = {2024-08-06},
	journal = {International Journal of Drug Policy},
	author = {Lasco, Gideon},
	month = mar,
	year = {2023},
	keywords = {Philippines, Drug education, Drug testing, War on drugs, Young people},
	pages = {103961},
}

@article{toldo_moreira_environmental_2024,
	title = {Environmental economic valuation of production and preservation of fresh water: {A} systematic review},
	volume = {80},
	issn = {1617-1381},
	shorttitle = {Environmental economic valuation of production and preservation of fresh water},
	url = {https://www.sciencedirect.com/science/article/pii/S1617138124001043},
	doi = {10.1016/j.jnc.2024.126655},
	abstract = {Environmental economic valuation consists of estimating the economic value attributed to environmental resources. Despite the lack of a recognized price for environmental resources on the market, the economic value of these resources is measured by the extent to which their use affects levels of well-being. The present study aimed to conduct a literature review on environmental economic valuation directed at the production and preservation of freshwater resources. The main methods and variables used in the assessments and discussed topics were also investigated. For such, the Methodi Ordinatio was used, which enables the systematization of the search process. The Science Direct, Scopus, and Web of Science databases were searched for relevant articles published up to the year February 2024, using the keywords: (“environmental valuation” OR “economic valuation”) AND (“water” OR “river” OR “lake”). The search led to the retrieval of 1,995 records, 297 of which were ranked and the initial 66 composed the present systematic review. The articles were classified into four main topics: 1) water quality; 2) ecosystem and environmental services; 3) river restoration and protection; and 4) public policy and management. The main method employed in the studies was contingent valuation and variables associated with ‘willingness to pay’. The present systematic review contributes to identifying the most adequate method for a given study as well as analyzing the applicability and discussion of the environmental economic valuation of freshwater resources.},
	urldate = {2024-08-06},
	journal = {Journal for Nature Conservation},
	author = {Toldo Moreira, Tais and José Simioni, Flávio and Antunes Vieira, Sabrina and Emilia Siegloch, Ana},
	month = jul,
	year = {2024},
	keywords = {Economic valuation, Environmental valuation, Freshwater resources, Valuation methods},
	pages = {126655},
}

@article{arroyo_web_2018,
	series = {Transport {Survey} {Methods} in the era of big data:facing the challenges},
	title = {Web based survey to measuring social interactions, values, attitudes and travel behavior},
	volume = {32},
	issn = {2352-1465},
	url = {https://www.sciencedirect.com/science/article/pii/S2352146518301856},
	doi = {10.1016/j.trpro.2018.10.031},
	abstract = {This paper presents the data collection methodology developed for Minerva research project. The aim of Minerva is to study the influence of values, attitudes and social interactions on travel behavior. For this purpose, a web based survey has been developed, which consists of several questionnaires to collect respondents’ values and attitudes; a two-day activity-travel diary; information about social interactions; and socio-demographic characteristics. To identify the social contacts, it is being used a contact diary methodology together with the activity-travel diary.},
	urldate = {2024-08-06},
	journal = {Transportation Research Procedia},
	author = {Arroyo, Rosa and Ruiz, Tomás and Mars, Lidón and Serna, Ainhoa},
	month = jan,
	year = {2018},
	keywords = {attitudes, perceptions, social interactions, travel-behaviour, values, web-based survey},
	pages = {174--183},
}

@article{valdivia-garcia_characterizing_2018,
	title = {Characterizing and predicting blocking bugs in open source projects},
	volume = {143},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218300530},
	doi = {10.1016/j.jss.2018.03.053},
	abstract = {Software engineering researchers have studied specific types of issues such reopened bugs, performance bugs, dormant bugs, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems. In this paper, we study blocking bugs in eight open source projects and propose a model to predict them early on. We extract 14 different factors (from the bug repositories) that are made available within 24 hours after the initial submission of the bug reports. Then, we build decision trees to predict whether a bug will be a blocking bugs or not. Our results show that our prediction models achieve F-measures of 21\%–54\%, which is a two-fold improvement over the baseline predictors. We also analyze the fixes of these blocking bugs to understand their negative impact. We find that fixing blocking bugs requires more lines of code to be touched compared to non-blocking bugs. In addition, our file-level analysis shows that files affected by blocking bugs are more negatively impacted in terms of cohesion, coupling complexity and size than files affected by non-blocking bugs.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Valdivia-Garcia, Harold and Shihab, Emad and Nagappan, Meiyappan},
	month = sep,
	year = {2018},
	keywords = {Code metrics, Post-release defects, Process metrics},
	pages = {44--58},
}

@article{xiao_effect_2023,
	title = {Effect of inoculating \textit{{Pichia}} spp. starters on flavor formation of fermented chili pepper: {Metabolomics} and genomics approaches},
	volume = {173},
	issn = {0963-9969},
	shorttitle = {Effect of inoculating \textit{{Pichia}} spp. starters on flavor formation of fermented chili pepper},
	url = {https://www.sciencedirect.com/science/article/pii/S0963996923009420},
	doi = {10.1016/j.foodres.2023.113397},
	abstract = {The influence of Pichia spp. on flavor formation and metabolic pathways during chili pepper fermentation was investigated in this study. Multiple omics approaches were employed, including metabolomics analysis to identify volatile and non-volatile flavor compounds, and genomic analysis to gain insights into the underlying molecular mechanism driving flavor formation of chili peppers inoculated with Pichia spp. The results showed that inoculation with Pichia spp. accelerated fermentation process of chili peppers compared to spontaneous fermentation. Metabolomics analysis showed P. fermentans promoted characteristic terpenes [e.g., (Z)-β-ocimene and linalool], L-glutamate, gamma-aminobutyric acid, and succinate production, while P. manshurica produced more alcohols (e.g., isoamyl alcohol and phenylethyl alcohol) and phenols (e.g., 4-ethylguaiacol and 2-methoxy-4-methylphenol). Genomics analysis revealed that a substantial portion of the genes in Pichia spp. were associated with amino acid and carbohydrate metabolism. Specifically, the pathways involved in amino acid metabolism and the release of glycoside-bound aromatic compounds were identified as the primary drivers behind the unique flavor of fermented chili peppers, facilitated by Pichia spp.},
	urldate = {2024-08-06},
	journal = {Food Research International},
	author = {Xiao, Yue and Zhang, Shiyao and Liu, Zhijia and Wang, Tao and Cai, Shengbao and Chu, Chuanqi and Hu, Xiaosong and Yi, Junjie},
	month = nov,
	year = {2023},
	keywords = {Genomics, Fermented chili pepper, Flavor, Metabolic pathways, Metabolomics, spp.},
	pages = {113397},
}

@article{turkovic_orodispersible_2022,
	title = {Orodispersible films — {Pharmaceutical} development for improved performance: {A} review},
	volume = {75},
	issn = {1773-2247},
	shorttitle = {Orodispersible films — {Pharmaceutical} development for improved performance},
	url = {https://www.sciencedirect.com/science/article/pii/S1773224722006190},
	doi = {10.1016/j.jddst.2022.103708},
	abstract = {Orodispersible films (ODFs) have recently emerged as innovative dosage form which provides distinct advantages in the patient centric pharmaceutical drug product design due to inherent dosing flexibility and improved patient acceptability. Although their potential advantages in pharmacotherapy are well recognized, there is still a lot of research work to be done in order to explore and understand complex relationships among different formulation factors, film mechanical properties, and their bioperformance. Lack of standardized characterization methods and relevant specifications pose additional limitation for their wider application. In the present study, in-depth review of the available body of data published on ODF development and characterization was performed. In total, 112 papers published between November 2008 and April 2022 were taken into consideration for dataset building. Data collected have been critically evaluated and compiled into the representative dataset formed around three domains, namely: (A) Manufacturing method and composition; (B) ODF characteristics; and (C) ODF sensory attributes. Based on the investigated dataset, an attempt was made to identify the acceptable range of Critical Quality Attributes (CQA) values and propose ODF specific Quality Targeted Product Profile (QTPP) as a foundation which should guide and facilitate pharmaceutical development.},
	urldate = {2024-08-06},
	journal = {Journal of Drug Delivery Science and Technology},
	author = {Turković, Erna and Vasiljević, Ivana and Drašković, Milica and Parojčić, Jelena},
	month = sep,
	year = {2022},
	keywords = {Critical quality attributes, Disintegration time, Mechanical properties, ODF composition, ODF manufacturing methods, ODF sensory attributes, Quality target product profile},
	pages = {103708},
}

@incollection{thomasian_chapter_2022,
	title = {Chapter 1 - {Introduction}},
	isbn = {978-0-323-90796-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323907965000103},
	abstract = {Computers gained popularity in commercial, scientific,engineering, and military applications after WW II. IBM introduced the S/360 computer family in 1964 with Multiple Virtual Storage - MVS operating system, which is perhaps the only aspect of IBM that has survived as the z-series with 64 bit addressing, hardwares assists), etc. Other computer companies have their own computer families and operating systems, although High Level Languages - HLLs were standardized. I discuss my brief experience as a systems engineer at IBM and then at an electric utility. Microprocessors introduced in 1970 and were later enhanced by GPUs and FPGAs. Parallel and vector computers for parallel computation. Virtual and cache memory organizations and their replacement algorithms are discussed. Queueing Network Models - QNMs and their solution methods, which were used in capacity planning are introduced. Rules of thumb in computing such as Moore and Amdahl laws are discussed.},
	urldate = {2024-08-06},
	booktitle = {Storage {Systems}},
	publisher = {Morgan Kaufmann},
	author = {Thomasian, Alexander},
	editor = {Thomasian, Alexander},
	month = jan,
	year = {2022},
	doi = {10.1016/B978-0-32-390796-5.00010-3},
	keywords = {CPU caches, database buffers, early computer application for billing, Early computers, effect of data representation, high level programming languages, IBM S/360 computers, microprocessors, multiprogrammed computer systems, parallel computers, prefetching, queueing analysis, rules of thumb, supercomputers, vector computers, virtual memories},
	pages = {1--87},
}

@incollection{litwack_chapter_2022,
	address = {Boston},
	title = {Chapter 13 - {Metabolism} of {Amino} {Acids}},
	isbn = {978-0-323-85718-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323857185000200},
	abstract = {Hyperammonemia is the condition discussed in this chapter. Other topics discussed in this chapter are as follows: urea cycle, transamination, transamidation, deamination, amino acid racemization, l-amino acid decarboxylation, metabolism of amino acids to active substances, formation of catecholamines, melanin, phenylalanine and tyrosine blockages, and diseases and catabolism of amino acids. The chapter ends with a summary, a list of papers and books as suggested reading, summary, multiple-choice questions, and a case-based problem.},
	urldate = {2024-08-06},
	booktitle = {Human {Biochemistry} ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Litwack, Gerald},
	editor = {Litwack, Gerald},
	month = jan,
	year = {2022},
	doi = {10.1016/B978-0-323-85718-5.00020-0},
	keywords = {amino acid diseases, amino acid metabolism, amino acid oxidation, branched-chain amino acids, deamination, decarboxylation, glutathione, hyperammonemia, melanin, racemization, transamidation, transamination, Urea cycle},
	pages = {403--440},
}

@article{yeo_innovative_2024,
	title = {Innovative biomaterials for food packaging: {Unlocking} the potential of polyhydroxyalkanoate ({PHA}) biopolymers},
	volume = {163},
	issn = {2772-9508},
	shorttitle = {Innovative biomaterials for food packaging},
	url = {https://www.sciencedirect.com/science/article/pii/S2772950824001729},
	doi = {10.1016/j.bioadv.2024.213929},
	abstract = {Polyhydroxyalkanoate (PHA) biopolyesters show a good balance between sustainability and performance, making them a competitive alternative to conventional plastics for ecofriendly food packaging. With an emphasis on developments over the last decade (2014–2024), this review examines the revolutionary potential of PHAs as a sustainable food packaging material option. It also delves into the current state of commercial development, competitiveness, and the carbon footprint associated with PHA-based products. First, a critical examination of the challenges experienced by PHAs in terms of food packaging requirements is undertaken, followed by an assessment of contemporary strategies addressing permeability, mechanical properties, and processing considerations. The various PHA packaging end-of-life options, including a comprehensive overview of the environmental impact and potential solutions will also be discussed. Finally, conclusions and future perspectives are elucidated with a view of prospecting PHAs as future green materials, with a blend of performance and sustainability of food packaging solutions.},
	urldate = {2024-08-06},
	journal = {Biomaterials Advances},
	author = {Yeo, Jayven Chee Chuan and Muiruri, Joseph Kinyanjui and Fei, Xunchang and Wang, Tong and Zhang, Xikui and Xiao, Yihang and Thitsartarn, Warintorn and Tanoto, Hendrix and He, Chaobin and Li, Zibiao},
	month = oct,
	year = {2024},
	keywords = {Biopolymers, Food packaging materials, Polyhydroxyalkanoates, Renewable resources, Sustainability},
	pages = {213929},
}

@incollection{zimdahl_3_2012,
	address = {Oxford},
	title = {3 - {When} {Things} {Go} {Wrong}—{Balancing} {Technology}’s {Safety} and {Risk}},
	isbn = {978-0-12-416043-9},
	url = {https://www.sciencedirect.com/science/article/pii/B9780124160439000039},
	urldate = {2024-08-06},
	booktitle = {Agriculture's {Ethical} {Horizon} ({Second} {Edition})},
	publisher = {Elsevier},
	author = {Zimdahl, Robert L.},
	editor = {Zimdahl, Robert L.},
	month = jan,
	year = {2012},
	doi = {10.1016/B978-0-12-416043-9.00003-9},
	pages = {29--51},
}

@incollection{obodovskiy_chapter_2019,
	title = {Chapter 1 - {Peculiarities} of the {Processes} in {Microcosm}},
	isbn = {978-0-444-63979-0},
	url = {https://www.sciencedirect.com/science/article/pii/B978044463979000001X},
	abstract = {This chapter introduces in a quite popular form some selected questions of quantum laws, atomic and molecular physics that are required to read and understand the main contents of the book. The reader will find the compact summary of the main peculiarities of the microcosm, the difference between micro- and macrocosm. Brief information is given about the model of the Bohr atom and more detailed information on the quantum structure of the electron levels of an atom. The structure and electronic properties of molecules and various types of chemical bond: covalent, ionic, van der Waals are described. The hybridization of electronic levels is discussed. Information is provided on intermolecular bonds, including hydrogen bonding. Considerable attention is paid to the role of quantum laws in biology, an idea of quantum superposition, quantum entanglement, and nonlocality is given. Biological processes are described, for the explanation of which the main provisions of quantum mechanics are involved: photosynthesis, magnetoreception, sense of odor, origin and evolution of life.},
	urldate = {2024-08-06},
	booktitle = {Radiation},
	publisher = {Elsevier},
	author = {Obodovskiy, Ilya},
	editor = {Obodovskiy, Ilya},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-444-63979-0.00001-X},
	keywords = {Constitution and electron structure of molecules, Electron levels of an atom, Intermolecular interactions, Nontrivial quantum effects in biology, Quantum laws},
	pages = {3--39},
}

@incollection{tarlow_chapter_2017,
	title = {Chapter 3 - {Sports} as a {Tourism} {Attraction}},
	isbn = {978-0-12-805099-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128050996000038},
	abstract = {This chapter looks at sports and sporting events as a tourism attraction. The chapter examines both the positives and negatives of sports as they interact with tourism. The chapter studies how the Olympic Games have impacted the image of Rio de Janeiro and some of the side issues with this mega-event. The chapter touches upon everything from sports as an economic development tool to the interaction between sexual activity and the world of athletics. The chapter also distinguishes types of people who travel due to sporting events, from the single-person team (such as perhaps tennis) to the large complex team (such as football), and then it delineates the problems from youth sports travel to professional travel.},
	urldate = {2024-08-06},
	booktitle = {Sports {Travel} {Security}},
	publisher = {Butterworth-Heinemann},
	author = {Tarlow, Peter E.},
	editor = {Tarlow, Peter E.},
	month = jan,
	year = {2017},
	doi = {10.1016/B978-0-12-805099-6.00003-8},
	keywords = {Economic impact, mega events, neighborhood integrity, Rio de Janeiro, sexual exploitation, sexuality, sports tourism, tourist attraction, World Cup},
	pages = {51--75},
}

@article{waseem_design_2021-1,
	title = {Design, monitoring, and testing of microservices systems: {The} practitioners’ perspective},
	volume = {182},
	issn = {0164-1212},
	shorttitle = {Design, monitoring, and testing of microservices systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221001588},
	doi = {10.1016/j.jss.2021.111061},
	abstract = {Context:
Microservices Architecture (MSA) has received significant attention in the software industry. However, little empirical evidence exists on design, monitoring, and testing of microservices systems.
Objective:
This research aims to gain a deep understanding of how microservices systems are designed, monitored, and tested in the industry.
Methods:
A mixed-methods study was conducted with 106 survey responses and 6 interviews from microservices practitioners.
Results:
The main findings are: (1) a combination of domain-driven design and business capability is the most used strategy to decompose an application into microservices, (2) over half of the participants used architecture evaluation and architecture implementation when designing microservices systems, (3) API gateway and Backend for frontend patterns are the most used MSA patterns, (4) resource usage and load balancing as monitoring metrics, log management and exception tracking as monitoring practices are widely used, (5) unit and end-to-end testing are the most used testing strategies, and (6) the complexity of microservices systems poses challenges for their design, monitoring, and testing, for which there are no dedicated solutions.
Conclusions:
Our findings reveal that more research is needed to (1) deal with microservices complexity at the design level, (2) handle security in microservices systems, and (3) address the monitoring and testing challenges through dedicated solutions.},
	urldate = {2024-08-06},
	journal = {Journal of Systems and Software},
	author = {Waseem, Muhammad and Liang, Peng and Shahin, Mojtaba and Di Salle, Amleto and Márquez, Gastón},
	month = dec,
	year = {2021},
	keywords = {Design, Testing, Industrial survey, Microservices architecture, Monitoring},
	pages = {111061},
	file = {Versão submetida:files/1389/Waseem et al. - 2021 - Design, monitoring, and testing of microservices s.pdf:application/pdf},
}

@incollection{licuanan_chapter_2019,
	title = {Chapter 23 - {The} {Philippines}},
	isbn = {978-0-08-100853-9},
	url = {https://www.sciencedirect.com/science/article/pii/B9780081008539000518},
	abstract = {The Philippines is a large archipelago of over 7000 islands, mostly lined by coral reefs, seagrass meadows, and mangrove forests, and surrounded by waters with variable currents driven by a reversing monsoon system. The islands have narrow coastal plains, north-south mountain ranges, active volcanoes, and high seismological activity. These elements, along with the country’s complex geologic history and geographic position, make the Philippines one of the most biologically diverse, most disaster prone, and most environmentally threatened in the world. High dependence on coastal resources, overfishing and destructive fishing, and high rates of habitat loss make fishers in the country the poorest of the poor. The large population sizes and disproportionate impact of climate change here makes the poor’s situation worse. However, the country remains resource-rich, and its people have proven to be very resilient. More habitats are being protected and rehabilitated, and successful community-based fisheries management efforts are now being upscaled to higher governance levels. Although more conservation and management efforts are needed, there is much that could be learned from the Philippine experience.},
	urldate = {2024-08-06},
	booktitle = {World {Seas}: an {Environmental} {Evaluation} ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Licuanan, Wilfredo Y. and Cabreira, Reine W. and Aliño, Porfirio M.},
	editor = {Sheppard, Charles},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-08-100853-9.00051-8},
	keywords = {Overfishing, Philippines, Biodiversity, Coral reefs, Destructive fishing, Mangroves, Marine protected areas},
	pages = {515--537},
}
